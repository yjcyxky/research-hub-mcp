{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a vector store from a duckdb table using custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/jy006/miniconda3/envs/paper-parser/lib/python310.zip', '/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10', '/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/lib-dynload', '', '/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/site-packages', '/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/site-packages/setuptools/_vendor', '/Users/jy006/Documents/Code/BioMedGPS/paper-parser/scripts']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Please change the paths to your own paths\n",
    "paper_parser_dir = \"/Users/jy006/Documents/Code/BioMedGPS/paper-parser\"\n",
    "parquet_file = \"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/text_chunks/paragraphs.parquet\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "script_dir = os.path.join(paper_parser_dir, \"scripts\")\n",
    "sys.path.append(script_dir)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU qdrant-client duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────┐\n",
      "│ count_star() │\n",
      "│    int64     │\n",
      "├──────────────┤\n",
      "│          961 │\n",
      "└──────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect()\n",
    "conn.sql(f\"SELECT COUNT(*) FROM read_parquet('{parquet_file}')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ took 0.00 milliseconds\n",
      "load took 0.00 milliseconds\n"
     ]
    }
   ],
   "source": [
    "from embedding import DuckDBLoader\n",
    "\n",
    "raw_documents = DuckDBLoader(\n",
    "    query=f\"SELECT * FROM read_parquet('{parquet_file}') LIMIT 100\",\n",
    "    page_content_column=\"text\",\n",
    "    metadata_columns=[\"pmid\", \"pmc\", \"doi\", \"pubdate\"],\n",
    ").load()\n",
    "\n",
    "sentences = [doc.get(\"text\") for doc in list(raw_documents)[:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NV-Embed-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01db8e7ee3ae416fbf874c5e286fe33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m  \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnvidia/NV-Embed-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:347\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model =  SentenceTransformer(\"nvidia/NV-Embed-v2\", trust_remote_code=True, model_kwargs={\"torch_dtype\": torch.float16}, device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "__init__ took 0.00 milliseconds\n",
      "load took 0.00 milliseconds\n",
      "Initializing vector database...\n",
      "Loading model...\n",
      "Initializing cache database...\n",
      "__init__ took 68.21 milliseconds\n",
      "Computing embeddings and inserting into vector database...\n",
      "Starting multi-process pool...\n",
      "Processing batch 1\n",
      "Time taken to check: 0.0017609596252441406\n",
      "Missing 0 texts, 8 cached\n",
      "Processing batch 2\n",
      "Time taken to check: 0.0004668235778808594\n",
      "Missing 0 texts, 8 cached\n",
      "Processing batch 3\n",
      "Time taken to check: 0.0005509853363037109\n",
      "Missing 0 texts, 8 cached\n",
      "Processing batch 4\n",
      "Time taken to check: 6.818771362304688e-05\n",
      "Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/Users/jy006/miniconda3/envs/paper-parser/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to encode: 481.90996074676514\n",
      "Time taken to put: 0.011041879653930664\n",
      "Time taken to release GPU memory: 0.00011491775512695312\n",
      "Processing batch 5\n",
      "Time taken to check: 6.508827209472656e-05\n",
      "Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      6\u001b[0m num_documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mbuild_vector_db\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_db_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_document_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model_name=\"nvidia/NV-Embed-v2\",\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_content_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpmid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpmc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpubdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_documents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_batch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/BioMedGPS/paper-parser/scripts/embedding.py:15\u001b[0m, in \u001b[0;36mlog_time.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# 计算时间\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/BioMedGPS/paper-parser/scripts/embedding.py:416\u001b[0m, in \u001b[0;36mbuild_vector_db\u001b[0;34m(cache_filepath, raw_document_path, model_name, model, page_content_column, metadata_columns, num_documents, batch_size, allow_batch_mode, db_mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# vector_db.insert_documents(raw_documents, batch_size=10000)\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_batch_mode:\n\u001b[0;32m--> 416\u001b[0m     \u001b[43mcache_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_insert_documents_with_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     cache_db\u001b[38;5;241m.\u001b[39minsert_documents(preprocess_metadata(raw_documents), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, stats_filepath\u001b[38;5;241m=\u001b[39mstats_filepath)\n",
      "File \u001b[0;32m~/Documents/Code/BioMedGPS/paper-parser/scripts/embedding.py:15\u001b[0m, in \u001b[0;36mlog_time.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# 计算时间\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/BioMedGPS/paper-parser/scripts/embedding.py:336\u001b[0m, in \u001b[0;36mEmbeddingCacheDB.batch_insert_documents_with_pool\u001b[0;34m(self, documents, batch_size, document_column, metadata_column, stats_filepath)\u001b[0m\n\u001b[1;32m    334\u001b[0m s_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Generate embeddings for the batch and convert them to numpy\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_multi_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissing_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken to encode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39ms_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m s_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:975\u001b[0m, in \u001b[0;36mSentenceTransformer.encode_multi_process\u001b[0;34m(self, sentences, pool, prompt_name, prompt, batch_size, chunk_size, show_progress_bar, precision, normalize_embeddings)\u001b[0m\n\u001b[1;32m    971\u001b[0m     last_chunk_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    973\u001b[0m output_queue \u001b[38;5;241m=\u001b[39m pool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    974\u001b[0m results_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m--> 975\u001b[0m     [output_queue\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m trange(last_chunk_id, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar)],\n\u001b[1;32m    976\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    978\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results_list])\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:975\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    971\u001b[0m     last_chunk_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    973\u001b[0m output_queue \u001b[38;5;241m=\u001b[39m pool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    974\u001b[0m results_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m--> 975\u001b[0m     [\u001b[43moutput_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m trange(last_chunk_id, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar)],\n\u001b[1;32m    976\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    978\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results_list])\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/paper-parser/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from embedding import build_vector_db\n",
    "\n",
    "vector_db_dir = \"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/pmc_embedding/test_rocksdb_no_speedup\"\n",
    "batch_size = 8\n",
    "num_documents = 100\n",
    "\n",
    "build_vector_db(\n",
    "    cache_filepath=vector_db_dir,\n",
    "    raw_document_path=parquet_file,\n",
    "    # model_name=\"nvidia/NV-Embed-v2\",\n",
    "    model=model,\n",
    "    page_content_column=\"text\",\n",
    "    metadata_columns=[\"pmid\", \"pmc\", \"doi\", \"pubdate\"],\n",
    "    num_documents=num_documents,\n",
    "    batch_size=batch_size,\n",
    "    allow_batch_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:37,695 - INFO - Loading documents...\n",
      "INFO:embedding:Loading documents...\n",
      "2025-01-25 09:53:37,696 - INFO - __init__ took 0.00 milliseconds\n",
      "INFO:embedding:__init__ took 0.00 milliseconds\n",
      "2025-01-25 09:53:37,696 - INFO - load took 0.00 milliseconds\n",
      "INFO:embedding:load took 0.00 milliseconds\n",
      "2025-01-25 09:53:37,697 - INFO - Initializing vector database...\n",
      "INFO:embedding:Initializing vector database...\n",
      "2025-01-25 09:53:37,698 - INFO - Loading model...\n",
      "INFO:embedding:Loading model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-01-25 09:53:38,537 - INFO - Initializing cache database...\n",
      "INFO:embedding:Initializing cache database...\n",
      "2025-01-25 09:53:38,592 - INFO - __init__ took 894.19 milliseconds\n",
      "INFO:embedding:__init__ took 894.19 milliseconds\n",
      "2025-01-25 09:53:38,593 - INFO - Computing embeddings and inserting into vector database...\n",
      "INFO:embedding:Computing embeddings and inserting into vector database...\n",
      "2025-01-25 09:53:38,595 - INFO - Starting multi-process pool...\n",
      "INFO:embedding:Starting multi-process pool...\n",
      "INFO:sentence_transformers.SentenceTransformer:CUDA/NPU is not available. Starting 4 CPU workers\n",
      "INFO:sentence_transformers.SentenceTransformer:Start multi-process pool on devices: cpu, cpu, cpu, cpu\n",
      "2025-01-25 09:53:47,170 - INFO - \n",
      "Processing batch 1\n",
      "INFO:embedding:\n",
      "Processing batch 1\n",
      "2025-01-25 09:53:47,171 - INFO - Time taken to check: 0.00035500526428222656\n",
      "INFO:embedding:Time taken to check: 0.00035500526428222656\n",
      "2025-01-25 09:53:47,172 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7933b7d07e2f4639a823ec934a824404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,275 - INFO - Time taken to encode: 0.10180878639221191\n",
      "INFO:embedding:Time taken to encode: 0.10180878639221191\n",
      "2025-01-25 09:53:47,277 - INFO - Time taken to put: 0.0017070770263671875\n",
      "INFO:embedding:Time taken to put: 0.0017070770263671875\n",
      "2025-01-25 09:53:47,278 - INFO - Time taken to release GPU memory: 2.9087066650390625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.9087066650390625e-05\n",
      "2025-01-25 09:53:47,282 - INFO - \n",
      "Processing batch 2\n",
      "INFO:embedding:\n",
      "Processing batch 2\n",
      "2025-01-25 09:53:47,283 - INFO - Time taken to check: 0.00023174285888671875\n",
      "INFO:embedding:Time taken to check: 0.00023174285888671875\n",
      "2025-01-25 09:53:47,283 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b856f6c38284c98b0197f1dca26175a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,377 - INFO - Time taken to encode: 0.09264302253723145\n",
      "INFO:embedding:Time taken to encode: 0.09264302253723145\n",
      "2025-01-25 09:53:47,380 - INFO - Time taken to put: 0.002841949462890625\n",
      "INFO:embedding:Time taken to put: 0.002841949462890625\n",
      "2025-01-25 09:53:47,381 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:47,384 - INFO - \n",
      "Processing batch 3\n",
      "INFO:embedding:\n",
      "Processing batch 3\n",
      "2025-01-25 09:53:47,385 - INFO - Time taken to check: 0.0002639293670654297\n",
      "INFO:embedding:Time taken to check: 0.0002639293670654297\n",
      "2025-01-25 09:53:47,386 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a70b40d39c4887a0b147f75edcacf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,537 - INFO - Time taken to encode: 0.15041494369506836\n",
      "INFO:embedding:Time taken to encode: 0.15041494369506836\n",
      "2025-01-25 09:53:47,540 - INFO - Time taken to put: 0.0024480819702148438\n",
      "INFO:embedding:Time taken to put: 0.0024480819702148438\n",
      "2025-01-25 09:53:47,541 - INFO - Time taken to release GPU memory: 1.5735626220703125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5735626220703125e-05\n",
      "2025-01-25 09:53:47,543 - INFO - \n",
      "Processing batch 4\n",
      "INFO:embedding:\n",
      "Processing batch 4\n",
      "2025-01-25 09:53:47,544 - INFO - Time taken to check: 0.0007958412170410156\n",
      "INFO:embedding:Time taken to check: 0.0007958412170410156\n",
      "2025-01-25 09:53:47,545 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fa3e9549ce4477a151047a09b1931d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,654 - INFO - Time taken to encode: 0.10814714431762695\n",
      "INFO:embedding:Time taken to encode: 0.10814714431762695\n",
      "2025-01-25 09:53:47,656 - INFO - Time taken to put: 0.0020210742950439453\n",
      "INFO:embedding:Time taken to put: 0.0020210742950439453\n",
      "2025-01-25 09:53:47,657 - INFO - Time taken to release GPU memory: 1.621246337890625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.621246337890625e-05\n",
      "2025-01-25 09:53:47,659 - INFO - \n",
      "Processing batch 5\n",
      "INFO:embedding:\n",
      "Processing batch 5\n",
      "2025-01-25 09:53:47,660 - INFO - Time taken to check: 0.00031113624572753906\n",
      "INFO:embedding:Time taken to check: 0.00031113624572753906\n",
      "2025-01-25 09:53:47,661 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3234687671c342ab8d8d1f8770744758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,757 - INFO - Time taken to encode: 0.09605622291564941\n",
      "INFO:embedding:Time taken to encode: 0.09605622291564941\n",
      "2025-01-25 09:53:47,760 - INFO - Time taken to put: 0.002229928970336914\n",
      "INFO:embedding:Time taken to put: 0.002229928970336914\n",
      "2025-01-25 09:53:47,761 - INFO - Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "2025-01-25 09:53:47,763 - INFO - \n",
      "Processing batch 6\n",
      "INFO:embedding:\n",
      "Processing batch 6\n",
      "2025-01-25 09:53:47,764 - INFO - Time taken to check: 0.0002167224884033203\n",
      "INFO:embedding:Time taken to check: 0.0002167224884033203\n",
      "2025-01-25 09:53:47,765 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae166e81258843b7b4b12af158a83688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,895 - INFO - Time taken to encode: 0.12892603874206543\n",
      "INFO:embedding:Time taken to encode: 0.12892603874206543\n",
      "2025-01-25 09:53:47,898 - INFO - Time taken to put: 0.0025179386138916016\n",
      "INFO:embedding:Time taken to put: 0.0025179386138916016\n",
      "2025-01-25 09:53:47,899 - INFO - Time taken to release GPU memory: 1.71661376953125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.71661376953125e-05\n",
      "2025-01-25 09:53:47,901 - INFO - \n",
      "Processing batch 7\n",
      "INFO:embedding:\n",
      "Processing batch 7\n",
      "2025-01-25 09:53:47,902 - INFO - Time taken to check: 0.00020623207092285156\n",
      "INFO:embedding:Time taken to check: 0.00020623207092285156\n",
      "2025-01-25 09:53:47,903 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d7402d4c9a4b7ca4857a2a58bece43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:47,995 - INFO - Time taken to encode: 0.09137392044067383\n",
      "INFO:embedding:Time taken to encode: 0.09137392044067383\n",
      "2025-01-25 09:53:47,997 - INFO - Time taken to put: 0.0016429424285888672\n",
      "INFO:embedding:Time taken to put: 0.0016429424285888672\n",
      "2025-01-25 09:53:47,998 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:48,001 - INFO - \n",
      "Processing batch 8\n",
      "INFO:embedding:\n",
      "Processing batch 8\n",
      "2025-01-25 09:53:48,001 - INFO - Time taken to check: 0.00015115737915039062\n",
      "INFO:embedding:Time taken to check: 0.00015115737915039062\n",
      "2025-01-25 09:53:48,002 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af61a20ed1f7434e9b8627d67c388ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,108 - INFO - Time taken to encode: 0.1048440933227539\n",
      "INFO:embedding:Time taken to encode: 0.1048440933227539\n",
      "2025-01-25 09:53:48,110 - INFO - Time taken to put: 0.0014369487762451172\n",
      "INFO:embedding:Time taken to put: 0.0014369487762451172\n",
      "2025-01-25 09:53:48,111 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:48,113 - INFO - \n",
      "Processing batch 9\n",
      "INFO:embedding:\n",
      "Processing batch 9\n",
      "2025-01-25 09:53:48,114 - INFO - Time taken to check: 0.0003230571746826172\n",
      "INFO:embedding:Time taken to check: 0.0003230571746826172\n",
      "2025-01-25 09:53:48,115 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f95baabe66486ea2ec4ab5a90eace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,265 - INFO - Time taken to encode: 0.1496272087097168\n",
      "INFO:embedding:Time taken to encode: 0.1496272087097168\n",
      "2025-01-25 09:53:48,267 - INFO - Time taken to put: 0.0013933181762695312\n",
      "INFO:embedding:Time taken to put: 0.0013933181762695312\n",
      "2025-01-25 09:53:48,268 - INFO - Time taken to release GPU memory: 1.4781951904296875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.4781951904296875e-05\n",
      "2025-01-25 09:53:48,270 - INFO - \n",
      "Processing batch 10\n",
      "INFO:embedding:\n",
      "Processing batch 10\n",
      "2025-01-25 09:53:48,271 - INFO - Time taken to check: 0.0003571510314941406\n",
      "INFO:embedding:Time taken to check: 0.0003571510314941406\n",
      "2025-01-25 09:53:48,271 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ae0652396c4f27abb3e437f0edb2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,364 - INFO - Time taken to encode: 0.09194707870483398\n",
      "INFO:embedding:Time taken to encode: 0.09194707870483398\n",
      "2025-01-25 09:53:48,366 - INFO - Time taken to put: 0.0015299320220947266\n",
      "INFO:embedding:Time taken to put: 0.0015299320220947266\n",
      "2025-01-25 09:53:48,367 - INFO - Time taken to release GPU memory: 2.7894973754882812e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.7894973754882812e-05\n",
      "2025-01-25 09:53:48,369 - INFO - \n",
      "Processing batch 11\n",
      "INFO:embedding:\n",
      "Processing batch 11\n",
      "2025-01-25 09:53:48,370 - INFO - Time taken to check: 0.00035500526428222656\n",
      "INFO:embedding:Time taken to check: 0.00035500526428222656\n",
      "2025-01-25 09:53:48,371 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a678adecf29e4940addecb7eaccba883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,457 - INFO - Time taken to encode: 0.08608007431030273\n",
      "INFO:embedding:Time taken to encode: 0.08608007431030273\n",
      "2025-01-25 09:53:48,460 - INFO - Time taken to put: 0.00173187255859375\n",
      "INFO:embedding:Time taken to put: 0.00173187255859375\n",
      "2025-01-25 09:53:48,461 - INFO - Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "2025-01-25 09:53:48,463 - INFO - \n",
      "Processing batch 12\n",
      "INFO:embedding:\n",
      "Processing batch 12\n",
      "2025-01-25 09:53:48,463 - INFO - Time taken to check: 0.0002989768981933594\n",
      "INFO:embedding:Time taken to check: 0.0002989768981933594\n",
      "2025-01-25 09:53:48,464 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ecaf6d9d1243ddb3822ed33b124295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,614 - INFO - Time taken to encode: 0.1484518051147461\n",
      "INFO:embedding:Time taken to encode: 0.1484518051147461\n",
      "2025-01-25 09:53:48,617 - INFO - Time taken to put: 0.002101898193359375\n",
      "INFO:embedding:Time taken to put: 0.002101898193359375\n",
      "2025-01-25 09:53:48,617 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:48,619 - INFO - \n",
      "Processing batch 13\n",
      "INFO:embedding:\n",
      "Processing batch 13\n",
      "2025-01-25 09:53:48,620 - INFO - Time taken to check: 9.608268737792969e-05\n",
      "INFO:embedding:Time taken to check: 9.608268737792969e-05\n",
      "2025-01-25 09:53:48,620 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b2b221b1cb4d37a610cab47bc5c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,714 - INFO - Time taken to encode: 0.0925896167755127\n",
      "INFO:embedding:Time taken to encode: 0.0925896167755127\n",
      "2025-01-25 09:53:48,716 - INFO - Time taken to put: 0.0013971328735351562\n",
      "INFO:embedding:Time taken to put: 0.0013971328735351562\n",
      "2025-01-25 09:53:48,717 - INFO - Time taken to release GPU memory: 1.9073486328125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9073486328125e-05\n",
      "2025-01-25 09:53:48,719 - INFO - \n",
      "Processing batch 14\n",
      "INFO:embedding:\n",
      "Processing batch 14\n",
      "2025-01-25 09:53:48,719 - INFO - Time taken to check: 0.0002460479736328125\n",
      "INFO:embedding:Time taken to check: 0.0002460479736328125\n",
      "2025-01-25 09:53:48,720 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34bd38fc8ca4d06abf40858c6e0bf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,814 - INFO - Time taken to encode: 0.09287381172180176\n",
      "INFO:embedding:Time taken to encode: 0.09287381172180176\n",
      "2025-01-25 09:53:48,816 - INFO - Time taken to put: 0.0014400482177734375\n",
      "INFO:embedding:Time taken to put: 0.0014400482177734375\n",
      "2025-01-25 09:53:48,816 - INFO - Time taken to release GPU memory: 1.621246337890625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.621246337890625e-05\n",
      "2025-01-25 09:53:48,819 - INFO - \n",
      "Processing batch 15\n",
      "INFO:embedding:\n",
      "Processing batch 15\n",
      "2025-01-25 09:53:48,819 - INFO - Time taken to check: 0.00019884109497070312\n",
      "INFO:embedding:Time taken to check: 0.00019884109497070312\n",
      "2025-01-25 09:53:48,820 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561601de8a904f1d8e8657b927d415d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:48,957 - INFO - Time taken to encode: 0.13676881790161133\n",
      "INFO:embedding:Time taken to encode: 0.13676881790161133\n",
      "2025-01-25 09:53:48,960 - INFO - Time taken to put: 0.0016810894012451172\n",
      "INFO:embedding:Time taken to put: 0.0016810894012451172\n",
      "2025-01-25 09:53:48,961 - INFO - Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "2025-01-25 09:53:48,963 - INFO - \n",
      "Processing batch 16\n",
      "INFO:embedding:\n",
      "Processing batch 16\n",
      "2025-01-25 09:53:48,964 - INFO - Time taken to check: 0.0006840229034423828\n",
      "INFO:embedding:Time taken to check: 0.0006840229034423828\n",
      "2025-01-25 09:53:48,965 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0a1a8c5dff46e3ae9548521d9f5b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,080 - INFO - Time taken to encode: 0.11445403099060059\n",
      "INFO:embedding:Time taken to encode: 0.11445403099060059\n",
      "2025-01-25 09:53:49,084 - INFO - Time taken to put: 0.0023040771484375\n",
      "INFO:embedding:Time taken to put: 0.0023040771484375\n",
      "2025-01-25 09:53:49,084 - INFO - Time taken to release GPU memory: 1.621246337890625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.621246337890625e-05\n",
      "2025-01-25 09:53:49,086 - INFO - \n",
      "Processing batch 17\n",
      "INFO:embedding:\n",
      "Processing batch 17\n",
      "2025-01-25 09:53:49,087 - INFO - Time taken to check: 0.0005788803100585938\n",
      "INFO:embedding:Time taken to check: 0.0005788803100585938\n",
      "2025-01-25 09:53:49,088 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a313a44b76c4ba78c4d139c462924f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,192 - INFO - Time taken to encode: 0.1031501293182373\n",
      "INFO:embedding:Time taken to encode: 0.1031501293182373\n",
      "2025-01-25 09:53:49,195 - INFO - Time taken to put: 0.0022590160369873047\n",
      "INFO:embedding:Time taken to put: 0.0022590160369873047\n",
      "2025-01-25 09:53:49,195 - INFO - Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "2025-01-25 09:53:49,197 - INFO - \n",
      "Processing batch 18\n",
      "INFO:embedding:\n",
      "Processing batch 18\n",
      "2025-01-25 09:53:49,199 - INFO - Time taken to check: 0.0005476474761962891\n",
      "INFO:embedding:Time taken to check: 0.0005476474761962891\n",
      "2025-01-25 09:53:49,199 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49500204f0240e786f1c607dea00d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,354 - INFO - Time taken to encode: 0.1537799835205078\n",
      "INFO:embedding:Time taken to encode: 0.1537799835205078\n",
      "2025-01-25 09:53:49,356 - INFO - Time taken to put: 0.0015439987182617188\n",
      "INFO:embedding:Time taken to put: 0.0015439987182617188\n",
      "2025-01-25 09:53:49,357 - INFO - Time taken to release GPU memory: 1.621246337890625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.621246337890625e-05\n",
      "2025-01-25 09:53:49,359 - INFO - \n",
      "Processing batch 19\n",
      "INFO:embedding:\n",
      "Processing batch 19\n",
      "2025-01-25 09:53:49,360 - INFO - Time taken to check: 0.00018906593322753906\n",
      "INFO:embedding:Time taken to check: 0.00018906593322753906\n",
      "2025-01-25 09:53:49,361 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0b4aa7c33048bd9b686382322e1b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,457 - INFO - Time taken to encode: 0.09605813026428223\n",
      "INFO:embedding:Time taken to encode: 0.09605813026428223\n",
      "2025-01-25 09:53:49,460 - INFO - Time taken to put: 0.0015938282012939453\n",
      "INFO:embedding:Time taken to put: 0.0015938282012939453\n",
      "2025-01-25 09:53:49,460 - INFO - Time taken to release GPU memory: 1.4781951904296875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.4781951904296875e-05\n",
      "2025-01-25 09:53:49,462 - INFO - \n",
      "Processing batch 20\n",
      "INFO:embedding:\n",
      "Processing batch 20\n",
      "2025-01-25 09:53:49,463 - INFO - Time taken to check: 0.00014209747314453125\n",
      "INFO:embedding:Time taken to check: 0.00014209747314453125\n",
      "2025-01-25 09:53:49,464 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6f87d11ce44b2683c4c627aa4d4f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,570 - INFO - Time taken to encode: 0.10535383224487305\n",
      "INFO:embedding:Time taken to encode: 0.10535383224487305\n",
      "2025-01-25 09:53:49,572 - INFO - Time taken to put: 0.0013811588287353516\n",
      "INFO:embedding:Time taken to put: 0.0013811588287353516\n",
      "2025-01-25 09:53:49,573 - INFO - Time taken to release GPU memory: 1.621246337890625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.621246337890625e-05\n",
      "2025-01-25 09:53:49,575 - INFO - \n",
      "Processing batch 21\n",
      "INFO:embedding:\n",
      "Processing batch 21\n",
      "2025-01-25 09:53:49,576 - INFO - Time taken to check: 0.00037407875061035156\n",
      "INFO:embedding:Time taken to check: 0.00037407875061035156\n",
      "2025-01-25 09:53:49,576 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315c17eea6b2454187f2edd72133e239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,725 - INFO - Time taken to encode: 0.14765191078186035\n",
      "INFO:embedding:Time taken to encode: 0.14765191078186035\n",
      "2025-01-25 09:53:49,727 - INFO - Time taken to put: 0.0014958381652832031\n",
      "INFO:embedding:Time taken to put: 0.0014958381652832031\n",
      "2025-01-25 09:53:49,728 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:49,730 - INFO - \n",
      "Processing batch 22\n",
      "INFO:embedding:\n",
      "Processing batch 22\n",
      "2025-01-25 09:53:49,731 - INFO - Time taken to check: 0.0006480216979980469\n",
      "INFO:embedding:Time taken to check: 0.0006480216979980469\n",
      "2025-01-25 09:53:49,732 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8efe96b8ab41c6846ce2f7a6f73018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,843 - INFO - Time taken to encode: 0.11112308502197266\n",
      "INFO:embedding:Time taken to encode: 0.11112308502197266\n",
      "2025-01-25 09:53:49,846 - INFO - Time taken to put: 0.0018031597137451172\n",
      "INFO:embedding:Time taken to put: 0.0018031597137451172\n",
      "2025-01-25 09:53:49,847 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:49,849 - INFO - \n",
      "Processing batch 23\n",
      "INFO:embedding:\n",
      "Processing batch 23\n",
      "2025-01-25 09:53:49,850 - INFO - Time taken to check: 0.0002372264862060547\n",
      "INFO:embedding:Time taken to check: 0.0002372264862060547\n",
      "2025-01-25 09:53:49,851 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8c80e0ad8e4a66aca2eeaa7954e62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:49,954 - INFO - Time taken to encode: 0.10243368148803711\n",
      "INFO:embedding:Time taken to encode: 0.10243368148803711\n",
      "2025-01-25 09:53:49,957 - INFO - Time taken to put: 0.002043008804321289\n",
      "INFO:embedding:Time taken to put: 0.002043008804321289\n",
      "2025-01-25 09:53:49,957 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:49,960 - INFO - \n",
      "Processing batch 24\n",
      "INFO:embedding:\n",
      "Processing batch 24\n",
      "2025-01-25 09:53:49,961 - INFO - Time taken to check: 0.00013899803161621094\n",
      "INFO:embedding:Time taken to check: 0.00013899803161621094\n",
      "2025-01-25 09:53:49,961 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7420597afe07457d8769a383bcbb81ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,093 - INFO - Time taken to encode: 0.13101410865783691\n",
      "INFO:embedding:Time taken to encode: 0.13101410865783691\n",
      "2025-01-25 09:53:50,095 - INFO - Time taken to put: 0.0013458728790283203\n",
      "INFO:embedding:Time taken to put: 0.0013458728790283203\n",
      "2025-01-25 09:53:50,096 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:50,098 - INFO - \n",
      "Processing batch 25\n",
      "INFO:embedding:\n",
      "Processing batch 25\n",
      "2025-01-25 09:53:50,099 - INFO - Time taken to check: 0.00026798248291015625\n",
      "INFO:embedding:Time taken to check: 0.00026798248291015625\n",
      "2025-01-25 09:53:50,100 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d665ff80b2b04784992a20d8aa4d45e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,190 - INFO - Time taken to encode: 0.08985519409179688\n",
      "INFO:embedding:Time taken to encode: 0.08985519409179688\n",
      "2025-01-25 09:53:50,192 - INFO - Time taken to put: 0.001413106918334961\n",
      "INFO:embedding:Time taken to put: 0.001413106918334961\n",
      "2025-01-25 09:53:50,193 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:50,195 - INFO - \n",
      "Processing batch 26\n",
      "INFO:embedding:\n",
      "Processing batch 26\n",
      "2025-01-25 09:53:50,196 - INFO - Time taken to check: 0.00019598007202148438\n",
      "INFO:embedding:Time taken to check: 0.00019598007202148438\n",
      "2025-01-25 09:53:50,197 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd2eb9dc05844c6865e252ae89e6f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,340 - INFO - Time taken to encode: 0.14290118217468262\n",
      "INFO:embedding:Time taken to encode: 0.14290118217468262\n",
      "2025-01-25 09:53:50,343 - INFO - Time taken to put: 0.0015397071838378906\n",
      "INFO:embedding:Time taken to put: 0.0015397071838378906\n",
      "2025-01-25 09:53:50,343 - INFO - Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "2025-01-25 09:53:50,345 - INFO - \n",
      "Processing batch 27\n",
      "INFO:embedding:\n",
      "Processing batch 27\n",
      "2025-01-25 09:53:50,346 - INFO - Time taken to check: 0.00017595291137695312\n",
      "INFO:embedding:Time taken to check: 0.00017595291137695312\n",
      "2025-01-25 09:53:50,347 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79359a714c1f4c089613d602e4d162cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,449 - INFO - Time taken to encode: 0.10181784629821777\n",
      "INFO:embedding:Time taken to encode: 0.10181784629821777\n",
      "2025-01-25 09:53:50,452 - INFO - Time taken to put: 0.0022428035736083984\n",
      "INFO:embedding:Time taken to put: 0.0022428035736083984\n",
      "2025-01-25 09:53:50,453 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:53:50,455 - INFO - \n",
      "Processing batch 28\n",
      "INFO:embedding:\n",
      "Processing batch 28\n",
      "2025-01-25 09:53:50,456 - INFO - Time taken to check: 0.0002548694610595703\n",
      "INFO:embedding:Time taken to check: 0.0002548694610595703\n",
      "2025-01-25 09:53:50,457 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7f2f4d88b840c7af7174f86bbeb14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,556 - INFO - Time taken to encode: 0.09830498695373535\n",
      "INFO:embedding:Time taken to encode: 0.09830498695373535\n",
      "2025-01-25 09:53:50,558 - INFO - Time taken to put: 0.001615762710571289\n",
      "INFO:embedding:Time taken to put: 0.001615762710571289\n",
      "2025-01-25 09:53:50,559 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:50,562 - INFO - \n",
      "Processing batch 29\n",
      "INFO:embedding:\n",
      "Processing batch 29\n",
      "2025-01-25 09:53:50,562 - INFO - Time taken to check: 0.00021576881408691406\n",
      "INFO:embedding:Time taken to check: 0.00021576881408691406\n",
      "2025-01-25 09:53:50,563 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024d54d401464f2aa0963ce065395a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,693 - INFO - Time taken to encode: 0.12963509559631348\n",
      "INFO:embedding:Time taken to encode: 0.12963509559631348\n",
      "2025-01-25 09:53:50,696 - INFO - Time taken to put: 0.0016522407531738281\n",
      "INFO:embedding:Time taken to put: 0.0016522407531738281\n",
      "2025-01-25 09:53:50,696 - INFO - Time taken to release GPU memory: 1.71661376953125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.71661376953125e-05\n",
      "2025-01-25 09:53:50,699 - INFO - \n",
      "Processing batch 30\n",
      "INFO:embedding:\n",
      "Processing batch 30\n",
      "2025-01-25 09:53:50,700 - INFO - Time taken to check: 0.00010013580322265625\n",
      "INFO:embedding:Time taken to check: 0.00010013580322265625\n",
      "2025-01-25 09:53:50,701 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ea15fc295c4dfa86ceb1cc665ee9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,790 - INFO - Time taken to encode: 0.08865714073181152\n",
      "INFO:embedding:Time taken to encode: 0.08865714073181152\n",
      "2025-01-25 09:53:50,793 - INFO - Time taken to put: 0.0013909339904785156\n",
      "INFO:embedding:Time taken to put: 0.0013909339904785156\n",
      "2025-01-25 09:53:50,793 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:50,795 - INFO - \n",
      "Processing batch 31\n",
      "INFO:embedding:\n",
      "Processing batch 31\n",
      "2025-01-25 09:53:50,796 - INFO - Time taken to check: 0.00037407875061035156\n",
      "INFO:embedding:Time taken to check: 0.00037407875061035156\n",
      "2025-01-25 09:53:50,797 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9825424ec9344b2ca11916a0a7647ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:50,889 - INFO - Time taken to encode: 0.09105777740478516\n",
      "INFO:embedding:Time taken to encode: 0.09105777740478516\n",
      "2025-01-25 09:53:50,892 - INFO - Time taken to put: 0.0017049312591552734\n",
      "INFO:embedding:Time taken to put: 0.0017049312591552734\n",
      "2025-01-25 09:53:50,892 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:50,895 - INFO - \n",
      "Processing batch 32\n",
      "INFO:embedding:\n",
      "Processing batch 32\n",
      "2025-01-25 09:53:50,896 - INFO - Time taken to check: 0.00021123886108398438\n",
      "INFO:embedding:Time taken to check: 0.00021123886108398438\n",
      "2025-01-25 09:53:50,897 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aeef272d45243c6a4a3e5b108fa2036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,034 - INFO - Time taken to encode: 0.13643312454223633\n",
      "INFO:embedding:Time taken to encode: 0.13643312454223633\n",
      "2025-01-25 09:53:51,036 - INFO - Time taken to put: 0.001524209976196289\n",
      "INFO:embedding:Time taken to put: 0.001524209976196289\n",
      "2025-01-25 09:53:51,037 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:51,039 - INFO - \n",
      "Processing batch 33\n",
      "INFO:embedding:\n",
      "Processing batch 33\n",
      "2025-01-25 09:53:51,040 - INFO - Time taken to check: 0.0002009868621826172\n",
      "INFO:embedding:Time taken to check: 0.0002009868621826172\n",
      "2025-01-25 09:53:51,040 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b520e626f414f428c33dceae0f93e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,136 - INFO - Time taken to encode: 0.09482502937316895\n",
      "INFO:embedding:Time taken to encode: 0.09482502937316895\n",
      "2025-01-25 09:53:51,138 - INFO - Time taken to put: 0.0016551017761230469\n",
      "INFO:embedding:Time taken to put: 0.0016551017761230469\n",
      "2025-01-25 09:53:51,139 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:51,141 - INFO - \n",
      "Processing batch 34\n",
      "INFO:embedding:\n",
      "Processing batch 34\n",
      "2025-01-25 09:53:51,142 - INFO - Time taken to check: 0.0001671314239501953\n",
      "INFO:embedding:Time taken to check: 0.0001671314239501953\n",
      "2025-01-25 09:53:51,143 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d97f0c3a1e6472f9803dcd724a44406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,221 - INFO - Time taken to encode: 0.07823419570922852\n",
      "INFO:embedding:Time taken to encode: 0.07823419570922852\n",
      "2025-01-25 09:53:51,224 - INFO - Time taken to put: 0.001786947250366211\n",
      "INFO:embedding:Time taken to put: 0.001786947250366211\n",
      "2025-01-25 09:53:51,225 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:51,227 - INFO - \n",
      "Processing batch 35\n",
      "INFO:embedding:\n",
      "Processing batch 35\n",
      "2025-01-25 09:53:51,228 - INFO - Time taken to check: 0.00010204315185546875\n",
      "INFO:embedding:Time taken to check: 0.00010204315185546875\n",
      "2025-01-25 09:53:51,228 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f0d7f8d3de4c3d9f13b200fba8930c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,358 - INFO - Time taken to encode: 0.12914204597473145\n",
      "INFO:embedding:Time taken to encode: 0.12914204597473145\n",
      "2025-01-25 09:53:51,360 - INFO - Time taken to put: 0.001409769058227539\n",
      "INFO:embedding:Time taken to put: 0.001409769058227539\n",
      "2025-01-25 09:53:51,361 - INFO - Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5020370483398438e-05\n",
      "2025-01-25 09:53:51,363 - INFO - \n",
      "Processing batch 36\n",
      "INFO:embedding:\n",
      "Processing batch 36\n",
      "2025-01-25 09:53:51,364 - INFO - Time taken to check: 0.0005090236663818359\n",
      "INFO:embedding:Time taken to check: 0.0005090236663818359\n",
      "2025-01-25 09:53:51,365 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5653c88509bc4a4ea5e76c69eab6cb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,475 - INFO - Time taken to encode: 0.10943412780761719\n",
      "INFO:embedding:Time taken to encode: 0.10943412780761719\n",
      "2025-01-25 09:53:51,478 - INFO - Time taken to put: 0.0017440319061279297\n",
      "INFO:embedding:Time taken to put: 0.0017440319061279297\n",
      "2025-01-25 09:53:51,479 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:51,482 - INFO - \n",
      "Processing batch 37\n",
      "INFO:embedding:\n",
      "Processing batch 37\n",
      "2025-01-25 09:53:51,483 - INFO - Time taken to check: 0.0003299713134765625\n",
      "INFO:embedding:Time taken to check: 0.0003299713134765625\n",
      "2025-01-25 09:53:51,483 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5142138c684ffc86d26feeeb0687ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,585 - INFO - Time taken to encode: 0.10094070434570312\n",
      "INFO:embedding:Time taken to encode: 0.10094070434570312\n",
      "2025-01-25 09:53:51,587 - INFO - Time taken to put: 0.0014870166778564453\n",
      "INFO:embedding:Time taken to put: 0.0014870166778564453\n",
      "2025-01-25 09:53:51,588 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:51,590 - INFO - \n",
      "Processing batch 38\n",
      "INFO:embedding:\n",
      "Processing batch 38\n",
      "2025-01-25 09:53:51,591 - INFO - Time taken to check: 0.00016498565673828125\n",
      "INFO:embedding:Time taken to check: 0.00016498565673828125\n",
      "2025-01-25 09:53:51,591 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb4df3a4dc1407ab9e7746dfdc66454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,731 - INFO - Time taken to encode: 0.13857007026672363\n",
      "INFO:embedding:Time taken to encode: 0.13857007026672363\n",
      "2025-01-25 09:53:51,733 - INFO - Time taken to put: 0.00145721435546875\n",
      "INFO:embedding:Time taken to put: 0.00145721435546875\n",
      "2025-01-25 09:53:51,734 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:51,736 - INFO - \n",
      "Processing batch 39\n",
      "INFO:embedding:\n",
      "Processing batch 39\n",
      "2025-01-25 09:53:51,737 - INFO - Time taken to check: 0.00010895729064941406\n",
      "INFO:embedding:Time taken to check: 0.00010895729064941406\n",
      "2025-01-25 09:53:51,737 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317ed1cbebf6488eb8001c6021904a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,837 - INFO - Time taken to encode: 0.09939026832580566\n",
      "INFO:embedding:Time taken to encode: 0.09939026832580566\n",
      "2025-01-25 09:53:51,840 - INFO - Time taken to put: 0.0018699169158935547\n",
      "INFO:embedding:Time taken to put: 0.0018699169158935547\n",
      "2025-01-25 09:53:51,841 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:51,843 - INFO - \n",
      "Processing batch 40\n",
      "INFO:embedding:\n",
      "Processing batch 40\n",
      "2025-01-25 09:53:51,844 - INFO - Time taken to check: 0.00011014938354492188\n",
      "INFO:embedding:Time taken to check: 0.00011014938354492188\n",
      "2025-01-25 09:53:51,845 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5414467c5dfe459196e5182aa6441a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:51,933 - INFO - Time taken to encode: 0.08761715888977051\n",
      "INFO:embedding:Time taken to encode: 0.08761715888977051\n",
      "2025-01-25 09:53:51,935 - INFO - Time taken to put: 0.0014088153839111328\n",
      "INFO:embedding:Time taken to put: 0.0014088153839111328\n",
      "2025-01-25 09:53:51,936 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:51,938 - INFO - \n",
      "Processing batch 41\n",
      "INFO:embedding:\n",
      "Processing batch 41\n",
      "2025-01-25 09:53:51,939 - INFO - Time taken to check: 0.0001289844512939453\n",
      "INFO:embedding:Time taken to check: 0.0001289844512939453\n",
      "2025-01-25 09:53:51,940 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d58388a66c4b7185fdd2537b0e39f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,074 - INFO - Time taken to encode: 0.13344931602478027\n",
      "INFO:embedding:Time taken to encode: 0.13344931602478027\n",
      "2025-01-25 09:53:52,076 - INFO - Time taken to put: 0.0014338493347167969\n",
      "INFO:embedding:Time taken to put: 0.0014338493347167969\n",
      "2025-01-25 09:53:52,077 - INFO - Time taken to release GPU memory: 1.71661376953125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.71661376953125e-05\n",
      "2025-01-25 09:53:52,079 - INFO - \n",
      "Processing batch 42\n",
      "INFO:embedding:\n",
      "Processing batch 42\n",
      "2025-01-25 09:53:52,080 - INFO - Time taken to check: 0.000164031982421875\n",
      "INFO:embedding:Time taken to check: 0.000164031982421875\n",
      "2025-01-25 09:53:52,081 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c7c220180743dd94f1f0ef4ec93477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,164 - INFO - Time taken to encode: 0.08268189430236816\n",
      "INFO:embedding:Time taken to encode: 0.08268189430236816\n",
      "2025-01-25 09:53:52,167 - INFO - Time taken to put: 0.0018808841705322266\n",
      "INFO:embedding:Time taken to put: 0.0018808841705322266\n",
      "2025-01-25 09:53:52,168 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:52,170 - INFO - \n",
      "Processing batch 43\n",
      "INFO:embedding:\n",
      "Processing batch 43\n",
      "2025-01-25 09:53:52,171 - INFO - Time taken to check: 0.00015997886657714844\n",
      "INFO:embedding:Time taken to check: 0.00015997886657714844\n",
      "2025-01-25 09:53:52,171 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffadbb2ee084c339097b78c3c474e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,276 - INFO - Time taken to encode: 0.10394597053527832\n",
      "INFO:embedding:Time taken to encode: 0.10394597053527832\n",
      "2025-01-25 09:53:52,278 - INFO - Time taken to put: 0.00138092041015625\n",
      "INFO:embedding:Time taken to put: 0.00138092041015625\n",
      "2025-01-25 09:53:52,279 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:52,281 - INFO - \n",
      "Processing batch 44\n",
      "INFO:embedding:\n",
      "Processing batch 44\n",
      "2025-01-25 09:53:52,282 - INFO - Time taken to check: 0.00041413307189941406\n",
      "INFO:embedding:Time taken to check: 0.00041413307189941406\n",
      "2025-01-25 09:53:52,283 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc51aa83b09444aa97c9b4d3c42ff57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,422 - INFO - Time taken to encode: 0.13854694366455078\n",
      "INFO:embedding:Time taken to encode: 0.13854694366455078\n",
      "2025-01-25 09:53:52,424 - INFO - Time taken to put: 0.0014789104461669922\n",
      "INFO:embedding:Time taken to put: 0.0014789104461669922\n",
      "2025-01-25 09:53:52,425 - INFO - Time taken to release GPU memory: 1.6689300537109375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6689300537109375e-05\n",
      "2025-01-25 09:53:52,427 - INFO - \n",
      "Processing batch 45\n",
      "INFO:embedding:\n",
      "Processing batch 45\n",
      "2025-01-25 09:53:52,428 - INFO - Time taken to check: 0.00028204917907714844\n",
      "INFO:embedding:Time taken to check: 0.00028204917907714844\n",
      "2025-01-25 09:53:52,429 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f72e58116744b348219b5e37da5ee49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,506 - INFO - Time taken to encode: 0.07672905921936035\n",
      "INFO:embedding:Time taken to encode: 0.07672905921936035\n",
      "2025-01-25 09:53:52,509 - INFO - Time taken to put: 0.0015301704406738281\n",
      "INFO:embedding:Time taken to put: 0.0015301704406738281\n",
      "2025-01-25 09:53:52,510 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:52,512 - INFO - \n",
      "Processing batch 46\n",
      "INFO:embedding:\n",
      "Processing batch 46\n",
      "2025-01-25 09:53:52,513 - INFO - Time taken to check: 0.0001201629638671875\n",
      "INFO:embedding:Time taken to check: 0.0001201629638671875\n",
      "2025-01-25 09:53:52,513 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b51723568046ea8388c1c89e5cd4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,608 - INFO - Time taken to encode: 0.09373211860656738\n",
      "INFO:embedding:Time taken to encode: 0.09373211860656738\n",
      "2025-01-25 09:53:52,610 - INFO - Time taken to put: 0.0016102790832519531\n",
      "INFO:embedding:Time taken to put: 0.0016102790832519531\n",
      "2025-01-25 09:53:52,611 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:52,613 - INFO - \n",
      "Processing batch 47\n",
      "INFO:embedding:\n",
      "Processing batch 47\n",
      "2025-01-25 09:53:52,614 - INFO - Time taken to check: 0.0003402233123779297\n",
      "INFO:embedding:Time taken to check: 0.0003402233123779297\n",
      "2025-01-25 09:53:52,614 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac41086a8fbc4fe9bd32a224a4c88465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,741 - INFO - Time taken to encode: 0.12572503089904785\n",
      "INFO:embedding:Time taken to encode: 0.12572503089904785\n",
      "2025-01-25 09:53:52,743 - INFO - Time taken to put: 0.0015783309936523438\n",
      "INFO:embedding:Time taken to put: 0.0015783309936523438\n",
      "2025-01-25 09:53:52,744 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:52,746 - INFO - \n",
      "Processing batch 48\n",
      "INFO:embedding:\n",
      "Processing batch 48\n",
      "2025-01-25 09:53:52,747 - INFO - Time taken to check: 0.00033473968505859375\n",
      "INFO:embedding:Time taken to check: 0.00033473968505859375\n",
      "2025-01-25 09:53:52,748 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111d3ea5a0074cffa278c50a3645469c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,855 - INFO - Time taken to encode: 0.10727596282958984\n",
      "INFO:embedding:Time taken to encode: 0.10727596282958984\n",
      "2025-01-25 09:53:52,858 - INFO - Time taken to put: 0.0019729137420654297\n",
      "INFO:embedding:Time taken to put: 0.0019729137420654297\n",
      "2025-01-25 09:53:52,859 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:52,861 - INFO - \n",
      "Processing batch 49\n",
      "INFO:embedding:\n",
      "Processing batch 49\n",
      "2025-01-25 09:53:52,862 - INFO - Time taken to check: 0.00010800361633300781\n",
      "INFO:embedding:Time taken to check: 0.00010800361633300781\n",
      "2025-01-25 09:53:52,863 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d2f2fb9fb74b44b5f1f292ddf41767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:52,950 - INFO - Time taken to encode: 0.08668804168701172\n",
      "INFO:embedding:Time taken to encode: 0.08668804168701172\n",
      "2025-01-25 09:53:52,952 - INFO - Time taken to put: 0.0013191699981689453\n",
      "INFO:embedding:Time taken to put: 0.0013191699981689453\n",
      "2025-01-25 09:53:52,953 - INFO - Time taken to release GPU memory: 1.71661376953125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.71661376953125e-05\n",
      "2025-01-25 09:53:52,955 - INFO - \n",
      "Processing batch 50\n",
      "INFO:embedding:\n",
      "Processing batch 50\n",
      "2025-01-25 09:53:52,956 - INFO - Time taken to check: 0.00025963783264160156\n",
      "INFO:embedding:Time taken to check: 0.00025963783264160156\n",
      "2025-01-25 09:53:52,957 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c978da6a85dd437e8ce37eb85365536c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,089 - INFO - Time taken to encode: 0.13088297843933105\n",
      "INFO:embedding:Time taken to encode: 0.13088297843933105\n",
      "2025-01-25 09:53:53,091 - INFO - Time taken to put: 0.0015668869018554688\n",
      "INFO:embedding:Time taken to put: 0.0015668869018554688\n",
      "2025-01-25 09:53:53,092 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:53,095 - INFO - \n",
      "Processing batch 51\n",
      "INFO:embedding:\n",
      "Processing batch 51\n",
      "2025-01-25 09:53:53,095 - INFO - Time taken to check: 0.00018477439880371094\n",
      "INFO:embedding:Time taken to check: 0.00018477439880371094\n",
      "2025-01-25 09:53:53,096 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d613ecfc0c64a86bce951b8c463f758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,189 - INFO - Time taken to encode: 0.09250617027282715\n",
      "INFO:embedding:Time taken to encode: 0.09250617027282715\n",
      "2025-01-25 09:53:53,191 - INFO - Time taken to put: 0.0014116764068603516\n",
      "INFO:embedding:Time taken to put: 0.0014116764068603516\n",
      "2025-01-25 09:53:53,192 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:53,194 - INFO - \n",
      "Processing batch 52\n",
      "INFO:embedding:\n",
      "Processing batch 52\n",
      "2025-01-25 09:53:53,195 - INFO - Time taken to check: 0.00018978118896484375\n",
      "INFO:embedding:Time taken to check: 0.00018978118896484375\n",
      "2025-01-25 09:53:53,196 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fffa8baa51045368bb7714478999246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,290 - INFO - Time taken to encode: 0.0938570499420166\n",
      "INFO:embedding:Time taken to encode: 0.0938570499420166\n",
      "2025-01-25 09:53:53,292 - INFO - Time taken to put: 0.001558065414428711\n",
      "INFO:embedding:Time taken to put: 0.001558065414428711\n",
      "2025-01-25 09:53:53,293 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:53,295 - INFO - \n",
      "Processing batch 53\n",
      "INFO:embedding:\n",
      "Processing batch 53\n",
      "2025-01-25 09:53:53,296 - INFO - Time taken to check: 9.584426879882812e-05\n",
      "INFO:embedding:Time taken to check: 9.584426879882812e-05\n",
      "2025-01-25 09:53:53,296 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9d246637d4463cbea8c175ec223cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,417 - INFO - Time taken to encode: 0.12034320831298828\n",
      "INFO:embedding:Time taken to encode: 0.12034320831298828\n",
      "2025-01-25 09:53:53,419 - INFO - Time taken to put: 0.0014033317565917969\n",
      "INFO:embedding:Time taken to put: 0.0014033317565917969\n",
      "2025-01-25 09:53:53,420 - INFO - Time taken to release GPU memory: 3.2901763916015625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 3.2901763916015625e-05\n",
      "2025-01-25 09:53:53,423 - INFO - \n",
      "Processing batch 54\n",
      "INFO:embedding:\n",
      "Processing batch 54\n",
      "2025-01-25 09:53:53,424 - INFO - Time taken to check: 0.00015687942504882812\n",
      "INFO:embedding:Time taken to check: 0.00015687942504882812\n",
      "2025-01-25 09:53:53,424 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd1e1e86d4049869c818147539af5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,520 - INFO - Time taken to encode: 0.09486913681030273\n",
      "INFO:embedding:Time taken to encode: 0.09486913681030273\n",
      "2025-01-25 09:53:53,522 - INFO - Time taken to put: 0.0015790462493896484\n",
      "INFO:embedding:Time taken to put: 0.0015790462493896484\n",
      "2025-01-25 09:53:53,523 - INFO - Time taken to release GPU memory: 1.4781951904296875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.4781951904296875e-05\n",
      "2025-01-25 09:53:53,525 - INFO - \n",
      "Processing batch 55\n",
      "INFO:embedding:\n",
      "Processing batch 55\n",
      "2025-01-25 09:53:53,526 - INFO - Time taken to check: 0.00017499923706054688\n",
      "INFO:embedding:Time taken to check: 0.00017499923706054688\n",
      "2025-01-25 09:53:53,526 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849ae35cb2224aba8e2c461679a6dce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,617 - INFO - Time taken to encode: 0.09018778800964355\n",
      "INFO:embedding:Time taken to encode: 0.09018778800964355\n",
      "2025-01-25 09:53:53,620 - INFO - Time taken to put: 0.001661062240600586\n",
      "INFO:embedding:Time taken to put: 0.001661062240600586\n",
      "2025-01-25 09:53:53,620 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:53,622 - INFO - \n",
      "Processing batch 56\n",
      "INFO:embedding:\n",
      "Processing batch 56\n",
      "2025-01-25 09:53:53,623 - INFO - Time taken to check: 0.00011992454528808594\n",
      "INFO:embedding:Time taken to check: 0.00011992454528808594\n",
      "2025-01-25 09:53:53,624 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576ed21c466641139eec25afae9769e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,730 - INFO - Time taken to encode: 0.10589885711669922\n",
      "INFO:embedding:Time taken to encode: 0.10589885711669922\n",
      "2025-01-25 09:53:53,733 - INFO - Time taken to put: 0.0014431476593017578\n",
      "INFO:embedding:Time taken to put: 0.0014431476593017578\n",
      "2025-01-25 09:53:53,733 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:53:53,735 - INFO - \n",
      "Processing batch 57\n",
      "INFO:embedding:\n",
      "Processing batch 57\n",
      "2025-01-25 09:53:53,736 - INFO - Time taken to check: 0.0003101825714111328\n",
      "INFO:embedding:Time taken to check: 0.0003101825714111328\n",
      "2025-01-25 09:53:53,737 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d73915a2e7b44f5bbabc362ee676f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,841 - INFO - Time taken to encode: 0.10357522964477539\n",
      "INFO:embedding:Time taken to encode: 0.10357522964477539\n",
      "2025-01-25 09:53:53,844 - INFO - Time taken to put: 0.00185394287109375\n",
      "INFO:embedding:Time taken to put: 0.00185394287109375\n",
      "2025-01-25 09:53:53,845 - INFO - Time taken to release GPU memory: 1.9073486328125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9073486328125e-05\n",
      "2025-01-25 09:53:53,847 - INFO - \n",
      "Processing batch 58\n",
      "INFO:embedding:\n",
      "Processing batch 58\n",
      "2025-01-25 09:53:53,848 - INFO - Time taken to check: 0.0005390644073486328\n",
      "INFO:embedding:Time taken to check: 0.0005390644073486328\n",
      "2025-01-25 09:53:53,848 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e936fab216d0446fb795a0f63aad29de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:53,940 - INFO - Time taken to encode: 0.0915980339050293\n",
      "INFO:embedding:Time taken to encode: 0.0915980339050293\n",
      "2025-01-25 09:53:53,943 - INFO - Time taken to put: 0.0016701221466064453\n",
      "INFO:embedding:Time taken to put: 0.0016701221466064453\n",
      "2025-01-25 09:53:53,943 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:53,946 - INFO - \n",
      "Processing batch 59\n",
      "INFO:embedding:\n",
      "Processing batch 59\n",
      "2025-01-25 09:53:53,947 - INFO - Time taken to check: 0.00022077560424804688\n",
      "INFO:embedding:Time taken to check: 0.00022077560424804688\n",
      "2025-01-25 09:53:53,948 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b35f4655d441bc882d11692bc981b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,090 - INFO - Time taken to encode: 0.14229297637939453\n",
      "INFO:embedding:Time taken to encode: 0.14229297637939453\n",
      "2025-01-25 09:53:54,094 - INFO - Time taken to put: 0.0023288726806640625\n",
      "INFO:embedding:Time taken to put: 0.0023288726806640625\n",
      "2025-01-25 09:53:54,094 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:54,096 - INFO - \n",
      "Processing batch 60\n",
      "INFO:embedding:\n",
      "Processing batch 60\n",
      "2025-01-25 09:53:54,097 - INFO - Time taken to check: 9.107589721679688e-05\n",
      "INFO:embedding:Time taken to check: 9.107589721679688e-05\n",
      "2025-01-25 09:53:54,098 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dcf9d52ded40329000527d5b5ae6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,172 - INFO - Time taken to encode: 0.07414698600769043\n",
      "INFO:embedding:Time taken to encode: 0.07414698600769043\n",
      "2025-01-25 09:53:54,175 - INFO - Time taken to put: 0.0013551712036132812\n",
      "INFO:embedding:Time taken to put: 0.0013551712036132812\n",
      "2025-01-25 09:53:54,175 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:54,178 - INFO - \n",
      "Processing batch 61\n",
      "INFO:embedding:\n",
      "Processing batch 61\n",
      "2025-01-25 09:53:54,179 - INFO - Time taken to check: 0.00028204917907714844\n",
      "INFO:embedding:Time taken to check: 0.00028204917907714844\n",
      "2025-01-25 09:53:54,179 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea0739ab24d4ea8b10a832bca0c3263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,270 - INFO - Time taken to encode: 0.09039926528930664\n",
      "INFO:embedding:Time taken to encode: 0.09039926528930664\n",
      "2025-01-25 09:53:54,273 - INFO - Time taken to put: 0.0015881061553955078\n",
      "INFO:embedding:Time taken to put: 0.0015881061553955078\n",
      "2025-01-25 09:53:54,273 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:54,276 - INFO - \n",
      "Processing batch 62\n",
      "INFO:embedding:\n",
      "Processing batch 62\n",
      "2025-01-25 09:53:54,276 - INFO - Time taken to check: 0.00023603439331054688\n",
      "INFO:embedding:Time taken to check: 0.00023603439331054688\n",
      "2025-01-25 09:53:54,277 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e43dd2160424c0c9bb738eeeed7177e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,429 - INFO - Time taken to encode: 0.15166997909545898\n",
      "INFO:embedding:Time taken to encode: 0.15166997909545898\n",
      "2025-01-25 09:53:54,432 - INFO - Time taken to put: 0.0019137859344482422\n",
      "INFO:embedding:Time taken to put: 0.0019137859344482422\n",
      "2025-01-25 09:53:54,433 - INFO - Time taken to release GPU memory: 2.9802322387695312e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.9802322387695312e-05\n",
      "2025-01-25 09:53:54,435 - INFO - \n",
      "Processing batch 63\n",
      "INFO:embedding:\n",
      "Processing batch 63\n",
      "2025-01-25 09:53:54,436 - INFO - Time taken to check: 0.0003299713134765625\n",
      "INFO:embedding:Time taken to check: 0.0003299713134765625\n",
      "2025-01-25 09:53:54,437 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb182aa9aa1a4f929a807ac34de4ef10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,539 - INFO - Time taken to encode: 0.10163187980651855\n",
      "INFO:embedding:Time taken to encode: 0.10163187980651855\n",
      "2025-01-25 09:53:54,541 - INFO - Time taken to put: 0.0014200210571289062\n",
      "INFO:embedding:Time taken to put: 0.0014200210571289062\n",
      "2025-01-25 09:53:54,542 - INFO - Time taken to release GPU memory: 1.71661376953125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.71661376953125e-05\n",
      "2025-01-25 09:53:54,544 - INFO - \n",
      "Processing batch 64\n",
      "INFO:embedding:\n",
      "Processing batch 64\n",
      "2025-01-25 09:53:54,545 - INFO - Time taken to check: 0.00037789344787597656\n",
      "INFO:embedding:Time taken to check: 0.00037789344787597656\n",
      "2025-01-25 09:53:54,546 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d05ad2448a14b03ba8b3ec9831c8699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,629 - INFO - Time taken to encode: 0.08190488815307617\n",
      "INFO:embedding:Time taken to encode: 0.08190488815307617\n",
      "2025-01-25 09:53:54,632 - INFO - Time taken to put: 0.002488851547241211\n",
      "INFO:embedding:Time taken to put: 0.002488851547241211\n",
      "2025-01-25 09:53:54,633 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:54,635 - INFO - \n",
      "Processing batch 65\n",
      "INFO:embedding:\n",
      "Processing batch 65\n",
      "2025-01-25 09:53:54,636 - INFO - Time taken to check: 0.00027632713317871094\n",
      "INFO:embedding:Time taken to check: 0.00027632713317871094\n",
      "2025-01-25 09:53:54,636 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee42797f5c4ba78bd6f5b53daffe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,778 - INFO - Time taken to encode: 0.1409001350402832\n",
      "INFO:embedding:Time taken to encode: 0.1409001350402832\n",
      "2025-01-25 09:53:54,780 - INFO - Time taken to put: 0.0017979145050048828\n",
      "INFO:embedding:Time taken to put: 0.0017979145050048828\n",
      "2025-01-25 09:53:54,781 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:54,783 - INFO - \n",
      "Processing batch 66\n",
      "INFO:embedding:\n",
      "Processing batch 66\n",
      "2025-01-25 09:53:54,784 - INFO - Time taken to check: 0.0001857280731201172\n",
      "INFO:embedding:Time taken to check: 0.0001857280731201172\n",
      "2025-01-25 09:53:54,785 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18569ef42c3e4dd3a96482d5d2a3b0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,893 - INFO - Time taken to encode: 0.10709309577941895\n",
      "INFO:embedding:Time taken to encode: 0.10709309577941895\n",
      "2025-01-25 09:53:54,895 - INFO - Time taken to put: 0.0014698505401611328\n",
      "INFO:embedding:Time taken to put: 0.0014698505401611328\n",
      "2025-01-25 09:53:54,896 - INFO - Time taken to release GPU memory: 1.9073486328125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9073486328125e-05\n",
      "2025-01-25 09:53:54,898 - INFO - \n",
      "Processing batch 67\n",
      "INFO:embedding:\n",
      "Processing batch 67\n",
      "2025-01-25 09:53:54,898 - INFO - Time taken to check: 0.00014162063598632812\n",
      "INFO:embedding:Time taken to check: 0.00014162063598632812\n",
      "2025-01-25 09:53:54,899 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5770d89e91477182b0148b4aead72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:54,997 - INFO - Time taken to encode: 0.09756112098693848\n",
      "INFO:embedding:Time taken to encode: 0.09756112098693848\n",
      "2025-01-25 09:53:55,000 - INFO - Time taken to put: 0.001363992691040039\n",
      "INFO:embedding:Time taken to put: 0.001363992691040039\n",
      "2025-01-25 09:53:55,001 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:55,003 - INFO - \n",
      "Processing batch 68\n",
      "INFO:embedding:\n",
      "Processing batch 68\n",
      "2025-01-25 09:53:55,004 - INFO - Time taken to check: 0.00017499923706054688\n",
      "INFO:embedding:Time taken to check: 0.00017499923706054688\n",
      "2025-01-25 09:53:55,004 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a927dbc66447069079e1ecd89191f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,122 - INFO - Time taken to encode: 0.11756086349487305\n",
      "INFO:embedding:Time taken to encode: 0.11756086349487305\n",
      "2025-01-25 09:53:55,125 - INFO - Time taken to put: 0.0014493465423583984\n",
      "INFO:embedding:Time taken to put: 0.0014493465423583984\n",
      "2025-01-25 09:53:55,126 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:55,128 - INFO - \n",
      "Processing batch 69\n",
      "INFO:embedding:\n",
      "Processing batch 69\n",
      "2025-01-25 09:53:55,128 - INFO - Time taken to check: 0.00022602081298828125\n",
      "INFO:embedding:Time taken to check: 0.00022602081298828125\n",
      "2025-01-25 09:53:55,129 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a65280e20b45f5b4b627be7daec5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,203 - INFO - Time taken to encode: 0.07295107841491699\n",
      "INFO:embedding:Time taken to encode: 0.07295107841491699\n",
      "2025-01-25 09:53:55,205 - INFO - Time taken to put: 0.0015981197357177734\n",
      "INFO:embedding:Time taken to put: 0.0015981197357177734\n",
      "2025-01-25 09:53:55,206 - INFO - Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5974044799804688e-05\n",
      "2025-01-25 09:53:55,208 - INFO - \n",
      "Processing batch 70\n",
      "INFO:embedding:\n",
      "Processing batch 70\n",
      "2025-01-25 09:53:55,209 - INFO - Time taken to check: 0.0003058910369873047\n",
      "INFO:embedding:Time taken to check: 0.0003058910369873047\n",
      "2025-01-25 09:53:55,209 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22334cf3d974ac8b073488272c95803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,310 - INFO - Time taken to encode: 0.10051894187927246\n",
      "INFO:embedding:Time taken to encode: 0.10051894187927246\n",
      "2025-01-25 09:53:55,313 - INFO - Time taken to put: 0.0016791820526123047\n",
      "INFO:embedding:Time taken to put: 0.0016791820526123047\n",
      "2025-01-25 09:53:55,314 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:55,316 - INFO - \n",
      "Processing batch 71\n",
      "INFO:embedding:\n",
      "Processing batch 71\n",
      "2025-01-25 09:53:55,317 - INFO - Time taken to check: 0.00016689300537109375\n",
      "INFO:embedding:Time taken to check: 0.00016689300537109375\n",
      "2025-01-25 09:53:55,318 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16776332944d40d49defa84fd641098f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,447 - INFO - Time taken to encode: 0.1284031867980957\n",
      "INFO:embedding:Time taken to encode: 0.1284031867980957\n",
      "2025-01-25 09:53:55,449 - INFO - Time taken to put: 0.0018570423126220703\n",
      "INFO:embedding:Time taken to put: 0.0018570423126220703\n",
      "2025-01-25 09:53:55,450 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:55,452 - INFO - \n",
      "Processing batch 72\n",
      "INFO:embedding:\n",
      "Processing batch 72\n",
      "2025-01-25 09:53:55,453 - INFO - Time taken to check: 0.00033736228942871094\n",
      "INFO:embedding:Time taken to check: 0.00033736228942871094\n",
      "2025-01-25 09:53:55,453 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cf121f078f49f095b836d41189d042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,566 - INFO - Time taken to encode: 0.11218500137329102\n",
      "INFO:embedding:Time taken to encode: 0.11218500137329102\n",
      "2025-01-25 09:53:55,570 - INFO - Time taken to put: 0.0023453235626220703\n",
      "INFO:embedding:Time taken to put: 0.0023453235626220703\n",
      "2025-01-25 09:53:55,571 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:55,573 - INFO - \n",
      "Processing batch 73\n",
      "INFO:embedding:\n",
      "Processing batch 73\n",
      "2025-01-25 09:53:55,575 - INFO - Time taken to check: 0.0004482269287109375\n",
      "INFO:embedding:Time taken to check: 0.0004482269287109375\n",
      "2025-01-25 09:53:55,576 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9fbd1b9fdc49e2985146ef61a77b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,725 - INFO - Time taken to encode: 0.14891529083251953\n",
      "INFO:embedding:Time taken to encode: 0.14891529083251953\n",
      "2025-01-25 09:53:55,728 - INFO - Time taken to put: 0.0019130706787109375\n",
      "INFO:embedding:Time taken to put: 0.0019130706787109375\n",
      "2025-01-25 09:53:55,729 - INFO - Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.6927719116210938e-05\n",
      "2025-01-25 09:53:55,731 - INFO - \n",
      "Processing batch 74\n",
      "INFO:embedding:\n",
      "Processing batch 74\n",
      "2025-01-25 09:53:55,732 - INFO - Time taken to check: 0.00023794174194335938\n",
      "INFO:embedding:Time taken to check: 0.00023794174194335938\n",
      "2025-01-25 09:53:55,732 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434077ce4718448f98d5736a65b623f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,874 - INFO - Time taken to encode: 0.14125871658325195\n",
      "INFO:embedding:Time taken to encode: 0.14125871658325195\n",
      "2025-01-25 09:53:55,877 - INFO - Time taken to put: 0.0017352104187011719\n",
      "INFO:embedding:Time taken to put: 0.0017352104187011719\n",
      "2025-01-25 09:53:55,877 - INFO - Time taken to release GPU memory: 2.3126602172851562e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.3126602172851562e-05\n",
      "2025-01-25 09:53:55,880 - INFO - \n",
      "Processing batch 75\n",
      "INFO:embedding:\n",
      "Processing batch 75\n",
      "2025-01-25 09:53:55,881 - INFO - Time taken to check: 0.00015091896057128906\n",
      "INFO:embedding:Time taken to check: 0.00015091896057128906\n",
      "2025-01-25 09:53:55,881 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6999008618724d89bca3590a71f8cee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:55,980 - INFO - Time taken to encode: 0.09825301170349121\n",
      "INFO:embedding:Time taken to encode: 0.09825301170349121\n",
      "2025-01-25 09:53:55,983 - INFO - Time taken to put: 0.001355886459350586\n",
      "INFO:embedding:Time taken to put: 0.001355886459350586\n",
      "2025-01-25 09:53:55,983 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:55,986 - INFO - \n",
      "Processing batch 76\n",
      "INFO:embedding:\n",
      "Processing batch 76\n",
      "2025-01-25 09:53:55,987 - INFO - Time taken to check: 0.00031304359436035156\n",
      "INFO:embedding:Time taken to check: 0.00031304359436035156\n",
      "2025-01-25 09:53:55,987 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3500f56ba64402290b6806c887f2c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,075 - INFO - Time taken to encode: 0.0868079662322998\n",
      "INFO:embedding:Time taken to encode: 0.0868079662322998\n",
      "2025-01-25 09:53:56,077 - INFO - Time taken to put: 0.0015320777893066406\n",
      "INFO:embedding:Time taken to put: 0.0015320777893066406\n",
      "2025-01-25 09:53:56,078 - INFO - Time taken to release GPU memory: 2.1219253540039062e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.1219253540039062e-05\n",
      "2025-01-25 09:53:56,080 - INFO - \n",
      "Processing batch 77\n",
      "INFO:embedding:\n",
      "Processing batch 77\n",
      "2025-01-25 09:53:56,081 - INFO - Time taken to check: 0.00015616416931152344\n",
      "INFO:embedding:Time taken to check: 0.00015616416931152344\n",
      "2025-01-25 09:53:56,082 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e63504278a647458014566f7c411d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,218 - INFO - Time taken to encode: 0.1358048915863037\n",
      "INFO:embedding:Time taken to encode: 0.1358048915863037\n",
      "2025-01-25 09:53:56,220 - INFO - Time taken to put: 0.001428365707397461\n",
      "INFO:embedding:Time taken to put: 0.001428365707397461\n",
      "2025-01-25 09:53:56,221 - INFO - Time taken to release GPU memory: 1.621246337890625e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.621246337890625e-05\n",
      "2025-01-25 09:53:56,223 - INFO - \n",
      "Processing batch 78\n",
      "INFO:embedding:\n",
      "Processing batch 78\n",
      "2025-01-25 09:53:56,224 - INFO - Time taken to check: 0.00028586387634277344\n",
      "INFO:embedding:Time taken to check: 0.00028586387634277344\n",
      "2025-01-25 09:53:56,225 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b05e2d7ccb4e0eb6570f8804282932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,338 - INFO - Time taken to encode: 0.11300516128540039\n",
      "INFO:embedding:Time taken to encode: 0.11300516128540039\n",
      "2025-01-25 09:53:56,341 - INFO - Time taken to put: 0.0015552043914794922\n",
      "INFO:embedding:Time taken to put: 0.0015552043914794922\n",
      "2025-01-25 09:53:56,341 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:56,344 - INFO - \n",
      "Processing batch 79\n",
      "INFO:embedding:\n",
      "Processing batch 79\n",
      "2025-01-25 09:53:56,344 - INFO - Time taken to check: 0.00019288063049316406\n",
      "INFO:embedding:Time taken to check: 0.00019288063049316406\n",
      "2025-01-25 09:53:56,345 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd93dacc1844237b195fd736d289ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,452 - INFO - Time taken to encode: 0.1060020923614502\n",
      "INFO:embedding:Time taken to encode: 0.1060020923614502\n",
      "2025-01-25 09:53:56,455 - INFO - Time taken to put: 0.0015408992767333984\n",
      "INFO:embedding:Time taken to put: 0.0015408992767333984\n",
      "2025-01-25 09:53:56,455 - INFO - Time taken to release GPU memory: 1.71661376953125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.71661376953125e-05\n",
      "2025-01-25 09:53:56,458 - INFO - \n",
      "Processing batch 80\n",
      "INFO:embedding:\n",
      "Processing batch 80\n",
      "2025-01-25 09:53:56,459 - INFO - Time taken to check: 0.0003120899200439453\n",
      "INFO:embedding:Time taken to check: 0.0003120899200439453\n",
      "2025-01-25 09:53:56,460 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87078cc9cc934b50b5cf531035808820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,597 - INFO - Time taken to encode: 0.13660597801208496\n",
      "INFO:embedding:Time taken to encode: 0.13660597801208496\n",
      "2025-01-25 09:53:56,599 - INFO - Time taken to put: 0.0017321109771728516\n",
      "INFO:embedding:Time taken to put: 0.0017321109771728516\n",
      "2025-01-25 09:53:56,600 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:53:56,602 - INFO - \n",
      "Processing batch 81\n",
      "INFO:embedding:\n",
      "Processing batch 81\n",
      "2025-01-25 09:53:56,603 - INFO - Time taken to check: 0.0004870891571044922\n",
      "INFO:embedding:Time taken to check: 0.0004870891571044922\n",
      "2025-01-25 09:53:56,604 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324515442a354d6a8ca438863e90c6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,692 - INFO - Time taken to encode: 0.08760333061218262\n",
      "INFO:embedding:Time taken to encode: 0.08760333061218262\n",
      "2025-01-25 09:53:56,695 - INFO - Time taken to put: 0.00203704833984375\n",
      "INFO:embedding:Time taken to put: 0.00203704833984375\n",
      "2025-01-25 09:53:56,696 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:56,698 - INFO - \n",
      "Processing batch 82\n",
      "INFO:embedding:\n",
      "Processing batch 82\n",
      "2025-01-25 09:53:56,699 - INFO - Time taken to check: 0.0001590251922607422\n",
      "INFO:embedding:Time taken to check: 0.0001590251922607422\n",
      "2025-01-25 09:53:56,699 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c44869246d4f2988fc108106903f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,779 - INFO - Time taken to encode: 0.07867884635925293\n",
      "INFO:embedding:Time taken to encode: 0.07867884635925293\n",
      "2025-01-25 09:53:56,781 - INFO - Time taken to put: 0.00145721435546875\n",
      "INFO:embedding:Time taken to put: 0.00145721435546875\n",
      "2025-01-25 09:53:56,782 - INFO - Time taken to release GPU memory: 2.5033950805664062e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.5033950805664062e-05\n",
      "2025-01-25 09:53:56,784 - INFO - \n",
      "Processing batch 83\n",
      "INFO:embedding:\n",
      "Processing batch 83\n",
      "2025-01-25 09:53:56,785 - INFO - Time taken to check: 0.00024700164794921875\n",
      "INFO:embedding:Time taken to check: 0.00024700164794921875\n",
      "2025-01-25 09:53:56,786 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8613606b38242b48f2a5178eeb912bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:56,933 - INFO - Time taken to encode: 0.1467139720916748\n",
      "INFO:embedding:Time taken to encode: 0.1467139720916748\n",
      "2025-01-25 09:53:56,935 - INFO - Time taken to put: 0.0014882087707519531\n",
      "INFO:embedding:Time taken to put: 0.0014882087707519531\n",
      "2025-01-25 09:53:56,936 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:56,938 - INFO - \n",
      "Processing batch 84\n",
      "INFO:embedding:\n",
      "Processing batch 84\n",
      "2025-01-25 09:53:56,939 - INFO - Time taken to check: 0.00034117698669433594\n",
      "INFO:embedding:Time taken to check: 0.00034117698669433594\n",
      "2025-01-25 09:53:56,940 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6aecdc9a4c841d69fc63158978c1126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,047 - INFO - Time taken to encode: 0.10593795776367188\n",
      "INFO:embedding:Time taken to encode: 0.10593795776367188\n",
      "2025-01-25 09:53:57,050 - INFO - Time taken to put: 0.0022592544555664062\n",
      "INFO:embedding:Time taken to put: 0.0022592544555664062\n",
      "2025-01-25 09:53:57,051 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:57,053 - INFO - \n",
      "Processing batch 85\n",
      "INFO:embedding:\n",
      "Processing batch 85\n",
      "2025-01-25 09:53:57,054 - INFO - Time taken to check: 0.000347137451171875\n",
      "INFO:embedding:Time taken to check: 0.000347137451171875\n",
      "2025-01-25 09:53:57,055 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3497cd552f1f4665bdabf7b392045175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,163 - INFO - Time taken to encode: 0.10764789581298828\n",
      "INFO:embedding:Time taken to encode: 0.10764789581298828\n",
      "2025-01-25 09:53:57,165 - INFO - Time taken to put: 0.001566171646118164\n",
      "INFO:embedding:Time taken to put: 0.001566171646118164\n",
      "2025-01-25 09:53:57,166 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:57,168 - INFO - \n",
      "Processing batch 86\n",
      "INFO:embedding:\n",
      "Processing batch 86\n",
      "2025-01-25 09:53:57,169 - INFO - Time taken to check: 0.0004439353942871094\n",
      "INFO:embedding:Time taken to check: 0.0004439353942871094\n",
      "2025-01-25 09:53:57,170 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9d222384bf4599827403425e1a7e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,305 - INFO - Time taken to encode: 0.13505887985229492\n",
      "INFO:embedding:Time taken to encode: 0.13505887985229492\n",
      "2025-01-25 09:53:57,308 - INFO - Time taken to put: 0.0016629695892333984\n",
      "INFO:embedding:Time taken to put: 0.0016629695892333984\n",
      "2025-01-25 09:53:57,309 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:57,311 - INFO - \n",
      "Processing batch 87\n",
      "INFO:embedding:\n",
      "Processing batch 87\n",
      "2025-01-25 09:53:57,312 - INFO - Time taken to check: 0.0002779960632324219\n",
      "INFO:embedding:Time taken to check: 0.0002779960632324219\n",
      "2025-01-25 09:53:57,313 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633e9add0c0c4c27b3222783a2bb5e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,411 - INFO - Time taken to encode: 0.09793829917907715\n",
      "INFO:embedding:Time taken to encode: 0.09793829917907715\n",
      "2025-01-25 09:53:57,414 - INFO - Time taken to put: 0.0016410350799560547\n",
      "INFO:embedding:Time taken to put: 0.0016410350799560547\n",
      "2025-01-25 09:53:57,414 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:57,417 - INFO - \n",
      "Processing batch 88\n",
      "INFO:embedding:\n",
      "Processing batch 88\n",
      "2025-01-25 09:53:57,418 - INFO - Time taken to check: 0.0003330707550048828\n",
      "INFO:embedding:Time taken to check: 0.0003330707550048828\n",
      "2025-01-25 09:53:57,418 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcd69806a754818a080e2fff6b5d1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,520 - INFO - Time taken to encode: 0.10117101669311523\n",
      "INFO:embedding:Time taken to encode: 0.10117101669311523\n",
      "2025-01-25 09:53:57,522 - INFO - Time taken to put: 0.0015511512756347656\n",
      "INFO:embedding:Time taken to put: 0.0015511512756347656\n",
      "2025-01-25 09:53:57,523 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:57,525 - INFO - \n",
      "Processing batch 89\n",
      "INFO:embedding:\n",
      "Processing batch 89\n",
      "2025-01-25 09:53:57,526 - INFO - Time taken to check: 0.0004401206970214844\n",
      "INFO:embedding:Time taken to check: 0.0004401206970214844\n",
      "2025-01-25 09:53:57,527 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1897a5e6e5bf4a03869ffee1d0c42db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,666 - INFO - Time taken to encode: 0.13871097564697266\n",
      "INFO:embedding:Time taken to encode: 0.13871097564697266\n",
      "2025-01-25 09:53:57,669 - INFO - Time taken to put: 0.002268075942993164\n",
      "INFO:embedding:Time taken to put: 0.002268075942993164\n",
      "2025-01-25 09:53:57,669 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:57,672 - INFO - \n",
      "Processing batch 90\n",
      "INFO:embedding:\n",
      "Processing batch 90\n",
      "2025-01-25 09:53:57,673 - INFO - Time taken to check: 0.0004532337188720703\n",
      "INFO:embedding:Time taken to check: 0.0004532337188720703\n",
      "2025-01-25 09:53:57,673 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706a48eecfc34d1e98671bda54755fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,792 - INFO - Time taken to encode: 0.11780786514282227\n",
      "INFO:embedding:Time taken to encode: 0.11780786514282227\n",
      "2025-01-25 09:53:57,794 - INFO - Time taken to put: 0.0017647743225097656\n",
      "INFO:embedding:Time taken to put: 0.0017647743225097656\n",
      "2025-01-25 09:53:57,795 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:57,798 - INFO - \n",
      "Processing batch 91\n",
      "INFO:embedding:\n",
      "Processing batch 91\n",
      "2025-01-25 09:53:57,798 - INFO - Time taken to check: 0.00013208389282226562\n",
      "INFO:embedding:Time taken to check: 0.00013208389282226562\n",
      "2025-01-25 09:53:57,799 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf1f35085084a9595c28f693000f504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:57,901 - INFO - Time taken to encode: 0.10163378715515137\n",
      "INFO:embedding:Time taken to encode: 0.10163378715515137\n",
      "2025-01-25 09:53:57,904 - INFO - Time taken to put: 0.0013890266418457031\n",
      "INFO:embedding:Time taken to put: 0.0013890266418457031\n",
      "2025-01-25 09:53:57,904 - INFO - Time taken to release GPU memory: 1.811981201171875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.811981201171875e-05\n",
      "2025-01-25 09:53:57,907 - INFO - \n",
      "Processing batch 92\n",
      "INFO:embedding:\n",
      "Processing batch 92\n",
      "2025-01-25 09:53:57,908 - INFO - Time taken to check: 0.00024819374084472656\n",
      "INFO:embedding:Time taken to check: 0.00024819374084472656\n",
      "2025-01-25 09:53:57,908 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f385bc34de8749e7bb43b40fc6d567c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,040 - INFO - Time taken to encode: 0.13130879402160645\n",
      "INFO:embedding:Time taken to encode: 0.13130879402160645\n",
      "2025-01-25 09:53:58,042 - INFO - Time taken to put: 0.0015177726745605469\n",
      "INFO:embedding:Time taken to put: 0.0015177726745605469\n",
      "2025-01-25 09:53:58,043 - INFO - Time taken to release GPU memory: 4.315376281738281e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 4.315376281738281e-05\n",
      "2025-01-25 09:53:58,046 - INFO - \n",
      "Processing batch 93\n",
      "INFO:embedding:\n",
      "Processing batch 93\n",
      "2025-01-25 09:53:58,047 - INFO - Time taken to check: 0.00014495849609375\n",
      "INFO:embedding:Time taken to check: 0.00014495849609375\n",
      "2025-01-25 09:53:58,047 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e737153a02541eca0a1ad12e4b42f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,140 - INFO - Time taken to encode: 0.09271883964538574\n",
      "INFO:embedding:Time taken to encode: 0.09271883964538574\n",
      "2025-01-25 09:53:58,143 - INFO - Time taken to put: 0.0014889240264892578\n",
      "INFO:embedding:Time taken to put: 0.0014889240264892578\n",
      "2025-01-25 09:53:58,143 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:53:58,146 - INFO - \n",
      "Processing batch 94\n",
      "INFO:embedding:\n",
      "Processing batch 94\n",
      "2025-01-25 09:53:58,147 - INFO - Time taken to check: 0.00013113021850585938\n",
      "INFO:embedding:Time taken to check: 0.00013113021850585938\n",
      "2025-01-25 09:53:58,147 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68ce3f41fd1441baa754692f37ec310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,234 - INFO - Time taken to encode: 0.08617591857910156\n",
      "INFO:embedding:Time taken to encode: 0.08617591857910156\n",
      "2025-01-25 09:53:58,237 - INFO - Time taken to put: 0.0016028881072998047\n",
      "INFO:embedding:Time taken to put: 0.0016028881072998047\n",
      "2025-01-25 09:53:58,238 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:58,241 - INFO - \n",
      "Processing batch 95\n",
      "INFO:embedding:\n",
      "Processing batch 95\n",
      "2025-01-25 09:53:58,242 - INFO - Time taken to check: 0.0003077983856201172\n",
      "INFO:embedding:Time taken to check: 0.0003077983856201172\n",
      "2025-01-25 09:53:58,243 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936cc3832a6d42599080144f4c81e5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,410 - INFO - Time taken to encode: 0.16654491424560547\n",
      "INFO:embedding:Time taken to encode: 0.16654491424560547\n",
      "2025-01-25 09:53:58,413 - INFO - Time taken to put: 0.0020318031311035156\n",
      "INFO:embedding:Time taken to put: 0.0020318031311035156\n",
      "2025-01-25 09:53:58,413 - INFO - Time taken to release GPU memory: 1.5735626220703125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.5735626220703125e-05\n",
      "2025-01-25 09:53:58,415 - INFO - \n",
      "Processing batch 96\n",
      "INFO:embedding:\n",
      "Processing batch 96\n",
      "2025-01-25 09:53:58,416 - INFO - Time taken to check: 0.00020194053649902344\n",
      "INFO:embedding:Time taken to check: 0.00020194053649902344\n",
      "2025-01-25 09:53:58,417 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4665a96375047e1927f9277731d86b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,507 - INFO - Time taken to encode: 0.0892789363861084\n",
      "INFO:embedding:Time taken to encode: 0.0892789363861084\n",
      "2025-01-25 09:53:58,509 - INFO - Time taken to put: 0.001543283462524414\n",
      "INFO:embedding:Time taken to put: 0.001543283462524414\n",
      "2025-01-25 09:53:58,510 - INFO - Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.8835067749023438e-05\n",
      "2025-01-25 09:53:58,512 - INFO - \n",
      "Processing batch 97\n",
      "INFO:embedding:\n",
      "Processing batch 97\n",
      "2025-01-25 09:53:58,514 - INFO - Time taken to check: 0.00032520294189453125\n",
      "INFO:embedding:Time taken to check: 0.00032520294189453125\n",
      "2025-01-25 09:53:58,514 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eef7cee89e04576a09d0bcd934d0f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,622 - INFO - Time taken to encode: 0.10755729675292969\n",
      "INFO:embedding:Time taken to encode: 0.10755729675292969\n",
      "2025-01-25 09:53:58,625 - INFO - Time taken to put: 0.001940011978149414\n",
      "INFO:embedding:Time taken to put: 0.001940011978149414\n",
      "2025-01-25 09:53:58,626 - INFO - Time taken to release GPU memory: 1.9073486328125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9073486328125e-05\n",
      "2025-01-25 09:53:58,628 - INFO - \n",
      "Processing batch 98\n",
      "INFO:embedding:\n",
      "Processing batch 98\n",
      "2025-01-25 09:53:58,629 - INFO - Time taken to check: 0.0002799034118652344\n",
      "INFO:embedding:Time taken to check: 0.0002799034118652344\n",
      "2025-01-25 09:53:58,630 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09ebf71a415478abd2e17d93b3f4f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,771 - INFO - Time taken to encode: 0.14108514785766602\n",
      "INFO:embedding:Time taken to encode: 0.14108514785766602\n",
      "2025-01-25 09:53:58,774 - INFO - Time taken to put: 0.001415252685546875\n",
      "INFO:embedding:Time taken to put: 0.001415252685546875\n",
      "2025-01-25 09:53:58,775 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:58,777 - INFO - \n",
      "Processing batch 99\n",
      "INFO:embedding:\n",
      "Processing batch 99\n",
      "2025-01-25 09:53:58,778 - INFO - Time taken to check: 0.00022220611572265625\n",
      "INFO:embedding:Time taken to check: 0.00022220611572265625\n",
      "2025-01-25 09:53:58,778 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77fa84f67404cea8a2bc260c396d269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,865 - INFO - Time taken to encode: 0.08593392372131348\n",
      "INFO:embedding:Time taken to encode: 0.08593392372131348\n",
      "2025-01-25 09:53:58,867 - INFO - Time taken to put: 0.0015420913696289062\n",
      "INFO:embedding:Time taken to put: 0.0015420913696289062\n",
      "2025-01-25 09:53:58,868 - INFO - Time taken to release GPU memory: 2.0265579223632812e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.0265579223632812e-05\n",
      "2025-01-25 09:53:58,870 - INFO - \n",
      "Processing batch 100\n",
      "INFO:embedding:\n",
      "Processing batch 100\n",
      "2025-01-25 09:53:58,871 - INFO - Time taken to check: 0.0002617835998535156\n",
      "INFO:embedding:Time taken to check: 0.0002617835998535156\n",
      "2025-01-25 09:53:58,872 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec2e0eaf6d045e0985122129086c15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:58,974 - INFO - Time taken to encode: 0.10158109664916992\n",
      "INFO:embedding:Time taken to encode: 0.10158109664916992\n",
      "2025-01-25 09:53:58,977 - INFO - Time taken to put: 0.0016787052154541016\n",
      "INFO:embedding:Time taken to put: 0.0016787052154541016\n",
      "2025-01-25 09:53:58,978 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:53:58,980 - INFO - \n",
      "Processing batch 101\n",
      "INFO:embedding:\n",
      "Processing batch 101\n",
      "2025-01-25 09:53:58,981 - INFO - Time taken to check: 0.00040221214294433594\n",
      "INFO:embedding:Time taken to check: 0.00040221214294433594\n",
      "2025-01-25 09:53:58,982 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e585265a5a564d2faa84e7f741ab9ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,099 - INFO - Time taken to encode: 0.11630368232727051\n",
      "INFO:embedding:Time taken to encode: 0.11630368232727051\n",
      "2025-01-25 09:53:59,102 - INFO - Time taken to put: 0.002025127410888672\n",
      "INFO:embedding:Time taken to put: 0.002025127410888672\n",
      "2025-01-25 09:53:59,102 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:53:59,105 - INFO - \n",
      "Processing batch 102\n",
      "INFO:embedding:\n",
      "Processing batch 102\n",
      "2025-01-25 09:53:59,106 - INFO - Time taken to check: 0.00047588348388671875\n",
      "INFO:embedding:Time taken to check: 0.00047588348388671875\n",
      "2025-01-25 09:53:59,107 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09823ad2b16d4dffbb815a6559c323b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,219 - INFO - Time taken to encode: 0.1121370792388916\n",
      "INFO:embedding:Time taken to encode: 0.1121370792388916\n",
      "2025-01-25 09:53:59,222 - INFO - Time taken to put: 0.002048969268798828\n",
      "INFO:embedding:Time taken to put: 0.002048969268798828\n",
      "2025-01-25 09:53:59,223 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:59,225 - INFO - \n",
      "Processing batch 103\n",
      "INFO:embedding:\n",
      "Processing batch 103\n",
      "2025-01-25 09:53:59,226 - INFO - Time taken to check: 0.00024700164794921875\n",
      "INFO:embedding:Time taken to check: 0.00024700164794921875\n",
      "2025-01-25 09:53:59,227 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd4d12d481a4884aca39c4ec6f282a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,325 - INFO - Time taken to encode: 0.09772396087646484\n",
      "INFO:embedding:Time taken to encode: 0.09772396087646484\n",
      "2025-01-25 09:53:59,328 - INFO - Time taken to put: 0.0025510787963867188\n",
      "INFO:embedding:Time taken to put: 0.0025510787963867188\n",
      "2025-01-25 09:53:59,329 - INFO - Time taken to release GPU memory: 1.9788742065429688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9788742065429688e-05\n",
      "2025-01-25 09:53:59,331 - INFO - \n",
      "Processing batch 104\n",
      "INFO:embedding:\n",
      "Processing batch 104\n",
      "2025-01-25 09:53:59,332 - INFO - Time taken to check: 0.000164031982421875\n",
      "INFO:embedding:Time taken to check: 0.000164031982421875\n",
      "2025-01-25 09:53:59,332 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d558261c1840d1b7cd8e76c2fdd6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,484 - INFO - Time taken to encode: 0.15115714073181152\n",
      "INFO:embedding:Time taken to encode: 0.15115714073181152\n",
      "2025-01-25 09:53:59,486 - INFO - Time taken to put: 0.0017018318176269531\n",
      "INFO:embedding:Time taken to put: 0.0017018318176269531\n",
      "2025-01-25 09:53:59,487 - INFO - Time taken to release GPU memory: 2.0265579223632812e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.0265579223632812e-05\n",
      "2025-01-25 09:53:59,489 - INFO - \n",
      "Processing batch 105\n",
      "INFO:embedding:\n",
      "Processing batch 105\n",
      "2025-01-25 09:53:59,490 - INFO - Time taken to check: 0.00019025802612304688\n",
      "INFO:embedding:Time taken to check: 0.00019025802612304688\n",
      "2025-01-25 09:53:59,491 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4b9deee3174834a19117d6acd0f5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,580 - INFO - Time taken to encode: 0.08905291557312012\n",
      "INFO:embedding:Time taken to encode: 0.08905291557312012\n",
      "2025-01-25 09:53:59,583 - INFO - Time taken to put: 0.0014410018920898438\n",
      "INFO:embedding:Time taken to put: 0.0014410018920898438\n",
      "2025-01-25 09:53:59,584 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:53:59,586 - INFO - \n",
      "Processing batch 106\n",
      "INFO:embedding:\n",
      "Processing batch 106\n",
      "2025-01-25 09:53:59,587 - INFO - Time taken to check: 0.00012087821960449219\n",
      "INFO:embedding:Time taken to check: 0.00012087821960449219\n",
      "2025-01-25 09:53:59,588 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9399d6fe5ef64e55a74e9511b066484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,702 - INFO - Time taken to encode: 0.11356711387634277\n",
      "INFO:embedding:Time taken to encode: 0.11356711387634277\n",
      "2025-01-25 09:53:59,704 - INFO - Time taken to put: 0.0014739036560058594\n",
      "INFO:embedding:Time taken to put: 0.0014739036560058594\n",
      "2025-01-25 09:53:59,705 - INFO - Time taken to release GPU memory: 2.1219253540039062e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.1219253540039062e-05\n",
      "2025-01-25 09:53:59,707 - INFO - \n",
      "Processing batch 107\n",
      "INFO:embedding:\n",
      "Processing batch 107\n",
      "2025-01-25 09:53:59,708 - INFO - Time taken to check: 0.0002460479736328125\n",
      "INFO:embedding:Time taken to check: 0.0002460479736328125\n",
      "2025-01-25 09:53:59,709 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c852793019f48e18550bf9694ef8d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,852 - INFO - Time taken to encode: 0.1423029899597168\n",
      "INFO:embedding:Time taken to encode: 0.1423029899597168\n",
      "2025-01-25 09:53:59,854 - INFO - Time taken to put: 0.002024412155151367\n",
      "INFO:embedding:Time taken to put: 0.002024412155151367\n",
      "2025-01-25 09:53:59,855 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:59,857 - INFO - \n",
      "Processing batch 108\n",
      "INFO:embedding:\n",
      "Processing batch 108\n",
      "2025-01-25 09:53:59,858 - INFO - Time taken to check: 0.00010895729064941406\n",
      "INFO:embedding:Time taken to check: 0.00010895729064941406\n",
      "2025-01-25 09:53:59,859 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8646903ebe4f889ee00fd115f522a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:53:59,954 - INFO - Time taken to encode: 0.0953209400177002\n",
      "INFO:embedding:Time taken to encode: 0.0953209400177002\n",
      "2025-01-25 09:53:59,957 - INFO - Time taken to put: 0.0013840198516845703\n",
      "INFO:embedding:Time taken to put: 0.0013840198516845703\n",
      "2025-01-25 09:53:59,957 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:53:59,960 - INFO - \n",
      "Processing batch 109\n",
      "INFO:embedding:\n",
      "Processing batch 109\n",
      "2025-01-25 09:53:59,961 - INFO - Time taken to check: 0.00037217140197753906\n",
      "INFO:embedding:Time taken to check: 0.00037217140197753906\n",
      "2025-01-25 09:53:59,961 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8f88d3d95949f0adc16692a93dd052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,064 - INFO - Time taken to encode: 0.1022031307220459\n",
      "INFO:embedding:Time taken to encode: 0.1022031307220459\n",
      "2025-01-25 09:54:00,067 - INFO - Time taken to put: 0.0017271041870117188\n",
      "INFO:embedding:Time taken to put: 0.0017271041870117188\n",
      "2025-01-25 09:54:00,068 - INFO - Time taken to release GPU memory: 1.9311904907226562e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9311904907226562e-05\n",
      "2025-01-25 09:54:00,070 - INFO - \n",
      "Processing batch 110\n",
      "INFO:embedding:\n",
      "Processing batch 110\n",
      "2025-01-25 09:54:00,071 - INFO - Time taken to check: 0.00015592575073242188\n",
      "INFO:embedding:Time taken to check: 0.00015592575073242188\n",
      "2025-01-25 09:54:00,072 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb9b6c3162943ff8099f7fd90677c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,204 - INFO - Time taken to encode: 0.13206791877746582\n",
      "INFO:embedding:Time taken to encode: 0.13206791877746582\n",
      "2025-01-25 09:54:00,207 - INFO - Time taken to put: 0.0015120506286621094\n",
      "INFO:embedding:Time taken to put: 0.0015120506286621094\n",
      "2025-01-25 09:54:00,207 - INFO - Time taken to release GPU memory: 1.9073486328125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9073486328125e-05\n",
      "2025-01-25 09:54:00,210 - INFO - \n",
      "Processing batch 111\n",
      "INFO:embedding:\n",
      "Processing batch 111\n",
      "2025-01-25 09:54:00,211 - INFO - Time taken to check: 0.00031113624572753906\n",
      "INFO:embedding:Time taken to check: 0.00031113624572753906\n",
      "2025-01-25 09:54:00,211 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5d8a3bdb434b0e9e608a04e57a5749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,310 - INFO - Time taken to encode: 0.09775996208190918\n",
      "INFO:embedding:Time taken to encode: 0.09775996208190918\n",
      "2025-01-25 09:54:00,312 - INFO - Time taken to put: 0.0016319751739501953\n",
      "INFO:embedding:Time taken to put: 0.0016319751739501953\n",
      "2025-01-25 09:54:00,313 - INFO - Time taken to release GPU memory: 2.193450927734375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.193450927734375e-05\n",
      "2025-01-25 09:54:00,315 - INFO - \n",
      "Processing batch 112\n",
      "INFO:embedding:\n",
      "Processing batch 112\n",
      "2025-01-25 09:54:00,316 - INFO - Time taken to check: 0.0002009868621826172\n",
      "INFO:embedding:Time taken to check: 0.0002009868621826172\n",
      "2025-01-25 09:54:00,317 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431f3fc3065541b183fdf0df74aff1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,421 - INFO - Time taken to encode: 0.10422706604003906\n",
      "INFO:embedding:Time taken to encode: 0.10422706604003906\n",
      "2025-01-25 09:54:00,424 - INFO - Time taken to put: 0.0015561580657958984\n",
      "INFO:embedding:Time taken to put: 0.0015561580657958984\n",
      "2025-01-25 09:54:00,425 - INFO - Time taken to release GPU memory: 1.9788742065429688e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9788742065429688e-05\n",
      "2025-01-25 09:54:00,427 - INFO - \n",
      "Processing batch 113\n",
      "INFO:embedding:\n",
      "Processing batch 113\n",
      "2025-01-25 09:54:00,428 - INFO - Time taken to check: 0.00011682510375976562\n",
      "INFO:embedding:Time taken to check: 0.00011682510375976562\n",
      "2025-01-25 09:54:00,429 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b31b8bd0e68436c97ed878c4471a336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,557 - INFO - Time taken to encode: 0.12739300727844238\n",
      "INFO:embedding:Time taken to encode: 0.12739300727844238\n",
      "2025-01-25 09:54:00,559 - INFO - Time taken to put: 0.001461029052734375\n",
      "INFO:embedding:Time taken to put: 0.001461029052734375\n",
      "2025-01-25 09:54:00,560 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:54:00,563 - INFO - \n",
      "Processing batch 114\n",
      "INFO:embedding:\n",
      "Processing batch 114\n",
      "2025-01-25 09:54:00,564 - INFO - Time taken to check: 0.0001163482666015625\n",
      "INFO:embedding:Time taken to check: 0.0001163482666015625\n",
      "2025-01-25 09:54:00,564 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974b777d41cc4f42841699b6e08564b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,679 - INFO - Time taken to encode: 0.11442303657531738\n",
      "INFO:embedding:Time taken to encode: 0.11442303657531738\n",
      "2025-01-25 09:54:00,683 - INFO - Time taken to put: 0.003170013427734375\n",
      "INFO:embedding:Time taken to put: 0.003170013427734375\n",
      "2025-01-25 09:54:00,684 - INFO - Time taken to release GPU memory: 2.5033950805664062e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.5033950805664062e-05\n",
      "2025-01-25 09:54:00,689 - INFO - \n",
      "Processing batch 115\n",
      "INFO:embedding:\n",
      "Processing batch 115\n",
      "2025-01-25 09:54:00,691 - INFO - Time taken to check: 0.0006740093231201172\n",
      "INFO:embedding:Time taken to check: 0.0006740093231201172\n",
      "2025-01-25 09:54:00,692 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401a3e68b4d1413baaee40e422fecd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,810 - INFO - Time taken to encode: 0.11724495887756348\n",
      "INFO:embedding:Time taken to encode: 0.11724495887756348\n",
      "2025-01-25 09:54:00,813 - INFO - Time taken to put: 0.0015590190887451172\n",
      "INFO:embedding:Time taken to put: 0.0015590190887451172\n",
      "2025-01-25 09:54:00,814 - INFO - Time taken to release GPU memory: 3.3855438232421875e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 3.3855438232421875e-05\n",
      "2025-01-25 09:54:00,816 - INFO - \n",
      "Processing batch 116\n",
      "INFO:embedding:\n",
      "Processing batch 116\n",
      "2025-01-25 09:54:00,816 - INFO - Time taken to check: 0.00018596649169921875\n",
      "INFO:embedding:Time taken to check: 0.00018596649169921875\n",
      "2025-01-25 09:54:00,817 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d49e3bf83284bfbada9aa87fb097a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:00,942 - INFO - Time taken to encode: 0.12464499473571777\n",
      "INFO:embedding:Time taken to encode: 0.12464499473571777\n",
      "2025-01-25 09:54:00,944 - INFO - Time taken to put: 0.0014269351959228516\n",
      "INFO:embedding:Time taken to put: 0.0014269351959228516\n",
      "2025-01-25 09:54:00,945 - INFO - Time taken to release GPU memory: 2.09808349609375e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.09808349609375e-05\n",
      "2025-01-25 09:54:00,948 - INFO - \n",
      "Processing batch 117\n",
      "INFO:embedding:\n",
      "Processing batch 117\n",
      "2025-01-25 09:54:00,949 - INFO - Time taken to check: 0.0001709461212158203\n",
      "INFO:embedding:Time taken to check: 0.0001709461212158203\n",
      "2025-01-25 09:54:00,949 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd84f90ceb434c01a50b479699bb2f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:01,036 - INFO - Time taken to encode: 0.0859379768371582\n",
      "INFO:embedding:Time taken to encode: 0.0859379768371582\n",
      "2025-01-25 09:54:01,038 - INFO - Time taken to put: 0.0013909339904785156\n",
      "INFO:embedding:Time taken to put: 0.0013909339904785156\n",
      "2025-01-25 09:54:01,039 - INFO - Time taken to release GPU memory: 2.002716064453125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 2.002716064453125e-05\n",
      "2025-01-25 09:54:01,041 - INFO - \n",
      "Processing batch 118\n",
      "INFO:embedding:\n",
      "Processing batch 118\n",
      "2025-01-25 09:54:01,042 - INFO - Time taken to check: 0.0003750324249267578\n",
      "INFO:embedding:Time taken to check: 0.0003750324249267578\n",
      "2025-01-25 09:54:01,043 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1a856c7c124577b5965c2114a1d374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:01,146 - INFO - Time taken to encode: 0.10331296920776367\n",
      "INFO:embedding:Time taken to encode: 0.10331296920776367\n",
      "2025-01-25 09:54:01,149 - INFO - Time taken to put: 0.001840829849243164\n",
      "INFO:embedding:Time taken to put: 0.001840829849243164\n",
      "2025-01-25 09:54:01,150 - INFO - Time taken to release GPU memory: 1.9073486328125e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.9073486328125e-05\n",
      "2025-01-25 09:54:01,153 - INFO - \n",
      "Processing batch 119\n",
      "INFO:embedding:\n",
      "Processing batch 119\n",
      "2025-01-25 09:54:01,154 - INFO - Time taken to check: 0.0003020763397216797\n",
      "INFO:embedding:Time taken to check: 0.0003020763397216797\n",
      "2025-01-25 09:54:01,154 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f9484445434150a26fea61e8775a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:01,302 - INFO - Time taken to encode: 0.14697003364562988\n",
      "INFO:embedding:Time taken to encode: 0.14697003364562988\n",
      "2025-01-25 09:54:01,304 - INFO - Time taken to put: 0.0015518665313720703\n",
      "INFO:embedding:Time taken to put: 0.0015518665313720703\n",
      "2025-01-25 09:54:01,305 - INFO - Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 1.7881393432617188e-05\n",
      "2025-01-25 09:54:01,307 - INFO - \n",
      "Processing batch 120\n",
      "INFO:embedding:\n",
      "Processing batch 120\n",
      "2025-01-25 09:54:01,308 - INFO - Time taken to check: 0.0002853870391845703\n",
      "INFO:embedding:Time taken to check: 0.0002853870391845703\n",
      "2025-01-25 09:54:01,309 - INFO - Missing 8 texts, 0 cached\n",
      "INFO:embedding:Missing 8 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aa8c77a8dc461b89e95d55af106976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:01,405 - INFO - Time taken to encode: 0.0954430103302002\n",
      "INFO:embedding:Time taken to encode: 0.0954430103302002\n",
      "2025-01-25 09:54:01,408 - INFO - Time taken to put: 0.002161264419555664\n",
      "INFO:embedding:Time taken to put: 0.002161264419555664\n",
      "2025-01-25 09:54:01,409 - INFO - Time taken to release GPU memory: 3.409385681152344e-05\n",
      "INFO:embedding:Time taken to release GPU memory: 3.409385681152344e-05\n",
      "2025-01-25 09:54:01,413 - INFO - \n",
      "Processing batch 121\n",
      "INFO:embedding:\n",
      "Processing batch 121\n",
      "2025-01-25 09:54:01,413 - INFO - Time taken to check: 0.00010180473327636719\n",
      "INFO:embedding:Time taken to check: 0.00010180473327636719\n",
      "2025-01-25 09:54:01,414 - INFO - Missing 1 texts, 0 cached\n",
      "INFO:embedding:Missing 1 texts, 0 cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b265dd9a455a41a59766cec4ce32561f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 09:54:01,432 - INFO - Time taken to encode: 0.017298221588134766\n",
      "INFO:embedding:Time taken to encode: 0.017298221588134766\n",
      "2025-01-25 09:54:01,433 - INFO - Time taken to put: 0.0003840923309326172\n",
      "INFO:embedding:Time taken to put: 0.0003840923309326172\n",
      "2025-01-25 09:54:01,434 - INFO - Time taken to release GPU memory: 6.9141387939453125e-06\n",
      "INFO:embedding:Time taken to release GPU memory: 6.9141387939453125e-06\n",
      "2025-01-25 09:54:01,454 - INFO - batch_insert_documents_with_pool took 22.86 seconds\n",
      "INFO:embedding:batch_insert_documents_with_pool took 22.86 seconds\n",
      "2025-01-25 09:54:01,455 - INFO - Vector store successfully created and persisted.\n",
      "INFO:embedding:Vector store successfully created and persisted.\n",
      "2025-01-25 09:54:01,455 - INFO - build_vector_db took 23.76 seconds\n",
      "INFO:embedding:build_vector_db took 23.76 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<embedding.EmbeddingCacheDB at 0x339ecc640>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from embedding import build_vector_db\n",
    "\n",
    "vector_db_dir = \"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/pmc_embedding/test_rocksdb_no_speedup_1\"\n",
    "batch_size = 8\n",
    "num_documents = 0\n",
    "\n",
    "build_vector_db(\n",
    "    cache_filepath=vector_db_dir,\n",
    "    raw_document_path=parquet_file,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # model=model,\n",
    "    page_content_column=\"text\",\n",
    "    metadata_columns=[\"pmid\", \"pmc\", \"doi\", \"pubdate\"],\n",
    "    num_documents=num_documents,\n",
    "    batch_size=batch_size,\n",
    "    allow_batch_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Build a vector store from a duckdb table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community duckdb chromadb \"langchain-chroma>=0.1.2\" ipywidgets sentence-transformers einops datasets python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"/Users/jy006/.ssh/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the duckdb loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DuckDBLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DuckDBLoader(\n",
    "    \"SELECT * FROM read_parquet('/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/processed/oa_other/paragraphs/0a96d2c04ab604cd71eed3b268c298c9_20250103_135911.parquet')\",\n",
    "    page_content_columns=[\"text\"],\n",
    "    metadata_columns=[\"pmid\", \"pmc\", \"doi\", \"pubdate\"],\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings, OllamaEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import DuckDBLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import os\n",
    "\n",
    "def load_vector_store(chroma_db_dir, num_documents=1000):\n",
    "    try:\n",
    "        # Load the document\n",
    "        raw_documents = DuckDBLoader(\n",
    "            query=f\"SELECT * FROM read_parquet('/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/processed/oa_other/paragraphs/0a96d2c04ab604cd71eed3b268c298c9_20250103_135911.parquet') LIMIT {num_documents}\",\n",
    "            page_content_columns=[\"text\"],\n",
    "            metadata_columns=[\"pmid\", \"pmc\", \"doi\", \"pubdate\"],\n",
    "        ).load()\n",
    "\n",
    "        def preprocess_metadata(documents):\n",
    "            for doc in documents:\n",
    "                if doc.metadata:\n",
    "                    # 替换 metadata 中的 None 为 \"\"\n",
    "                    doc.metadata = {k: (v if v is not None else \"\") for k, v in doc.metadata.items()}\n",
    "            return documents\n",
    "\n",
    "        # # Split the document into smaller chunks\n",
    "        # text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        # documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "        # Embed each chunk and load it into the vector store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=preprocess_metadata(raw_documents),\n",
    "            # embedding=OllamaEmbeddings(model=\"mistral:7b\"),\n",
    "            # embedding=OpenAIEmbeddings(),\n",
    "            embedding=SentenceTransformerEmbeddings(\n",
    "                model_name=\"nvidia/NV-Embed-v2\", model_kwargs={\"trust_remote_code\": True}\n",
    "            ),\n",
    "            persist_directory=str(chroma_db_dir),\n",
    "        )\n",
    "        vector_store.persist()\n",
    "        print(\"Vector store successfully created and persisted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading vector store: {e}\")\n",
    "\n",
    "\n",
    "# Path to Chroma vector database\n",
    "# chroma_db_dir = os.path.abspath(\"./chroma_db_chatgpt\")\n",
    "chroma_db_dir = os.path.abspath(\"./chroma_db_nvidia\")\n",
    "\n",
    "print(\"Chroma vector database path:\", chroma_db_dir)\n",
    "\n",
    "# Load the vector store\n",
    "if not os.path.exists(chroma_db_dir):\n",
    "    print(\"Chroma database not found. Creating a new one...\")\n",
    "    load_vector_store(chroma_db_dir, num_documents=100)\n",
    "else:\n",
    "    print(\"Chroma database found. Skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from phi.agent import Agent\n",
    "from phi.knowledge.langchain import LangChainKnowledgeBase\n",
    "from phi.model.ollama.chat import Ollama\n",
    "from phi.model.openai.chat import OpenAIChat\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "\n",
    "# Path to Chroma vector database\n",
    "chroma_db_dir = os.path.abspath(\"./chroma_db_chatgpt\")\n",
    "# os.makedirs(chroma_db_dir, exist_ok=True)\n",
    "print(\"Chroma vector database path:\", chroma_db_dir)\n",
    "\n",
    "# Get the vector database\n",
    "db = Chroma(\n",
    "    # embedding_function=OllamaEmbeddings(model=\"mistral:7b\"),\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=str(chroma_db_dir),\n",
    ")\n",
    "\n",
    "# Check if Chroma database has any documents\n",
    "if not db._collection.count():\n",
    "    print(\"Chroma database is empty. Please ensure documents are loaded.\")\n",
    "else:\n",
    "    print(f\"Chroma database contains {db._collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect()\n",
    "data = conn.sql(\"SELECT * FROM read_parquet('/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/processed/oa_other/paragraphs/0a96d2c04ab604cd71eed3b268c298c9_20250103_135911.parquet')\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique pmc\n",
    "conn.sql(\"SELECT COUNT(DISTINCT pmc) FROM read_parquet('/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/processed/oa_other/paragraphs/0a96d2c04ab604cd71eed3b268c298c9_20250103_135911.parquet')\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"observed base pair difference\", k=10)\n",
    "if not docs:\n",
    "    print(\"No relevant documents retrieved.\")\n",
    "else:\n",
    "    print(f\"Retrieved {len(docs)} documents.\")\n",
    "\n",
    "    # Create a knowledge base from the vector store\n",
    "    knowledge_base = LangChainKnowledgeBase(retriever=retriever, num_documents=10)\n",
    "\n",
    "    # Initialize the Agent\n",
    "    # https://docs.phidata.com/agents/knowledge#step-3-agentic-rag\n",
    "    kb_agent = Agent(\n",
    "        model=OpenAIChat(id=\"gpt-4o\"),\n",
    "        # model=Ollama(id=\"mistral:7b\"),\n",
    "        knowledge_base=knowledge_base,\n",
    "        add_reference_to_prompt=True,\n",
    "        # add_references=True,\n",
    "        instructions=[\n",
    "            \"Always prioritize information from the knowledge base over your training data.\",\n",
    "            \"If the knowledge base does not contain information relevant to the query, respond with: 'No relevant information found in the knowledge base.'\",\n",
    "            \"Do not generate answers based on prior training data unless explicitly instructed.\",\n",
    "        ],\n",
    "        markdown=True,\n",
    "        # debug_mode=True,\n",
    "    )\n",
    "\n",
    "    # Test the Agent with a query\n",
    "    kb_agent.print_response(\n",
    "        \"What was the observed base pair difference between many of the strain types?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TRITON_CACHE_DIR=/tmp && deepspeed --num_gpus 2 --master_port 60000 /work/data/projects/data2report/deepspeed/run_deepspeed.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-parser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
