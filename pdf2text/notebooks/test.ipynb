{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML and get metadata, paragraphs, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = [\"scripts/build_database.py\"]\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/jy006/Documents/Code/BioMedGPS/paper-parser\")\n",
    "\n",
    "from scripts.build_database import process_article\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfile = \"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/extracted/oa_comm/oa_comm_xml.incr.2024-12-19/PMC006xxxxxx/PMC6627345\n",
    "        \n",
    "        \n",
    "        \n",
    "        .xml\"\n",
    "\n",
    "tree = ET.parse(xmlfile)\n",
    "root = tree.getroot()\n",
    "articles = root.findall('.//article')\n",
    "\n",
    "metadata, paragraphs, sentences = process_article(root)\n",
    "\n",
    "print(f\"Found {len(metadata)} metadata, {len(paragraphs)} paragraphs, {len(sentences)} sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfile = \"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/extracted/oa_comm/oa_comm_xml.incr.2024-12-25/PMC006xxxxxx/PMC6798053\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        .xml\"\n",
    "tree = ET.parse(xmlfile)\n",
    "root = tree.getroot()\n",
    "articles = root.findall('.//article')\n",
    "\n",
    "metadata, paragraphs, sentences = process_article(root)\n",
    "\n",
    "print(f\"Found {len(metadata)} metadata, {len(paragraphs)} paragraphs, {len(sentences)} sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/processed/oa_comm/metadata/bb/bbbc3b8cc66a2adf1e460ef57ce1f5f2_20250103_152209.parquet\"\n",
    "\n",
    "# Attempt to read the Parquet file and display the first few rows\n",
    "try:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "data = json.load(open(\"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/papers.json\"))\n",
    "\n",
    "pmids = [d.get(\"pmid\") for d in data]\n",
    "\n",
    "for pmid in pmids:\n",
    "    if not os.path.exists(os.path.join(\"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody\", \"pdfs\", f\"{pmid}.pdf\")):\n",
    "        print(f\"Cannot find {pmid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect()\n",
    "data = conn.sql(\n",
    "    \"SELECT COUNT(*) FROM read_parquet('/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/text_chunks/paragraphs.parquet')\"\n",
    ")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of records in the rocksdb db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rocksdb\n",
    "\n",
    "# æŒ‡å®šæ•°æ®åº“è·¯å¾„\n",
    "# root_dir = \"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/\"\n",
    "# db_path = os.path.join(\n",
    "#     root_dir, \"benchmarks\", \"antibody\", \"pmc_embedding\", \"test_rocksdb_no_speedup\"\n",
    "# )\n",
    "\n",
    "db_path = \"/work/data/projects/data2report/minillm-rocksdb-2025012500\"\n",
    "\n",
    "db = rocksdb.DB(\n",
    "    db_path,\n",
    "    rocksdb.Options(create_if_missing=False),\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–è®¡æ•°å™¨\n",
    "num_records = 0\n",
    "first_ten_records = []\n",
    "\n",
    "# åˆ›å»ºè¿­ä»£å™¨\n",
    "it = db.iterkeys()\n",
    "it.seek_to_first()\n",
    "\n",
    "# éå†æ•°æ®åº“\n",
    "for key in it:\n",
    "    num_records += 1\n",
    "    # è®°å½•å‰10æ¡æ•°æ®\n",
    "    if num_records <= 10:\n",
    "        value = db.get(key)\n",
    "        first_ten_records.append((key, value))\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(f\"Number of records in the RocksDB database: {num_records}\")\n",
    "print(\"First 10 records:\")\n",
    "for i, (key, value) in enumerate(first_ten_records, 1):\n",
    "    print(f\"{i}: Key = {key}, Value = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplicate the embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointIdsList\n",
    "\n",
    "client = QdrantClient(path=\"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/pmc_embedding/minillm-vector-store\")\n",
    "\n",
    "# è·å–æ‰€æœ‰æ•°æ®\n",
    "all_points = []\n",
    "next_page = None\n",
    "\n",
    "while True:\n",
    "    response = client.scroll(\n",
    "        collection_name=\"network-medicine\",\n",
    "        limit=1000,\n",
    "        offset=next_page,\n",
    "        with_payload=True,\n",
    "        with_vectors=True,\n",
    "    )\n",
    "    all_points.extend(response[0])\n",
    "    next_page = response[1]\n",
    "    if next_page is None:\n",
    "        break\n",
    "\n",
    "# å»é‡é€»è¾‘\n",
    "unique_points = {}\n",
    "for point in all_points:\n",
    "    text = point.payload.get(\"text\")\n",
    "    if text not in unique_points:\n",
    "        unique_points[text] = point\n",
    "\n",
    "# è·å–å»é‡åçš„æ•°æ®\n",
    "unique_points_list = list(unique_points.values())\n",
    "\n",
    "# å¯é€‰ï¼šåˆ é™¤é‡å¤æ•°æ®\n",
    "unique_ids = {point.id for point in unique_points_list}\n",
    "duplicate_ids = [point.id for point in all_points if point.id not in unique_ids]\n",
    "if duplicate_ids:\n",
    "    # æ³¨æ„ï¼Œè¿™é‡Œè¦ä½¿ç”¨ PointIdsListï¼Œè€Œä¸æ˜¯ç›´æ¥ç”¨ dict\n",
    "    client.delete(\n",
    "        collection_name=\"network-medicine\",\n",
    "        points_selector=PointIdsList(points=duplicate_ids),\n",
    "    )\n",
    "\n",
    "# å»é‡åçš„æ•°æ®é‡æ–°å­˜å‚¨\n",
    "for point in unique_points_list:\n",
    "    client.upsert(\n",
    "        collection_name=\"network-medicine\",\n",
    "        points=[\n",
    "            {\n",
    "                \"id\": point.id,\n",
    "                \"vector\": point.vector,\n",
    "                \"payload\": point.payload,\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "print(f\"å»é‡å®Œæˆï¼Œå…±ä¿ç•™ {len(unique_points_list)} æ¡è®°å½•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RocksDB to DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å•çº¿ç¨‹å¤„ç† RocksDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import rocksdb\n",
    "import pandas as pd\n",
    "import orjson\n",
    "import os\n",
    "\n",
    "# RocksDB æ•°æ®åº“è·¯å¾„\n",
    "rocksdb_path = \"/work/data/projects/data2report/minillm-rocksdb-2025012500\"\n",
    "\n",
    "# å­˜å‚¨ Parquet ç›®å½•\n",
    "parquet_dir = \"/work/data/projects/data2report/parquet_embeddings\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "# è¿æ¥ RocksDB\n",
    "try:\n",
    "    db = rocksdb.DB(rocksdb_path, rocksdb.Options(create_if_missing=False), read_only=True)\n",
    "except Exception as e:\n",
    "    print(f\"æ— æ³•æ‰“å¼€ RocksDB: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "batch_size = 1_000_000  # æ¯ 100 ä¸‡æ¡æ•°æ®å­˜ä¸€ä¸ª Parquet\n",
    "batch = []\n",
    "file_index = 0  # è®°å½•å½“å‰ Parquet æ–‡ä»¶ç¼–å·\n",
    "\n",
    "# è¯»å– RocksDB è¿­ä»£å™¨\n",
    "it = db.iterkeys()\n",
    "it.seek_to_first()\n",
    "\n",
    "processed_count = 0  # è®°å½•å·²å¤„ç†çš„æ•°æ®é‡\n",
    "log_interval = 100000  # æ¯ 10 ä¸‡æ¡è®°å½•è¾“å‡ºæ—¥å¿—\n",
    "\n",
    "for key in it:\n",
    "    value = db.get(key)  # è·å– Value\n",
    "    key_str = key.decode()  # è§£ç  Key\n",
    "    \n",
    "    try:\n",
    "        # è§£æ JSONï¼Œç¡®ä¿å­˜å‚¨ä¸º `LIST<FLOAT>` è€Œä¸æ˜¯ `TEXT`\n",
    "        embedding_list = orjson.loads(value)\n",
    "        if isinstance(embedding_list, list) and all(isinstance(x, (float, int)) for x in embedding_list):\n",
    "            batch.append((key_str, embedding_list))\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"è·³è¿‡æ— æ•ˆæ•°æ®: {key_str}\")\n",
    "        \n",
    "    processed_count += 1  # æ¯å¤„ç† 1 æ¡æ•°æ®ï¼Œè®¡æ•° +1\n",
    "\n",
    "    # æ¯ 10 ä¸‡æ¡æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    if processed_count % log_interval == 0:\n",
    "        print(f\"ğŸ”¹ å·²å¤„ç† {processed_count} æ¡æ•°æ®\")\n",
    "\n",
    "    # æ¯ batch_size æ¡æ•°æ®å­˜ä¸€ä¸ª Parquet\n",
    "    if len(batch) >= batch_size:\n",
    "        parquet_file = os.path.join(parquet_dir, f\"embeddings_{file_index}.parquet\")\n",
    "        df = pd.DataFrame(batch, columns=[\"key\", \"embedding\"])\n",
    "        df.to_parquet(parquet_file, compression=\"zstd\", engine=\"pyarrow\")\n",
    "        print(f\"âœ… å·²ä¿å­˜ {parquet_file}\")\n",
    "        batch = []\n",
    "        file_index += 1\n",
    "\n",
    "# å¤„ç†å‰©ä½™æ•°æ®\n",
    "if batch:\n",
    "    parquet_file = os.path.join(parquet_dir, f\"embeddings_{file_index}.parquet\")\n",
    "    df = pd.DataFrame(batch, columns=[\"key\", \"embedding\"])\n",
    "    df.to_parquet(parquet_file, compression=\"zstd\", engine=\"pyarrow\")\n",
    "    print(f\"âœ… å·²ä¿å­˜ {parquet_file}\")\n",
    "\n",
    "print(\"ğŸ”¹ æ‰€æœ‰æ•°æ®å·²æˆåŠŸå­˜ä¸º Parquetï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rocksdb\n",
    "import duckdb\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def process_record(record):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•æ¡è®°å½•ï¼šå°† key è§£ç ï¼Œå°† JSON æ ¼å¼çš„ value è§£ææˆ Python æ•°å€¼åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    key, value = record\n",
    "    try:\n",
    "        # key å¤„ç†ï¼šå¦‚æœæ˜¯ bytesï¼Œåˆ™è§£ç ä¸ºå­—ç¬¦ä¸²\n",
    "        key_str = key.decode() if isinstance(key, bytes) else str(key)\n",
    "        # value å¤„ç†ï¼šå…ˆè§£ç ï¼Œå†ç”¨ json è§£æ\n",
    "        json_text = value.decode() if isinstance(value, bytes) else str(value)\n",
    "        vector = orjson.loads(json_text)\n",
    "        if not isinstance(vector, list):\n",
    "            raise ValueError(\"è§£æåçš„å‘é‡ä¸æ˜¯åˆ—è¡¨\")\n",
    "        # if len(vector) != 387:\n",
    "        #     # å¯è®°å½•è­¦å‘Šï¼Œä½†ä¸é˜»æ­¢åç»­å¤„ç†\n",
    "        #     print(f\"è­¦å‘Šï¼škey {key_str} çš„å‘é‡ç»´åº¦ä¸º {len(vector)}ï¼ŒæœŸæœ› 387\")\n",
    "        return (key_str, vector)\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç† key {key} æ—¶å‡ºé”™ï¼š{e}\")\n",
    "        return None\n",
    "\n",
    "def migrate_rocksdb_to_duckdb(rocksdb_path, duckdb_path, batch_size=10000, num_workers=40):\n",
    "    \"\"\"\n",
    "    å°† RocksDB ä¸­çš„æ•°æ®æ‰¹é‡è¿ç§»åˆ° DuckDB ä¸­ï¼Œåˆ©ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè§£æ JSONï¼Œ\n",
    "    å¹¶å¯¹ DuckDB åšç›¸åº”çš„æ€§èƒ½ä¼˜åŒ–ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "      rocksdb_path: RocksDB æ•°æ®åº“è·¯å¾„\n",
    "      duckdb_path: ç›®æ ‡ DuckDB æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨æ—¶ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰\n",
    "      batch_size: æ¯æ‰¹æ¬¡å¤„ç†çš„è®°å½•æ•°\n",
    "      num_workers: å¹¶è¡Œå¤„ç†æ—¶çš„è¿›ç¨‹æ•°ï¼ˆå¯æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼‰\n",
    "    \"\"\"\n",
    "    # è®¾ç½® RocksDB é€‰é¡¹ï¼Œå¹¶ä»¥åªè¯»æ–¹å¼æ‰“å¼€æ•°æ®åº“\n",
    "    opts = rocksdb.Options()\n",
    "    opts.create_if_missing = False\n",
    "    db = rocksdb.DB(rocksdb_path, opts, read_only=True)\n",
    "    \n",
    "    # è¿æ¥ DuckDBï¼Œå¹¶è®¾ç½®é«˜å¹¶å‘ç›¸å…³çš„ PRAGMA å‚æ•°\n",
    "    con = duckdb.connect(duckdb_path)\n",
    "    con.execute(\"PRAGMA threads=80;\")               # ä½¿ç”¨ 80 ä¸ªçº¿ç¨‹ï¼ˆå……åˆ†åˆ©ç”¨ CPUï¼‰\n",
    "    con.execute(\"PRAGMA memory_limit='120GB';\")       # æé«˜å†…å­˜é™åˆ¶ï¼Œé¿å…å†…å­˜ä¸è¶³\n",
    "    # åˆ›å»ºç›®æ ‡è¡¨ï¼Œé‡‡ç”¨ key (VARCHAR) å’Œ vector (DOUBLE[]) å­˜å‚¨\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS vectors (\n",
    "        key VARCHAR,\n",
    "        vector DOUBLE[]\n",
    "    );\n",
    "    \"\"\"\n",
    "    con.execute(create_table_sql)\n",
    "    \n",
    "    # è·å– RocksDB è¿­ä»£å™¨ï¼Œé¡ºåºéå†æ‰€æœ‰è®°å½•\n",
    "    it = db.iteritems()\n",
    "    it.seek_to_first()\n",
    "    \n",
    "    batch_records = []   # ç”¨äºå­˜å‚¨å½“å‰æ‰¹æ¬¡çš„ (key, value) å…ƒç»„\n",
    "    total_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ä½¿ç”¨ ProcessPoolExecutor è¿›è¡Œå¤šè¿›ç¨‹ JSON è§£æ\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        def process_batch(records):\n",
    "            # å¹¶è¡Œè§£æå½“å‰æ‰¹æ¬¡è®°å½•\n",
    "            results = list(executor.map(process_record, records))\n",
    "            # è¿‡æ»¤æ‰è§£æå¤±è´¥çš„è®°å½•\n",
    "            return [r for r in results if r is not None]\n",
    "        \n",
    "        print(\"å¼€å§‹æ•°æ®è¿ç§»...\")\n",
    "        for key, value in it:\n",
    "            batch_records.append((key, value))\n",
    "            total_count += 1\n",
    "            if total_count % batch_size == 0:\n",
    "                processed = process_batch(batch_records)\n",
    "                if processed:\n",
    "                    # åˆ©ç”¨ Pandas DataFrame ç»„ç»‡æ‰¹æ¬¡æ•°æ®\n",
    "                    df = pd.DataFrame(processed, columns=[\"key\", \"vector\"])\n",
    "                    # åˆ©ç”¨ DuckDB çš„ register/INSERT æ–¹å¼æ‰¹é‡å†™å…¥æ•°æ®\n",
    "                    con.register(\"tmp_df\", df)\n",
    "                    con.execute(\"INSERT INTO vectors SELECT * FROM tmp_df\")\n",
    "                    con.unregister(\"tmp_df\")\n",
    "                print(f\"å·²å¤„ç†å¹¶æ’å…¥ {total_count} æ¡è®°å½•ï¼Œè€—æ—¶ {time.time()-start_time:.2f} ç§’\")\n",
    "                batch_records = []  # æ¸…ç©ºå½“å‰æ‰¹æ¬¡æ•°æ®\n",
    "        \n",
    "        # å¤„ç†æœ€åä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\n",
    "        if batch_records:\n",
    "            processed = process_batch(batch_records)\n",
    "            if processed:\n",
    "                df = pd.DataFrame(processed, columns=[\"key\", \"vector\"])\n",
    "                con.register(\"tmp_df\", df)\n",
    "                con.execute(\"INSERT INTO vectors SELECT * FROM tmp_df\")\n",
    "                con.unregister(\"tmp_df\")\n",
    "            print(f\"æœ€ç»ˆæ‰¹æ¬¡å¤„ç†å®Œæ¯•ï¼Œæ€»è®¡ {total_count} æ¡è®°å½•\")\n",
    "    \n",
    "    con.close()\n",
    "    print(\"æ•°æ®è¿ç§»å®Œæˆã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ RocksDB æ•°æ®åº“è·¯å¾„å’Œç›®æ ‡ DuckDB æ–‡ä»¶è·¯å¾„\n",
    "    rocksdb_path = \"/work/data/projects/data2report/minillm-rocksdb-2025012500\"         # ä¾‹å¦‚ï¼š\"/data/rocksdb_db\"\n",
    "    duckdb_path = \"/work/data/projects/data2report/paragraph-minillm-embedding-2025012500.duckdb\"  # ä¾‹å¦‚ï¼š\"/data/target.duckdb\"\n",
    "    \n",
    "    # æ ¹æ®æœåŠ¡å™¨é…ç½®è°ƒæ•´ batch_size å’Œ num_workers å‚æ•°\n",
    "    migrate_rocksdb_to_duckdb(rocksdb_path, duckdb_path, batch_size=10000, num_workers=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤šçº¿ç¨‹å¤„ç† RocksDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import rocksdb\n",
    "import pandas as pd\n",
    "import orjson\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# RocksDB æ•°æ®åº“è·¯å¾„\n",
    "rocksdb_path = \"/work/data/projects/data2report/minillm-rocksdb-2025012500\"\n",
    "\n",
    "# å­˜å‚¨ Parquet ç›®å½•\n",
    "parquet_dir = \"/work/data/projects/data2report/paragraph-minillm-embedding-2025012500\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "# è¿æ¥ RocksDB\n",
    "try:\n",
    "    options = rocksdb.Options()\n",
    "    \n",
    "    options.create_if_missing = True\n",
    "    options.error_if_exists = False\n",
    "    options.paranoid_checks = True\n",
    "    \n",
    "    # MemTable é…ç½®\n",
    "    options.write_buffer_size = 1024 * 1024 * 1024 * 8\n",
    "    options.max_write_buffer_number = 3\n",
    "    options.min_write_buffer_number_to_merge = 3\n",
    "\n",
    "    # Compaction é…ç½®\n",
    "    options.compression = rocksdb.CompressionType.snappy_compression\n",
    "\n",
    "    # Block Cache é…ç½®\n",
    "    block_cache = rocksdb.LRUCache(1024 * 1024 * 1024 * 50)\n",
    "    table_options = rocksdb.BlockBasedTableFactory(\n",
    "        block_cache=block_cache,\n",
    "    )\n",
    "    options.table_factory = table_options\n",
    "\n",
    "    # options.use_direct_reads = True\n",
    "    # options.allow_mmap_reads = True\n",
    "    # åå°çº¿ç¨‹é…ç½®\n",
    "    options.max_background_flushes = 10\n",
    "    options.max_background_compactions = 10\n",
    "    db = rocksdb.DB(rocksdb_path, options, read_only=True)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ— æ³•æ‰“å¼€ RocksDB: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# å‚æ•°è®¾ç½®\n",
    "skip_top_k = 0    # è·³è¿‡å‰ 0 æ¡æ•°æ®\n",
    "batch_size = 1_000_000    # æ¯ 100 ä¸‡æ¡æ•°æ®åšä¸€æ¬¡ multi_get\n",
    "sub_batch_size = 100_000  # æ¯ 10 ä¸‡æ¡æ•°æ®äº¤ç»™ä¸€ä¸ªçº¿ç¨‹å¤„ç†ï¼ˆç”Ÿæˆä¸€ä¸ª Parquet æ–‡ä»¶ï¼‰\n",
    "log_interval = 100_000    # æ¯ 10 ä¸‡æ¡è®°å½•è¾“å‡ºæ—¥å¿—\n",
    "thread_count = 10         # å·¥ä½œçº¿ç¨‹æ•°ï¼Œå¯æ ¹æ® CPU èµ„æºè°ƒæ•´\n",
    "task_queue = Queue(maxsize=10)  # é™åˆ¶ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦ï¼Œé˜²æ­¢ä»»åŠ¡å †ç§¯\n",
    "\n",
    "def process_batch(keys, values, file_idx):\n",
    "    \"\"\"\n",
    "    å¤„ç† sub_batch_size æ¡æ•°æ®å¹¶å­˜ä¸º Parquetã€‚\n",
    "    æ¯ä¸ªä»»åŠ¡å¤„ç† 10 ä¸‡æ¡æ•°æ®ï¼ˆæˆ–ä¸è¶³ 10 ä¸‡æ¡ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    decoded_values = [values.get(key).decode() for key in keys]\n",
    "    decoded_keys = [key.decode() for key in keys]\n",
    "    batch = [(key, orjson.loads(value)) for key, value in zip(decoded_keys, decoded_values)]\n",
    "\n",
    "    # å‡è®¾ batch æ˜¯ä¸€ä¸ª [(key, embedding_list), ...] çš„åˆ—è¡¨\n",
    "    keys_list, embeddings_list = zip(*batch)  # åˆ†ç¦» keys å’Œ embeddings\n",
    "    # æ„å»º pyarrow æ•°ç»„\n",
    "    key_array = pa.array(keys_list)\n",
    "    embedding_array = pa.array(embeddings_list)  # å¦‚æœæ¯ä¸ª embedding é•¿åº¦ä¸€è‡´ï¼Œåˆ™å¯ä»¥æ„å»ºäºŒç»´æ•°ç»„ï¼›å¦åˆ™ï¼Œæ„å»º ListArray\n",
    "\n",
    "    parquet_file = os.path.join(parquet_dir, f\"embeddings_{file_idx}.parquet\")\n",
    "    table = pa.Table.from_arrays([key_array, embedding_array], names=['key', 'embedding'])\n",
    "    pq.write_table(table, parquet_file, compression='snappy')\n",
    "    print(f\"âœ… å·²ä¿å­˜ {parquet_file}, è€—æ—¶ {time.time() - start_time:.2f} ç§’\")\n",
    "\n",
    "def worker():\n",
    "    \"\"\"å·¥ä½œçº¿ç¨‹å‡½æ•°ï¼šä¸æ–­ä»ä»»åŠ¡é˜Ÿåˆ—ä¸­å–ä»»åŠ¡æ‰§è¡Œï¼Œé‡åˆ° None æ—¶é€€å‡º\"\"\"\n",
    "    while True:\n",
    "        task = task_queue.get()\n",
    "        if task is None:\n",
    "            task_queue.task_done()\n",
    "            break\n",
    "        sub_keys, sub_values, file_idx = task\n",
    "        process_batch(sub_keys, sub_values, file_idx)\n",
    "        task_queue.task_done()\n",
    "\n",
    "# å¯åŠ¨å·¥ä½œçº¿ç¨‹\n",
    "threads = []\n",
    "for _ in range(thread_count):\n",
    "    t = threading.Thread(target=worker, daemon=True)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "# è¯»å– RocksDB æ•°æ®\n",
    "it = db.iterkeys()\n",
    "it.seek_to_first()\n",
    "\n",
    "keys = []\n",
    "processed_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        key = next(it)  # è·å–ä¸‹ä¸€ä¸ª key\n",
    "        processed_count += 1\n",
    "\n",
    "        # åœ¨è·³è¿‡é˜¶æ®µä¸ç´¯ç§¯ key\n",
    "        if processed_count <= skip_top_k:\n",
    "            continue\n",
    "\n",
    "        keys.append(key)\n",
    "\n",
    "        # æ¯ 10 ä¸‡æ¡æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "        if processed_count % log_interval == 0:\n",
    "            print(f\"ğŸ”¹ å·²è¯»å– {processed_count} æ¡æ•°æ®ï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’\")\n",
    "\n",
    "        if len(keys) >= batch_size:\n",
    "            start_time_batch = time.time()\n",
    "            values = db.multi_get(keys)  # æ‰¹é‡æŸ¥è¯¢ RocksDBï¼Œå‡å°‘ I/O\n",
    "            print(f\"â³ multi_get æŸ¥è¯¢ {len(keys)} æ¡æ•°æ®ï¼Œè€—æ—¶ {time.time() - start_time_batch:.2f} ç§’\")\n",
    "\n",
    "            # è¿‡æ»¤æ‰ None å€¼ï¼Œç¡®ä¿æ•°æ®å®Œæ•´\n",
    "            values = {k: v for k, v in values.items() if v is not None}\n",
    "\n",
    "            batch_index = 0\n",
    "            for i in range(0, batch_size, sub_batch_size):\n",
    "                sub_keys = keys[i:i + sub_batch_size]\n",
    "                sub_values = {k: values[k] for k in sub_keys if k in values}\n",
    "                batch_index += 1\n",
    "                task_queue.put((sub_keys, sub_values, f\"{processed_count}_{batch_index}\"))\n",
    "                print(f\"âœ… å·²æäº¤ {len(sub_keys)} æ¡æ•°æ®åˆ°çº¿ç¨‹æ± \")\n",
    "            keys = []  # æ¸…ç©º keys åˆ—è¡¨ï¼Œå‡†å¤‡ä¸‹ä¸€æ‰¹æ•°æ®\n",
    "\n",
    "    except StopIteration:\n",
    "        # éå†ç»“æŸ\n",
    "        break\n",
    "\n",
    "# å¤„ç†å‰©ä½™æ•°æ®ï¼ˆä¸è¶³ batch_size çš„éƒ¨åˆ†ï¼‰\n",
    "if keys:\n",
    "    start_time_sub = time.time()\n",
    "    values = db.multi_get(keys)\n",
    "    print(f\"â³ multi_get æŸ¥è¯¢ {len(keys)} æ¡æ•°æ®ï¼Œè€—æ—¶ {time.time() - start_time_sub:.2f} ç§’\")\n",
    "    for i in range(0, len(keys), sub_batch_size):\n",
    "        sub_keys = keys[i:i + sub_batch_size]\n",
    "        sub_values = {k: values[k] for k in sub_keys if k in values}\n",
    "        task_queue.put((sub_keys, sub_values, processed_count + i))\n",
    "        print(f\"âœ… å·²æäº¤ {len(sub_keys)} æ¡æ•°æ®åˆ°çº¿ç¨‹æ± \")\n",
    "\n",
    "# å‘æ¯ä¸ªå·¥ä½œçº¿ç¨‹å‘é€ç»ˆæ­¢ä¿¡å·\n",
    "for _ in range(thread_count):\n",
    "    task_queue.put(None)\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "task_queue.join()\n",
    "\n",
    "print(\"ğŸ”¹ æ‰€æœ‰æ•°æ®å·²æˆåŠŸæäº¤åˆ°çº¿ç¨‹æ± å¤„ç†ï¼\")\n",
    "\n",
    "# å¦‚æœéœ€è¦ç­‰å¾…çº¿ç¨‹é€€å‡ºï¼ˆè™½ç„¶ daemon çº¿ç¨‹åœ¨ä¸»çº¿ç¨‹é€€å‡ºæ—¶ä¼šè¢«å¼ºè¡Œç»“æŸï¼‰ï¼Œå¯ä»¥ joinï¼š\n",
    "for t in threads:\n",
    "    t.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åˆå¹¶ Parquet æ–‡ä»¶åˆ° DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import duckdb\n",
    "\n",
    "# Merge all parquet files into a duckdb database\n",
    "parquet_dir = \"/work/data/projects/data2report/paragraph-minillm-embedding-2025012500\"\n",
    "\n",
    "# è¿æ¥ DuckDB\n",
    "duckdb_path = \"/work/data/projects/data2report/paragraph-minillm-embedding-2025012500.duckdb\"\n",
    "con = duckdb.connect(duckdb_path)\n",
    "\n",
    "con.execute(\"PRAGMA threads=80;\")\n",
    "con.execute(\"PRAGMA memory_limit='120GB';\")\n",
    "\n",
    "embedding_size = 384\n",
    "\n",
    "# åˆ›å»º DuckDB è¡¨\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "        key VARCHAR(32) UNIQUE,\n",
    "        embedding FLOAT[{embedding_size}]\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "def extract_numbers(s):\n",
    "    \"\"\"æå–æ–‡ä»¶åä¸­çš„æ•°å­—å¹¶è½¬æ¢ä¸ºå…ƒç»„\"\"\"\n",
    "    numbers = re.findall(r'\\d+', s)  # æå–æ‰€æœ‰æ•°å­—\n",
    "    return tuple(map(int, numbers))  # è½¬æ¢ä¸ºæ•´æ•°å…ƒç»„è¿›è¡Œæ’åº\n",
    "\n",
    "# æ‰¹é‡å¯¼å…¥æ‰€æœ‰ Parquet\n",
    "parquet_files = sorted([os.path.join(parquet_dir, f) for f in os.listdir(parquet_dir) if f.endswith(\".parquet\")], key=extract_numbers)\n",
    "\n",
    "for parquet_file in parquet_files:\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO embeddings (key, embedding)\n",
    "        SELECT * FROM read_parquet('{parquet_file}')\n",
    "    \"\"\")\n",
    "    print(f\"âœ… å·²åˆå¹¶ {parquet_file}\")\n",
    "\n",
    "# **åˆ›å»ºç´¢å¼•**\n",
    "con.execute(\"CREATE UNIQUE INDEX idx_key ON embeddings(key);\")\n",
    "\n",
    "# å…³é—­æ•°æ®åº“\n",
    "con.close()\n",
    "\n",
    "print(\"ğŸ‰ æ‰€æœ‰ Parquet æ–‡ä»¶å·²åˆå¹¶åˆ° DuckDBï¼Œå¹¶åˆ›å»ºç´¢å¼•ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "\n",
    "root_dir = \"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk\"\n",
    "conn = duckdb.connect()\n",
    "\n",
    "sql = f\"\"\"\n",
    "PRAGMA threads=80; -- å¯ç”¨å¤šçº¿ç¨‹\n",
    "PRAGMA memory_limit='120GB'; -- é™åˆ¶å†…å­˜å ç”¨\n",
    "\n",
    "ATTACH '{root_dir}/paragraph-minillm-embedding-2025012500.duckdb' AS db;\n",
    "CREATE TABLE db.embeddings AS \n",
    "SELECT * FROM read_parquet(['{os.path.join(root_dir, \"paragraph-minillm-embedding-2025012500\")}/*.parquet']);\n",
    "\"\"\"\n",
    "\n",
    "conn.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# åˆå§‹åŒ– Qdrant clients\n",
    "target_client = QdrantClient(path=\"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/pmc_embedding/nvembed-vector_db\")\n",
    "source_client = QdrantClient(path=\"/Users/jy006/Documents/Code/BioMedGPS/paper-parser/benchmarks/antibody/pmc_embedding/benchmark_datasets-vector_db\")\n",
    "\n",
    "# åˆå¹¶æ•°æ®\n",
    "offset = None  # åˆå§‹åŒ– offset\n",
    "is_end = False\n",
    "while True:\n",
    "    # æ‰¹é‡è·å–æ•°æ®\n",
    "    points, offset = source_client.scroll(\n",
    "        collection_name=\"network-medicine\",\n",
    "        limit=100,          # æ¯æ¬¡è·å– 100 æ¡\n",
    "        offset=offset,      # ä¸Šæ¬¡è¿”å›çš„åç§»é‡\n",
    "        with_payload=True,  # åŒ…å« payload\n",
    "        with_vectors=True,  # åŒ…å«å‘é‡\n",
    "    )\n",
    "    \n",
    "    if len(points) < 100:\n",
    "        is_end = True\n",
    "    \n",
    "    if not points:\n",
    "        print(\"No more points to scroll. Merging completed.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Found {len(points)} points.\")\n",
    "\n",
    "    # æ’å…¥åˆ°ç›®æ ‡ collection\n",
    "    target_client.upsert(\n",
    "        collection_name=\"network-medicine\",\n",
    "        points=points,\n",
    "    )\n",
    "    \n",
    "    if is_end:\n",
    "        break\n",
    "\n",
    "print(\"Data merge completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_client.count(collection_name=\"network-medicine\"))\n",
    "print(target_client.count(collection_name=\"network-medicine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List parquet metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# è¯»å– Parquet æ–‡ä»¶å…ƒæ•°æ®\n",
    "df = duckdb.query(\n",
    "    \"SELECT * FROM parquet_metadata('/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/paragraphs_md5sum/paragraphs_0.parquet')\"\n",
    ").df()\n",
    "\n",
    "# æ˜¾ç¤ºå…ƒæ•°æ®\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute md5sums of all texts in a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import hashlib\n",
    "import polars as pl\n",
    "\n",
    "# å®šä¹‰æ‰¹å¤„ç†å¤§å°\n",
    "batch_size = 1000000  # æ¯æ‰¹æ¬¡å¤„ç†100ä¸‡è¡Œ\n",
    "\n",
    "conn = duckdb.connect()\n",
    "\n",
    "def count(conn, parquet_file):\n",
    "    sql = f\"SELECT COUNT(*) FROM read_parquet('{parquet_file}')\"\n",
    "    return conn.sql(sql).fetchone()[0]\n",
    "\n",
    "\n",
    "# è·å–æ•°æ®æ€»è¡Œæ•°ï¼Œä»¥ä¾¿æŒ‰æ‰¹æ¬¡å¤„ç†\n",
    "row_count = count(conn, \"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/paragraphs.parquet\")\n",
    "\n",
    "print(\"Found\", row_count, \"paragraphs\")\n",
    "\n",
    "for start in range(0, row_count, batch_size):\n",
    "    print(f\"Processing batch {start} to {start + batch_size}\")\n",
    "\n",
    "    # è¯»å–å½“å‰æ‰¹æ¬¡çš„æ•°æ®\n",
    "    batch = pl.scan_parquet(f\"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/paragraphs.parquet\").slice(start, batch_size).collect()\n",
    "\n",
    "    batch = batch.with_columns(\n",
    "        pl.col(\"text\")\n",
    "        .map_elements(\n",
    "            lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest(),\n",
    "            return_dtype=pl.Utf8,\n",
    "        )\n",
    "        .alias(\"md5sum\")\n",
    "    )\n",
    "\n",
    "    print(f\"Writing batch {start} to {start + batch_size}\")\n",
    "    batch.write_parquet(\n",
    "        f\"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/paragraphs_md5sum/paragraphs_{start}.parquet\",\n",
    "        row_group_size=100000,\n",
    "        compression=\"zstd\",\n",
    "        compression_level=3,\n",
    "        statistics=True,\n",
    "        use_pyarrow=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all points from a qdrant collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ†æ‰¹åŠ è½½æ‰€æœ‰ç‚¹ï¼ˆå‡è®¾ç‚¹æ•°è¾ƒå¤šï¼‰\n",
    "all_points = []\n",
    "batch_size = 1000\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    points = neural_searcher.qdrant_client.scroll(\n",
    "        collection_name=\"network-medicine\",\n",
    "        limit=batch_size,\n",
    "        offset=offset,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    all_points.extend(points[0])\n",
    "    if len(points[0]) < batch_size:  # å¦‚æœå·²ç»åŠ è½½å®Œæ‰€æœ‰æ•°æ®\n",
    "        break\n",
    "    offset += batch_size\n",
    "\n",
    "# æå– payload æ•°æ®\n",
    "payloads = [point.payload for point in all_points]\n",
    "\n",
    "# è½¬ä¸º DataFrame\n",
    "df = pd.DataFrame(payloads)\n",
    "\n",
    "# æŒ‰ pmid åˆ†ç»„ç»Ÿè®¡\n",
    "pmid_counts = df.groupby(\"pmid\").size()\n",
    "unique_pmids = pmid_counts.count()\n",
    "print(f\"Unique PMIDs: {unique_pmids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an HNSW Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# List versions\n",
    "duckdb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all extensions\n",
    "conn = duckdb.connect()\n",
    "conn.sql(\"SELECT * FROM duckdb_extensions();\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = \"/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/paragraphs_md5sum/\"\n",
    "parquet_files = os.listdir(root_dir)\n",
    "parquet_files = [os.path.join(root_dir, f) for f in parquet_files if f.endswith(\".parquet\")]\n",
    "\n",
    "# Create a new duckdb database\n",
    "conn.sql(\"ATTACH '/Volumes/Backup/ProjectData/Papers/PMC_OA_Bulk/paragraphs_md5sum.duckdb' AS db;\");\n",
    "conn.sql(f\"CREATE TABLE db.embeddings AS SELECT * FROM read_parquet('{parquet_files[0]}');\");\n",
    "\n",
    "for parquet_file in parquet_files[1:]:\n",
    "    print(f\"Copying {parquet_file} to db.embeddings\")\n",
    "    conn.sql(f\"COPY db.embeddings FROM '{parquet_file}' (FORMAT PARQUET)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the hnsw extension\n",
    "conn.sql(\"INSTALL vss;\")\n",
    "# Load the hnsw extension\n",
    "conn.sql(\"LOAD vss;\")\n",
    "\n",
    "# Create a VSS index\n",
    "conn.sql(\"CREATE INDEX vss_index ON embeddings USING HNSW (embedding);\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VSS Index for an embedding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "\n",
    "root_dir = \"/work/data/projects/data2report\"\n",
    "conn = duckdb.connect(\n",
    "    os.path.join(root_dir, \"paragraph-minillm-embedding-2025012500.duckdb\")\n",
    ")\n",
    "vss_path = \"/work/data/projects/data2report/vss.duckdb_extension\"\n",
    "\n",
    "print(\"Creating vss index\")\n",
    "# Install the hnsw extension\n",
    "# conn.sql(\"INSTALL vss;\")\n",
    "# Load the hnsw extension\n",
    "con.execute(\"PRAGMA threads=80;\")\n",
    "con.execute(\"PRAGMA memory_limit='120GB';\")\n",
    "conn.sql(f\"LOAD '{vss_path}';\")\n",
    "conn.sql(\"PRAGMA hnsw_enable_experimental_persistence=true;\")\n",
    "conn.execute(\"CREATE INDEX vss_index ON embeddings USING HNSW (embedding);\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FAISS Index\n",
    "\n",
    "https://medium.com/@deveshbajaj59/scaling-semantic-search-with-faiss-challenges-and-solutions-for-billion-scale-datasets-1cacb6f87f95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import faiss\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ------------------------\n",
    "# é…ç½®å‚æ•°\n",
    "# ------------------------\n",
    "root_dir = \"/work/data/projects/data2report\"\n",
    "DB_PATH = os.path.join(root_dir, \"paragraph-minillm-embedding-2025012500.duckdb\")\n",
    "TABLE_NAME = \"embeddings\"\n",
    "VECTOR_DIM = 384  # å‡è®¾ embedding çš„ç»´åº¦ä¸º 384\n",
    "INDEX_FILE_DIR = os.path.join(root_dir, \"paragraph-minillm-embedding-2025012500\")  # FAISS ç´¢å¼•å­˜å‚¨æ–‡ä»¶\n",
    "\n",
    "# ------------------------\n",
    "# è¿æ¥ DuckDB å¹¶è·å–æ€»æ•°æ®é‡\n",
    "# ------------------------\n",
    "conn = duckdb.connect(DB_PATH)\n",
    "total_rows = conn.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").fetchone()[0]\n",
    "print(f\"æ•°æ®åº“ä¸­å…±æœ‰ {total_rows} æ¡ embedding è®°å½•\")\n",
    "\n",
    "# ------------------------\n",
    "# å‚æ•°è®¾ç½®ï¼šåˆ†ç‰‡æ•°ã€æ¯ä¸ªåˆ†ç‰‡çš„è®°å½•æ•°ã€æ¯ä¸ªæ‰¹æ¬¡åŠ è½½çš„è®°å½•æ•°\n",
    "# ------------------------\n",
    "num_shards = 5\n",
    "# é‡‡ç”¨å‘ä¸Šå–æ•´ï¼Œä¿è¯æ‰€æœ‰è®°å½•éƒ½èƒ½è¢«å¤„ç†\n",
    "num_records_per_shard = math.ceil(total_rows / num_shards)\n",
    "# å¦‚æœæ¯ä¸ªåˆ†ç‰‡çš„è®°å½•æ•°è¿‡å¤§ï¼Œå¯ä»¥è¿›ä¸€æ­¥åˆ†æ‰¹å¤„ç†ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŠ è½½çš„è®°å½•æ•°ç‹¬ç«‹è®¾å®š\n",
    "BATCH_SIZE = 100000  # æ ¹æ®å†…å­˜å’Œæ€§èƒ½è¦æ±‚è°ƒæ•´\n",
    "\n",
    "print(f\"æ¯ä¸ªåˆ†ç‰‡å¤§çº¦å¤„ç† {num_records_per_shard} æ¡è®°å½•ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŠ è½½ {BATCH_SIZE} æ¡è®°å½•ï¼Œå…± {num_shards} ä¸ªåˆ†ç‰‡\")\n",
    "\n",
    "keys_list = []   # å­˜å‚¨ key åˆ—è¡¨\n",
    "total_vectors = 0  # ç»Ÿè®¡å·²åŠ è½½çš„å‘é‡æ•°é‡\n",
    "\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    # æŒ‰åˆ†ç‰‡å¤„ç†æ•°æ®\n",
    "    for shard in range(num_shards):\n",
    "        shard_start = shard * num_records_per_shard\n",
    "        # æœ€åä¸€ä¸ªåˆ†ç‰‡å¯èƒ½ä¸è¶³ num_records_per_shard\n",
    "        shard_end = min(total_rows, (shard + 1) * num_records_per_shard)\n",
    "        print(f\"\\næ­£åœ¨å¤„ç†åˆ†ç‰‡ {shard+1}/{num_shards}ï¼Œè®°å½•èŒƒå›´ï¼š{shard_start} ~ {shard_end}\")\n",
    "        \n",
    "        # ------------------------\n",
    "        # åˆå§‹åŒ– FAISS ç´¢å¼•ï¼ˆç¤ºä¾‹ä¸­é‡‡ç”¨ IVFPQ ç´¢å¼•ï¼Œå…ˆåœ¨ CPU ä¸Šåˆ›å»ºï¼Œå†è¿ç§»åˆ°æ‰€æœ‰ GPUï¼‰\n",
    "        # ------------------------\n",
    "        quantizer = faiss.IndexFlatL2(VECTOR_DIM)\n",
    "        faiss_index = faiss.IndexIVFPQ(quantizer, VECTOR_DIM, 1024, 32, 8)\n",
    "        gpu_faiss_index = faiss.index_cpu_to_all_gpus(faiss_index)\n",
    "\n",
    "        # åœ¨å½“å‰åˆ†ç‰‡å†…å†æŒ‰æ‰¹æ¬¡åŠ è½½æ•°æ®\n",
    "        for offset in range(shard_start, shard_end, BATCH_SIZE):\n",
    "            batch_end = min(offset + BATCH_SIZE, shard_end)\n",
    "            current_batch_size = batch_end - offset\n",
    "            print(f\"  æ­£åœ¨åŠ è½½æ•°æ® {offset} - {batch_end} ï¼ˆæ‰¹æ¬¡å¤§å°ï¼š{current_batch_size}ï¼‰\")\n",
    "\n",
    "            query = f\"SELECT key, embedding FROM {TABLE_NAME} LIMIT {current_batch_size} OFFSET {offset}\"\n",
    "            cursor.execute(query)\n",
    "\n",
    "            batch_keys = []\n",
    "            batch_vectors = []\n",
    "            # é€æ­¥è·å–æ•°æ®ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤ªå¤šæ•°æ®åˆ°å†…å­˜\n",
    "            for row in cursor.fetchmany(current_batch_size):\n",
    "                batch_keys.append(row[0])\n",
    "                # row[1] åº”è¯¥ä¸º embedding å‘é‡ï¼ˆä»¥æ•°ç»„æˆ–åˆ—è¡¨å½¢å¼å­˜å‚¨ï¼‰ï¼Œè½¬æ¢ä¸º np.float32 æ•°ç»„\n",
    "                batch_vectors.append(np.array(row[1], dtype=np.float32))\n",
    "\n",
    "            if batch_vectors:\n",
    "                batch_vectors_np = np.vstack(batch_vectors)\n",
    "                # æ³¨æ„ï¼šFAISS ç´¢å¼•é€šå¸¸åªéœ€è¦è®­ç»ƒä¸€æ¬¡\n",
    "                if not gpu_faiss_index.is_trained:\n",
    "                    print(\"  æ­£åœ¨è®­ç»ƒ FAISS ç´¢å¼•...\")\n",
    "                    gpu_faiss_index.train(batch_vectors_np)\n",
    "                print(\"  æ·»åŠ  batch åˆ° FAISS ç´¢å¼•...\")\n",
    "                gpu_faiss_index.add(batch_vectors_np)\n",
    "                keys_list.extend(batch_keys)\n",
    "                total_vectors += len(batch_keys)\n",
    "                print(f\"  å½“å‰åˆ†ç‰‡å·²åŠ è½½ {len(batch_keys)} æ¡è®°å½•ï¼Œæ€»å…±åŠ è½½ {total_vectors} æ¡è®°å½•\")\n",
    "            else:\n",
    "                print(\"  å½“å‰æ‰¹æ¬¡æ²¡æœ‰æ•°æ®\")\n",
    "    \n",
    "        print(f\"\\næ€»å…±åŠ è½½ {total_vectors} æ¡ embedding è¿›å…¥ FAISS ç´¢å¼•\")\n",
    "        # å°†ç´¢å¼•ä» GPU è¿å› CPUï¼Œå¹¶ä¿å­˜åˆ°ç£ç›˜\n",
    "        output_filepath = os.path.join(INDEX_FILE_DIR, f\"faiss_index_{shard}.faiss\")\n",
    "        faiss.write_index(faiss.index_gpu_to_cpu(gpu_faiss_index), output_filepath)\n",
    "        print(f\"FAISS ç´¢å¼•å·²ä¿å­˜åˆ° {output_filepath}\")\n",
    "        \n",
    "        # Save the keys list to a file\n",
    "        keys_filepath = os.path.join(INDEX_FILE_DIR, f\"keys_list_{shard}.json\")\n",
    "        with open(keys_filepath, \"w\") as f:\n",
    "            json.dump(keys_list, f)\n",
    "        print(f\"Keys listå·²ä¿å­˜åˆ° {keys_filepath}\")\n",
    "        \n",
    "        keys_list = []\n",
    "        total_vectors = 0\n",
    "        batch_keys = []\n",
    "        batch_vectors = []\n",
    "        del gpu_faiss_index\n",
    "        del faiss_index\n",
    "        del quantizer\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "\n",
    "finally:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet Files + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import faiss\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ------------------------\n",
    "# é…ç½®å‚æ•°\n",
    "# ------------------------\n",
    "\n",
    "def extract_numbers(s):\n",
    "    \"\"\"æå–æ–‡ä»¶åä¸­çš„æ•°å­—å¹¶è½¬æ¢ä¸ºå…ƒç»„\"\"\"\n",
    "    numbers = re.findall(r'\\d+', s)  # æå–æ‰€æœ‰æ•°å­—\n",
    "    return tuple(map(int, numbers))  # è½¬æ¢ä¸ºæ•´æ•°å…ƒç»„è¿›è¡Œæ’åº\n",
    "\n",
    "root_dir = \"/work/data/projects/data2report\"\n",
    "parquet_dir = os.path.join(root_dir, \"paragraph-minillm-embedding-2025012500\")\n",
    "# æ‰¹é‡å¯¼å…¥æ‰€æœ‰ Parquet\n",
    "parquet_files = sorted([os.path.join(parquet_dir, f) for f in os.listdir(parquet_dir) if f.endswith(\".parquet\")], key=extract_numbers)\n",
    "\n",
    "TABLE_NAME = \"embeddings\"\n",
    "VECTOR_DIM = 384  # å‡è®¾ embedding çš„ç»´åº¦ä¸º 384\n",
    "INDEX_FILE_DIR = os.path.join(root_dir, \"paragraph-faiss-2025012500\")  # FAISS ç´¢å¼•å’Œ key æ–‡ä»¶å­˜å‚¨ç›®å½•\n",
    "\n",
    "if not os.path.exists(INDEX_FILE_DIR):\n",
    "    os.makedirs(INDEX_FILE_DIR, exist_ok=True)\n",
    "\n",
    "num_gpus = 3                     # å¯ç”¨ GPU æ•°é‡\n",
    "num_shards = 9\n",
    "\n",
    "def group_files_by_prefix(parquet_files: list[str]) -> OrderedDict:\n",
    "    \"\"\"\n",
    "    å°†æ–‡ä»¶åˆ—è¡¨æŒ‰ç…§æ–‡ä»¶åä¸­çš„ç»„æ ‡è¯†ï¼ˆå¦‚æ•°å­—éƒ¨åˆ†ï¼‰è¿›è¡Œåˆ†ç»„ã€‚\n",
    "    å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º \"embeddings_<group>_something.parquet\"ï¼Œ\n",
    "    è¿”å›ä¸€ä¸ªæœ‰åºå­—å…¸ï¼Œå…¶é”®ä¸ºç»„æ ‡è¯†ï¼Œå€¼ä¸ºè¯¥ç»„çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’åºåï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'embeddings_(\\d+)')\n",
    "    groups = {}\n",
    "    for f in parquet_files:\n",
    "        basename = os.path.basename(f)\n",
    "        m = pattern.search(basename)\n",
    "        if m:\n",
    "            group = m.group(1)\n",
    "        else:\n",
    "            group = basename  # å¦‚æœæ— æ³•åŒ¹é…ï¼Œåˆ™ä»¥å®Œæ•´æ–‡ä»¶åä½œä¸ºåˆ†ç»„é”®\n",
    "        groups.setdefault(group, []).append(f)\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªç»„å†…éƒ¨è¿›è¡Œæ’åºï¼ˆä¾‹å¦‚æŒ‰æ–‡ä»¶åæ’åºï¼‰\n",
    "    for group in groups:\n",
    "        groups[group].sort()\n",
    "    \n",
    "    # å°†ç»„æŒ‰ç…§æ•°å€¼æ’åºï¼ˆç»„æ ‡è¯†éƒ½æ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼‰\n",
    "    sorted_groups = OrderedDict(\n",
    "        sorted(groups.items(), key=lambda x: int(x[0]))\n",
    "    )\n",
    "    return sorted_groups\n",
    "\n",
    "def partition_by_groups(parquet_files: list[str], num_shards: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    æŒ‰ç…§ç»„å°† parquet_files åˆ†ç‰‡ï¼Œä½¿å¾—ç›¸åŒç»„çš„æ–‡ä»¶ä¿æŒåœ¨ä¸€èµ·ã€‚\n",
    "    é‡‡ç”¨è´ªå¿ƒç®—æ³•ç´¯åŠ ç»„çš„æ–‡ä»¶æ•°ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œå†å¼€å§‹ä¸‹ä¸€ä¸ªåˆ†ç‰‡ã€‚\n",
    "    \"\"\"\n",
    "    # å…ˆæŒ‰ç»„åˆ†ç»„\n",
    "    grouped = group_files_by_prefix(parquet_files)\n",
    "    \n",
    "    # è®¡ç®—æ€»æ–‡ä»¶æ•°\n",
    "    total_files = len(parquet_files)\n",
    "    # ç›®æ ‡æ¯ä¸ªåˆ†ç‰‡çš„æ–‡ä»¶æ•°ï¼ˆå¹³å‡å€¼ï¼‰\n",
    "    target = total_files / num_shards\n",
    "\n",
    "    shards = []\n",
    "    current_shard = []\n",
    "    current_count = 0\n",
    "    \n",
    "    for group, files in grouped.items():\n",
    "        # å¦‚æœå½“å‰åˆ†ç‰‡ä¸­å·²æœ‰æ–‡ä»¶ä¸”åŠ ä¸Šå½“å‰ç»„åä¼šè¶…è¿‡ç›®æ ‡ï¼Œ\n",
    "        # ä¸”åˆ†ç‰‡æ•°é‡è¿˜æœªè¾¾åˆ° num_shards-1ï¼ˆä¿è¯æœ€åä¸€ç‰‡å¯ä»¥æ‹¿åˆ°å‰©ä½™æ–‡ä»¶ï¼‰ï¼Œ\n",
    "        # åˆ™å¼€å§‹ä¸€ä¸ªæ–°çš„åˆ†ç‰‡\n",
    "        if current_shard and current_count + len(files) > target and len(shards) < num_shards - 1:\n",
    "            shards.append(current_shard)\n",
    "            current_shard = []\n",
    "            current_count = 0\n",
    "        current_shard.extend(files)\n",
    "        current_count += len(files)\n",
    "    \n",
    "    # æ·»åŠ æœ€åä¸€ä¸ªåˆ†ç‰‡\n",
    "    if current_shard:\n",
    "        shards.append(current_shard)\n",
    "    \n",
    "    return shards\n",
    "\n",
    "def process_shard(shard: int):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªåˆ†ç‰‡ï¼š\n",
    "      - è®¡ç®—åˆ†ç‰‡åœ¨æ€»æ•°æ®ä¸­çš„èµ·å§‹ä¸ç»“æŸä½ç½®\n",
    "      - æ¯ä¸ªåˆ†ç‰‡å†…åˆ†æ‰¹åŠ è½½æ•°æ®\n",
    "      - åœ¨æŒ‡å®š GPU ä¸Šæ„å»º FAISS ç´¢å¼•ï¼Œå¹¶å°†ç´¢å¼•å’Œ key åˆ—è¡¨ä¿å­˜åˆ°ç£ç›˜\n",
    "    \"\"\"\n",
    "    local_parquet_files = partition_by_groups(parquet_files, num_shards)[shard]\n",
    "    print(f\"å¤„ç†åˆ†ç‰‡ {shard} çš„æ–‡ä»¶: {len(local_parquet_files)} {local_parquet_files}\")\n",
    "    \n",
    "    # return\n",
    "    output_filepath = os.path.join(INDEX_FILE_DIR, f\"faiss_index_{shard}.faiss\")\n",
    "    \n",
    "    if os.path.exists(output_filepath):\n",
    "        print(f\"[Shard {shard}] FAISS ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†\")\n",
    "        return\n",
    "    \n",
    "    # æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å»ºç«‹ DuckDB è¿æ¥\n",
    "    keys_list = []       # ç”¨äºä¿å­˜è¯¥åˆ†ç‰‡æ‰€æœ‰è®°å½•çš„ key\n",
    "    embeddings_list = []\n",
    "    total_vectors = 0    # ç»Ÿè®¡è¯¥åˆ†ç‰‡å·²åŠ è½½çš„å‘é‡æ•°é‡\n",
    "\n",
    "    # ------------------------\n",
    "    # åˆå§‹åŒ– FAISS ç´¢å¼•ï¼Œå¹¶æŒ‡å®šä½¿ç”¨çš„ GPUï¼ˆdevice_id = shard % num_gpusï¼‰\n",
    "    # ------------------------\n",
    "    quantizer = faiss.IndexFlatL2(VECTOR_DIM)\n",
    "    faiss_index = faiss.IndexIVFPQ(quantizer, VECTOR_DIM, 1024, 32, 8)\n",
    "    device_id = shard % num_gpus\n",
    "    gpu_res = faiss.StandardGpuResources()\n",
    "    gpu_faiss_index = faiss.index_cpu_to_gpu(gpu_res, device_id, faiss_index)\n",
    "    \n",
    "    # æŒ‰æ‰¹æ¬¡åŠ è½½å½“å‰åˆ†ç‰‡å†…çš„æ•°æ®\n",
    "    for file in local_parquet_files:\n",
    "        start_time = time.time()\n",
    "        conn = duckdb.connect()\n",
    "        df = conn.execute(f\"SELECT key, embedding FROM read_parquet('{file}')\").df()\n",
    "        batch_keys = df[\"key\"].to_list()\n",
    "        batch_vectors = np.array(df[\"embedding\"].to_list(), dtype=np.float32)\n",
    "        print(f\"[Shard {shard}] è¯»å–/è½¬æ¢æ•°æ® {file} è€—æ—¶: {time.time() - start_time} ç§’\")\n",
    "        \n",
    "        keys_list.extend(batch_keys)\n",
    "        total_vectors += len(batch_keys)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if len(batch_vectors) > 0 and len(batch_vectors) < 1024:\n",
    "            embeddings_list.extend(batch_vectors)\n",
    "            continue\n",
    "        elif len(batch_vectors) > 1024:\n",
    "            embeddings_list.extend(batch_vectors)\n",
    "        else:\n",
    "            print(f\"[Shard {shard}] å½“å‰æ‰¹æ¬¡æ²¡æœ‰æ•°æ®\")\n",
    "            continue\n",
    "            \n",
    "        stacked_embeddings_list = np.vstack(embeddings_list)\n",
    "        # FAISS ç´¢å¼•åªéœ€è¦è®­ç»ƒä¸€æ¬¡ï¼Œè®­ç»ƒæ­¥éª¤æ”¾åœ¨ç¬¬ä¸€æ¬¡æ‰¹æ¬¡ä¸­\n",
    "        if not gpu_faiss_index.is_trained:\n",
    "            print(f\"[Shard {shard}] æ­£åœ¨è®­ç»ƒ FAISS ç´¢å¼•...\")\n",
    "            gpu_faiss_index.train(stacked_embeddings_list)\n",
    "\n",
    "        print(f\"[Shard {shard}] æ·»åŠ  batch åˆ° FAISS ç´¢å¼•...\")\n",
    "        gpu_faiss_index.add(stacked_embeddings_list)\n",
    "        print(f\"[Shard {shard}] å½“å‰åˆ†ç‰‡æ€»å…±åŠ è½½ {total_vectors} æ¡è®°å½•\")\n",
    "        print(f\"[Shard {shard}] æ·»åŠ  batch åˆ° FAISS ç´¢å¼• è€—æ—¶: {time.time() - start_time} ç§’\")\n",
    "        \n",
    "        embeddings_list = []\n",
    "\n",
    "    # ä¿å­˜å½“å‰åˆ†ç‰‡æ„å»ºçš„ FAISS ç´¢å¼•\n",
    "    faiss.write_index(faiss.index_gpu_to_cpu(gpu_faiss_index), output_filepath)\n",
    "    print(f\"[Shard {shard}] FAISS ç´¢å¼•å·²ä¿å­˜åˆ° {output_filepath}\")\n",
    "    \n",
    "    # ä¿å­˜ key åˆ—è¡¨åˆ° JSON æ–‡ä»¶\n",
    "    keys_filepath = os.path.join(INDEX_FILE_DIR, f\"keys_list_{shard}.json\")\n",
    "    with open(keys_filepath, \"w\") as f:\n",
    "        json.dump(keys_list, f)\n",
    "    print(f\"[Shard {shard}] Keys list å·²ä¿å­˜åˆ° {keys_filepath}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # æ˜¾å¼é‡Šæ”¾ GPU ç›¸å…³å¯¹è±¡ï¼Œå¹¶è°ƒç”¨åƒåœ¾å›æ”¶\n",
    "    del gpu_faiss_index, faiss_index, quantizer, gpu_res\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å„ä¸ªåˆ†ç‰‡\n",
    "# ------------------------\n",
    "with ThreadPoolExecutor(max_workers=num_gpus) as executor:\n",
    "    # ä¸ºæ¯ä¸ªåˆ†ç‰‡æäº¤ä¸€ä¸ªä»»åŠ¡\n",
    "    futures = [executor.submit(process_shard, shard) for shard in range(num_shards)]\n",
    "    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç†åˆ†ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "\n",
    "print(\"æ‰€æœ‰åˆ†ç‰‡å‡å·²å¤„ç†å®Œæ¯•ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duckdb + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import faiss\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ------------------------\n",
    "# é…ç½®å‚æ•°\n",
    "# ------------------------\n",
    "root_dir = \"/work/data/projects/data2report\"\n",
    "DB_PATH = os.path.join(root_dir, \"paragraph-minillm-embedding-2025012500.duckdb\")\n",
    "TABLE_NAME = \"embeddings\"\n",
    "VECTOR_DIM = 384  # å‡è®¾ embedding çš„ç»´åº¦ä¸º 384\n",
    "INDEX_FILE_DIR = os.path.join(root_dir, \"paragraph-faiss-2025012500\")  # FAISS ç´¢å¼•å’Œ key æ–‡ä»¶å­˜å‚¨ç›®å½•\n",
    "\n",
    "if not os.path.exists(INDEX_FILE_DIR):\n",
    "    os.makedirs(INDEX_FILE_DIR, exist_ok=True)\n",
    "\n",
    "# åˆ†ç‰‡å’Œæ‰¹æ¬¡å‚æ•°\n",
    "num_shards = 9                   # æ€»åˆ†ç‰‡æ•°ï¼ˆæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªåˆ†ç‰‡ï¼‰\n",
    "batch_size = 100000              # æ¯ä¸ªæ‰¹æ¬¡åŠ è½½çš„è®°å½•æ•°ï¼ˆå¯æ ¹æ®å†…å­˜å’Œæ€§èƒ½è°ƒæ•´ï¼‰\n",
    "num_gpus = 3                     # å¯ç”¨ GPU æ•°é‡\n",
    "\n",
    "# ------------------------\n",
    "# è·å–æ€»æ•°æ®é‡ï¼ˆä¸»çº¿ç¨‹ä¸­è·å–ä¸€æ¬¡ï¼Œç„¶åä¼ é€’ç»™å„çº¿ç¨‹ï¼‰\n",
    "# ------------------------\n",
    "conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "total_rows = conn.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").fetchone()[0]\n",
    "conn.close()\n",
    "print(f\"æ•°æ®åº“ä¸­å…±æœ‰ {total_rows} æ¡ embedding è®°å½•\")\n",
    "\n",
    "granularity = 5000000\n",
    "\n",
    "\n",
    "def get_shard_ranges(total_rows: int, num_shards: int, granularity: int = 100000) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    æŒ‰ç…§ç»™å®šçš„ç²’åº¦ï¼ˆé»˜è®¤ 100,000 è¡Œï¼‰åˆ’åˆ†åˆ†ç‰‡ï¼š\n",
    "      - è®¡ç®—å¹³å‡å€¼ avg = total_rows / num_shardsï¼Œ\n",
    "      - å°† avg æŒ‰ç²’åº¦å‘ä¸Šå–æ•´å¾—åˆ° baseï¼Œ\n",
    "      - è®¡ç®—æœ€å¤§å¯åˆ†ç‰‡æ•° max_shards = floor(total_rows / base) + (1 if total_rows % base != 0 else 0)ï¼›\n",
    "      - å¦‚æœ max_shards å°äºè®¡åˆ’çš„ num_shardsï¼Œåˆ™è‡ªåŠ¨è°ƒæ•´ num_shards ä¸º max_shardsã€‚\n",
    "    è¿”å›ä¸€ä¸ªåŒ…å«æ¯ä¸ªåˆ†ç‰‡ (start, end) çš„åˆ—è¡¨ï¼Œå…¶ä¸­ end ä¸ºä¸åŒ…å«è¾¹ç•Œã€‚\n",
    "    \"\"\"\n",
    "    avg = total_rows / num_shards\n",
    "    base = int(math.ceil(avg / granularity)) * granularity\n",
    "\n",
    "    # è®¡ç®—æŒ‰ base èƒ½åˆ†æˆå¤šå°‘åˆ†ç‰‡\n",
    "    max_shards = math.floor(total_rows / base) + (1 if total_rows % base != 0 else 0)\n",
    "    if max_shards < num_shards:\n",
    "        print(f\"æ³¨æ„ï¼šæŒ‰å½“å‰ç²’åº¦ï¼Œæ¯ä»½ {base} è¡Œï¼Œå®é™…æœ€å¤šå¯åˆ† {max_shards} ä»½ï¼›å°†è‡ªåŠ¨è°ƒæ•´åˆ†ç‰‡æ•°ä¸º {max_shards}\")\n",
    "        num_shards = max_shards\n",
    "\n",
    "    ranges = []\n",
    "    start = 0\n",
    "    # ç”Ÿæˆå‰ num_shards - 1 ä¸ªåˆ†ç‰‡\n",
    "    for i in range(num_shards - 1):\n",
    "        end = start + base\n",
    "        if end > total_rows:\n",
    "            end = total_rows\n",
    "        ranges.append((start, end))\n",
    "        start = end\n",
    "    # æœ€åä¸€ä¸ªåˆ†ç‰‡å–å‰©ä½™éƒ¨åˆ†ï¼ˆå¯èƒ½ä¸è¶³ base è¡Œï¼‰\n",
    "    if start < total_rows:\n",
    "        ranges.append((start, total_rows))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def process_shard(shard: int):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªåˆ†ç‰‡ï¼š\n",
    "      - è®¡ç®—åˆ†ç‰‡åœ¨æ€»æ•°æ®ä¸­çš„èµ·å§‹ä¸ç»“æŸä½ç½®\n",
    "      - æ¯ä¸ªåˆ†ç‰‡å†…åˆ†æ‰¹åŠ è½½æ•°æ®\n",
    "      - åœ¨æŒ‡å®š GPU ä¸Šæ„å»º FAISS ç´¢å¼•ï¼Œå¹¶å°†ç´¢å¼•å’Œ key åˆ—è¡¨ä¿å­˜åˆ°ç£ç›˜\n",
    "    \"\"\"\n",
    "    output_filepath = os.path.join(INDEX_FILE_DIR, f\"faiss_index_{shard}.faiss\")\n",
    "    \n",
    "    if os.path.exists(output_filepath):\n",
    "        print(f\"[Shard {shard}] FAISS ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†\")\n",
    "        return\n",
    "    \n",
    "    # æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å»ºç«‹ DuckDB è¿æ¥\n",
    "    conn = duckdb.connect(DB_PATH, read_only=True)\n",
    "    conn.execute(\"PRAGMA threads=80;\")\n",
    "    conn.execute(\"PRAGMA memory_limit='120GB';\")\n",
    "    shard_ranges = get_shard_ranges(total_rows, num_shards, granularity)\n",
    "    print(\"Shard ranges:\", shard_ranges)\n",
    "    shard_start, shard_end = shard_ranges[shard]\n",
    "    # shard_start = shard * num_records_per_shard\n",
    "    # shard_end = min(total_rows, (shard + 1) * num_records_per_shard)\n",
    "    print(f\"[Shard {shard}] å¤„ç†è®°å½•èŒƒå›´ï¼š{shard_start} ~ {shard_end}\")\n",
    "\n",
    "    keys_list = []       # ç”¨äºä¿å­˜è¯¥åˆ†ç‰‡æ‰€æœ‰è®°å½•çš„ key\n",
    "    total_vectors = 0    # ç»Ÿè®¡è¯¥åˆ†ç‰‡å·²åŠ è½½çš„å‘é‡æ•°é‡\n",
    "\n",
    "    # ------------------------\n",
    "    # åˆå§‹åŒ– FAISS ç´¢å¼•ï¼Œå¹¶æŒ‡å®šä½¿ç”¨çš„ GPUï¼ˆdevice_id = shard % num_gpusï¼‰\n",
    "    # ------------------------\n",
    "    quantizer = faiss.IndexFlatL2(VECTOR_DIM)\n",
    "    faiss_index = faiss.IndexIVFPQ(quantizer, VECTOR_DIM, 1024, 32, 8)\n",
    "    # device_id = shard % num_gpus\n",
    "    # gpu_res = faiss.StandardGpuResources()\n",
    "    # gpu_faiss_index = faiss.index_cpu_to_gpu(gpu_res, device_id, faiss_index)\n",
    "    gpu_faiss_index = faiss.index_cpu_to_all_gpus(faiss_index)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    # æŒ‰æ‰¹æ¬¡åŠ è½½å½“å‰åˆ†ç‰‡å†…çš„æ•°æ®\n",
    "    for offset in range(shard_start, shard_end, batch_size):\n",
    "        batch_end = min(offset + batch_size, shard_end)\n",
    "        current_batch_size = batch_end - offset\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(f\"[Shard {shard}] æ­£åœ¨åŠ è½½æ•°æ® {offset} - {batch_end} ï¼ˆæ‰¹æ¬¡å¤§å°ï¼š{current_batch_size}ï¼‰\")        \n",
    "        query = f\"SELECT key, embedding FROM {TABLE_NAME} LIMIT {current_batch_size} OFFSET {offset}\"\n",
    "        # query = f\"\"\"\n",
    "        # SELECT key, embedding, row_number() OVER () AS rowid\n",
    "        # FROM {TABLE_NAME}\n",
    "        # WHERE rowid > {offset} AND rowid <= {batch_end}\n",
    "        # \"\"\"\n",
    "        cursor.execute(query)\n",
    "        print(f\"[Shard {shard}] æ­£åœ¨åŠ è½½æ•°æ® {offset} - {batch_end} è€—æ—¶: {time.time() - start_time} ç§’\")\n",
    "        \n",
    "        batch_keys = []\n",
    "        batch_vectors = []\n",
    "        # ä½¿ç”¨ fetchmany åˆ†æ‰¹å–æ•°æ®ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®\n",
    "        for row in cursor.fetchmany(current_batch_size):\n",
    "            batch_keys.append(row[0])\n",
    "            # å‡å®š row[1] å­˜å‚¨çš„æ˜¯ embedding å‘é‡ï¼ˆæ•°ç»„æˆ–åˆ—è¡¨å½¢å¼ï¼‰\n",
    "            batch_vectors.append(np.array(row[1], dtype=np.float32))\n",
    "        \n",
    "        if batch_vectors:\n",
    "            batch_vectors_np = np.vstack(batch_vectors)\n",
    "            # FAISS ç´¢å¼•åªéœ€è¦è®­ç»ƒä¸€æ¬¡ï¼Œè®­ç»ƒæ­¥éª¤æ”¾åœ¨ç¬¬ä¸€æ¬¡æ‰¹æ¬¡ä¸­\n",
    "            if not gpu_faiss_index.is_trained:\n",
    "                print(f\"[Shard {shard}] æ­£åœ¨è®­ç»ƒ FAISS ç´¢å¼•...\")\n",
    "                gpu_faiss_index.train(batch_vectors_np)\n",
    "            print(f\"[Shard {shard}] æ·»åŠ  batch åˆ° FAISS ç´¢å¼•...\")\n",
    "            gpu_faiss_index.add(batch_vectors_np)\n",
    "            keys_list.extend(batch_keys)\n",
    "            total_vectors += len(batch_keys)\n",
    "            print(f\"[Shard {shard}] å½“å‰åˆ†ç‰‡å·²åŠ è½½ {len(batch_keys)} æ¡è®°å½•ï¼Œæ€»å…±åŠ è½½ {total_vectors} æ¡è®°å½•\")\n",
    "        else:\n",
    "            print(f\"[Shard {shard}] å½“å‰æ‰¹æ¬¡æ²¡æœ‰æ•°æ®\")\n",
    "    \n",
    "    # ä¿å­˜å½“å‰åˆ†ç‰‡æ„å»ºçš„ FAISS ç´¢å¼•\n",
    "    faiss.write_index(faiss.index_gpu_to_cpu(gpu_faiss_index), output_filepath)\n",
    "    print(f\"[Shard {shard}] FAISS ç´¢å¼•å·²ä¿å­˜åˆ° {output_filepath}\")\n",
    "    \n",
    "    # ä¿å­˜ key åˆ—è¡¨åˆ° JSON æ–‡ä»¶\n",
    "    keys_filepath = os.path.join(INDEX_FILE_DIR, f\"keys_list_{shard}.json\")\n",
    "    with open(keys_filepath, \"w\") as f:\n",
    "        json.dump(keys_list, f)\n",
    "    print(f\"[Shard {shard}] Keys list å·²ä¿å­˜åˆ° {keys_filepath}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # æ˜¾å¼é‡Šæ”¾ GPU ç›¸å…³å¯¹è±¡ï¼Œå¹¶è°ƒç”¨åƒåœ¾å›æ”¶\n",
    "    del gpu_faiss_index, faiss_index, quantizer, gpu_res\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å„ä¸ªåˆ†ç‰‡\n",
    "# ------------------------\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    # ä¸ºæ¯ä¸ªåˆ†ç‰‡æäº¤ä¸€ä¸ªä»»åŠ¡\n",
    "    futures = [executor.submit(process_shard, shard) for shard in range(num_shards)]\n",
    "    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç†åˆ†ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "\n",
    "print(\"æ‰€æœ‰åˆ†ç‰‡å‡å·²å¤„ç†å®Œæ¯•ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/work/data/environments/cache/\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class NeuralSearcher:\n",
    "    def __init__(self, faiss_dir: str, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=None):\n",
    "        # Initialize encoder model\n",
    "        self.model = SentenceTransformer(model_name, device=device if device is not None else \"cpu\", trust_remote_code=True)\n",
    "        self.faiss_dir = faiss_dir\n",
    "        \n",
    "        self.index_files = sorted([os.path.join(faiss_dir, f) for f in os.listdir(faiss_dir) if f.endswith(\".faiss\")], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "        self.keys_list = sorted([os.path.join(faiss_dir, f) for f in os.listdir(faiss_dir) if f.endswith(\".json\")], key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    def _search(self, query_vector: str, top_k: int = 10):\n",
    "        final_results = []\n",
    "        for keys_file, index_file in zip(self.keys_list, self.index_files):\n",
    "            faiss_index = faiss.read_index(index_file)\n",
    "            res = faiss.StandardGpuResources()\n",
    "            gpu_faiss_index = faiss.index_cpu_to_all_gpus(faiss_index)\n",
    "            print(f\"å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š{faiss_index.ntotal}\")\n",
    "            \n",
    "            keys_list = json.load(open(keys_file))\n",
    "\n",
    "            def search_similar_vectors(query_vector, top_k=10):\n",
    "                \"\"\"\n",
    "                åœ¨ FAISS ç´¢å¼•ä¸­æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ top_k å‘é‡\n",
    "                \"\"\"\n",
    "                query_vector = np.array(query_vector, dtype=np.float32).reshape(1, -1)  # ç¡®ä¿æ­£ç¡®å½¢çŠ¶\n",
    "                distances, indices = gpu_faiss_index.search(query_vector, top_k)  # æ‰§è¡Œæœç´¢\n",
    "\n",
    "                # è§£ææŸ¥è¯¢ç»“æœ\n",
    "                nearest_keys = [keys_list[i] for i in indices[0]]\n",
    "                distances = distances[0]\n",
    "                \n",
    "                return {\n",
    "                    key: distance\n",
    "                    for key, distance in zip(nearest_keys, distances)\n",
    "                }\n",
    "\n",
    "\n",
    "            top_results = search_similar_vectors(query_vector, top_k=top_k)\n",
    "\n",
    "            final_results.append({\n",
    "                \"keys_file\": keys_file,\n",
    "                \"index_file\": index_file,\n",
    "                \"top_results\": top_results\n",
    "            })\n",
    "        \n",
    "        return final_results\n",
    "        \n",
    "    def search(self, text: str, limit: int = 10):\n",
    "        # Convert text query into vector\n",
    "        vector = self.model.encode(text).tolist()\n",
    "\n",
    "        # Use `vector` for search for closest vectors in the collection\n",
    "        return self._search(vector, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000960\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š26250317\n"
     ]
    }
   ],
   "source": [
    "neural_searcher = NeuralSearcher(faiss_dir=\"/work/data/projects/data2report/paragraph-faiss-2025012500\")\n",
    "results1 = neural_searcher.search(\"How to express recombinant Interleukin-1 beta?\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000960\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š26250317\n"
     ]
    }
   ],
   "source": [
    "results2 = neural_searcher.search(\"How to express recombinant Interleukin-1 beta?\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000960\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š21000000\n",
      "å·²åŠ è½½ FAISS ç´¢å¼•ï¼Œæ€»æ¡æ•°ï¼š26250317\n"
     ]
    }
   ],
   "source": [
    "results3 = neural_searcher.search(\"How to express recombinant Interleukin-1 beta?\", limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "key_scores = [result[\"top_results\"] for result in results]\n",
    "df = pd.DataFrame([\n",
    "    {\"key\": k, \"score\": v}\n",
    "    for d in key_scores\n",
    "    for k, v in d.items()\n",
    "])\n",
    "df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "keys = df[\"key\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26284424',\n",
       " '24671001',\n",
       " '24091292',\n",
       " '23514733',\n",
       " '21048425',\n",
       " '20332197',\n",
       " '18534016']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = {\n",
    "    \"PD-1\": {\n",
    "        \"prompt\": \"How to express recombinant PD-1?\",\n",
    "        \"dois\": [\n",
    "            \"10.1080/19420862.2019.1596514\",\n",
    "            \"10.1158/2159-8290.CD-20-1445\",\n",
    "            \"10.1158/1535-7163.MCT-16-0665\",\n",
    "            \"10.1080/19420862.2017.1371386\",\n",
    "            \"10.1056/NEJMoa1200690\",\n",
    "            \"10.1093/intimm/dxu095\",\n",
    "            \"10.1016/j.critrevonc.2016.02.001\",\n",
    "            \"10.1016/j.molmed.2014.10.009\",\n",
    "            \"10.1080/14653240802320237\",\n",
    "            \"10.1038/s41598-022-16776-1\"\n",
    "        ]\n",
    "    },\n",
    "    \"TNF-alpha\": {\n",
    "        \"prompt\": \"How to express recombinant TNF-alpha?\",\n",
    "        \"dois\": [\n",
    "            \"10.1016/j.intimp.2021.107418\",\n",
    "            \"10.1182/blood-2011-04-325225\",\n",
    "            \"10.1093/ecco-jcc/jjw053\",\n",
    "            \"10.1080/21655979.2021.1967710\",\n",
    "            \"10.1371/journal.pone.0006087\",\n",
    "            \"10.1371/journal.pone.0016373\",\n",
    "            \"10.3390/md20050307\",\n",
    "            \"10.4049/jimmunol.2101180\",\n",
    "            \"10.3390/jcm12041630\",\n",
    "            \"10.1080/17425255.2017.1360280\"\n",
    "        ]\n",
    "    },\n",
    "    \"Amyloid-beta\": {\n",
    "        \"prompt\": \"How to express recombinant Amyloid-beta?\",\n",
    "        \"dois\": [\n",
    "            \"10.1016/j.biopsych.2017.08.010\",\n",
    "            \"10.1038/aps.2017.28\",\n",
    "            \"10.3233/JAD-179941\",\n",
    "            \"10.1016/j.pep.2015.05.015\",\n",
    "            \"10.1016/j.pep.2012.08.021\",\n",
    "            \"10.3233/JAD-2011-110977\",\n",
    "            \"10.1002/ana.24188\",\n",
    "            \"10.1186/s13195-022-01117-1\",\n",
    "            \"10.1186/alzrt246\"\n",
    "        ]\n",
    "    },\n",
    "    \"Interleukin-1 beta\": {\n",
    "        \"prompt\": \"How to express recombinant Interleukin-1 beta?\",\n",
    "        \"dois\": [\n",
    "            \"10.1210/en.2009-1124\",\n",
    "            \"10.4161/mabs.28614\",\n",
    "            \"10.1186/ar2438\",\n",
    "            \"10.2337/dc12-1835\",\n",
    "            \"10.1016/j.molimm.2013.08.002\",\n",
    "            \"10.1080/19420862.2015.1081323\",\n",
    "            \"10.4161/mabs.28614\",\n",
    "            \"10.4161/mabs.3.1.13989\"\n",
    "        ]\n",
    "    },\n",
    "    \"COVID-19 spike protein\": {\n",
    "        \"prompt\": \"How to express recombinant COVID-19 spike protein?\",\n",
    "        \"dois\": [\n",
    "            \"10.1126/science.abc6952\",\n",
    "            \"10.1016/j.vaccine.2014.04.016\",\n",
    "            \"10.1038/s41591-020-0998-x\",\n",
    "            \"10.1016/j.biopha.2020.110337\",\n",
    "            \"10.1016/j.cell.2021.02.035\",\n",
    "            \"10.1038/s41423-020-0426-7\",\n",
    "            \"10.1080/21645515.2020.1740560\",\n",
    "            \"10.1371/journal.pone.0300297\",\n",
    "            \"10.1126/science.abc2241\",\n",
    "            \"10.1126/science.abc5902\"\n",
    "        ]\n",
    "    },\n",
    "    \"Helicobacter pylori\": {\n",
    "        \"prompt\": \"How to express recombinant Helicobacter pylori?\",\n",
    "        \"dois\": [\n",
    "            \"10.1128/cdli.9.1.75-78.2002\",\n",
    "            \"10.1128/IAI.70.8.4158-4164.2002\",\n",
    "            \"10.1016/j.actbio.2022.02.031\",\n",
    "            \"10.3390/microorganisms12061148\",\n",
    "            \"10.1016/j.jim.2012.06.010\",\n",
    "            \"10.1007/s00535-022-01933-0\",\n",
    "            \"10.1007/s00253-018-9068-4\",\n",
    "            \"10.1016/S0304-4165(00)00005-2\",\n",
    "            \"10.1016/j.jim.2007.05.005\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def load_papers(dois):\n",
    "    with open(\"/work/data/projects/paper-parser/benchmarks/antibody/papers.json\", \"r\") as f:\n",
    "        papers = json.load(f)\n",
    "\n",
    "    paper_metadata = [paper for paper in papers if paper[\"doi\"] in dois]\n",
    "\n",
    "    truth_pmids = [str(paper_metadata[\"pmid\"]) for paper_metadata in paper_metadata]\n",
    "    \n",
    "    return truth_pmids\n",
    "\n",
    "truth_pmids = load_papers(tests[\"Interleukin-1 beta\"][\"dois\"])\n",
    "truth_pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>md5sum</th>\n",
       "      <th>text</th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3123623c10be553a87bbbb9a62db4582</td>\n",
       "      <td>PD-L1 expression in cancer patients receiving ...</td>\n",
       "      <td>26895815</td>\n",
       "      <td>26895815-abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b361300ff28b57c63143434a6e3f60b</td>\n",
       "      <td>Introduction\\nA dynamic relationship exists be...</td>\n",
       "      <td>26895815</td>\n",
       "      <td>26895815-Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c6ed448e85c0656e3f781c8eb56334c9</td>\n",
       "      <td>Meta-analysis methods\\nA systematic literature...</td>\n",
       "      <td>26895815</td>\n",
       "      <td>26895815-Meta-analysis methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12fc9a1c01dbd10d7852f8949710433b</td>\n",
       "      <td>Literature search\\nPublished reports were obta...</td>\n",
       "      <td>26895815</td>\n",
       "      <td>26895815-Literature search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>697856e680f1c2b606de23d708646aa5</td>\n",
       "      <td>Statistical analysis\\nWe extracted data from a...</td>\n",
       "      <td>26895815</td>\n",
       "      <td>26895815-Statistical analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>552e72f182c130a0d49b2583e00f7222</td>\n",
       "      <td>FcÎ³-receptor mediated induction of wound heali...</td>\n",
       "      <td>26896086</td>\n",
       "      <td>26896086-FcÎ³-receptor mediated induction of wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>4c3f1796b2ae8d64aed4ec40ad4c6b67</td>\n",
       "      <td>Conclusions\\nAnti-TNFs may have multiple modes...</td>\n",
       "      <td>26896086</td>\n",
       "      <td>26896086-Conclusions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>dc5d9fbe4e9cd4913a358b292e7c2d3d</td>\n",
       "      <td>Funding\\nNo specific funding was obtained for ...</td>\n",
       "      <td>26896086</td>\n",
       "      <td>26896086-Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>023816cac915382a02118c89261a7c71</td>\n",
       "      <td>Conflict of Interest\\nThe authors declare no c...</td>\n",
       "      <td>26896086</td>\n",
       "      <td>26896086-Conflict of Interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>f3d044e58e1ee5d2c2e7beff111dbef5</td>\n",
       "      <td>Author Contributions\\nADL performed a review o...</td>\n",
       "      <td>26896086</td>\n",
       "      <td>26896086-Author Contributions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>961 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               md5sum  \\\n",
       "0    3123623c10be553a87bbbb9a62db4582   \n",
       "1    2b361300ff28b57c63143434a6e3f60b   \n",
       "2    c6ed448e85c0656e3f781c8eb56334c9   \n",
       "3    12fc9a1c01dbd10d7852f8949710433b   \n",
       "4    697856e680f1c2b606de23d708646aa5   \n",
       "..                                ...   \n",
       "956  552e72f182c130a0d49b2583e00f7222   \n",
       "957  4c3f1796b2ae8d64aed4ec40ad4c6b67   \n",
       "958  dc5d9fbe4e9cd4913a358b292e7c2d3d   \n",
       "959  023816cac915382a02118c89261a7c71   \n",
       "960  f3d044e58e1ee5d2c2e7beff111dbef5   \n",
       "\n",
       "                                                  text      pmid  \\\n",
       "0    PD-L1 expression in cancer patients receiving ...  26895815   \n",
       "1    Introduction\\nA dynamic relationship exists be...  26895815   \n",
       "2    Meta-analysis methods\\nA systematic literature...  26895815   \n",
       "3    Literature search\\nPublished reports were obta...  26895815   \n",
       "4    Statistical analysis\\nWe extracted data from a...  26895815   \n",
       "..                                                 ...       ...   \n",
       "956  FcÎ³-receptor mediated induction of wound heali...  26896086   \n",
       "957  Conclusions\\nAnti-TNFs may have multiple modes...  26896086   \n",
       "958  Funding\\nNo specific funding was obtained for ...  26896086   \n",
       "959  Conflict of Interest\\nThe authors declare no c...  26896086   \n",
       "960  Author Contributions\\nADL performed a review o...  26896086   \n",
       "\n",
       "                                                 title  \n",
       "0                                    26895815-abstract  \n",
       "1                                26895815-Introduction  \n",
       "2                       26895815-Meta-analysis methods  \n",
       "3                           26895815-Literature search  \n",
       "4                        26895815-Statistical analysis  \n",
       "..                                                 ...  \n",
       "956  26896086-FcÎ³-receptor mediated induction of wo...  \n",
       "957                               26896086-Conclusions  \n",
       "958                                   26896086-Funding  \n",
       "959                      26896086-Conflict of Interest  \n",
       "960                      26896086-Author Contributions  \n",
       "\n",
       "[961 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"/work/data/projects/data2report/paragraphs_md5sum/paragraphs_benchmark.parquet\"\n",
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect()\n",
    "\n",
    "df = conn.execute(f\"\"\"\n",
    "    SELECT md5sum, text, pmid, title\n",
    "    FROM read_parquet('{file}')\n",
    "    -- WHERE md5sum IN ({\", \".join(map(repr, keys))})\n",
    "\"\"\").df()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5SUM_PREFIXES:  10 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001ed2bf84894fa29298aacfb9b2f26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import duckdb\n",
    "import json\n",
    "\n",
    "paragraph_md5sum_file = \"/work/data/projects/data2report/paragraphs-md5sum-2025012501/md5sum_prefix=*/**.parquet\"\n",
    "conn = duckdb.connect()\n",
    "\n",
    "conn.execute(f\"\"\"\n",
    "PRAGMA threads=30;\n",
    "PRAGMA memory_limit='120GB';\n",
    "\"\"\")\n",
    "\n",
    "# keys = json.load(open(\"/work/data/projects/data2report/keys.json\"))\n",
    "keys = list(results[0][\"top_results\"].keys())\n",
    "md5sum_prefixes = [k[:2] for k in keys]\n",
    "\n",
    "print(\"MD5SUM_PREFIXES: \", len(md5sum_prefixes), len(keys))\n",
    "\n",
    "df = conn.execute(f\"\"\"\n",
    "    SELECT md5sum, text, pmid, title\n",
    "    FROM read_parquet('{paragraph_md5sum_file}')\n",
    "    WHERE md5sum IN ({\", \".join(map(repr, keys))}) AND md5sum_prefix IN ({\", \".join(map(repr, md5sum_prefixes))})\n",
    "\"\"\").df()\n",
    "\n",
    "pmids = df[\"pmid\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning by md5sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# é…ç½®å‚æ•°\n",
    "INPUT_PARQUET = \"/work/data/projects/data2report/paragraphs_md5sum/*.parquet\"  # åŸå§‹æ•°æ®è·¯å¾„ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰\n",
    "OUTPUT_DIR = \"/work/data/projects/data2report/paragraphs-md5sum-2025012501\"       # åˆ†åŒºåæ•°æ®è¾“å‡ºç›®å½•\n",
    "MIN_FILE_SIZE = \"128MB\"                    # åˆå¹¶æ–‡ä»¶çš„æœ€å°å¤§å°\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"é…ç½® DuckDB å‚æ•°ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰\"\"\"\n",
    "    conn = duckdb.connect()\n",
    "    conn.execute(\"PRAGMA threads=30;\")              \n",
    "    conn.execute(\"PRAGMA memory_limit='120GB';\")     \n",
    "    return conn\n",
    "\n",
    "def init_bucketed_export(conn):\n",
    "    \"\"\"\n",
    "    å°†åŸå§‹æ•°æ®æŒ‰ md5sum å‰ä¸¤ä½åˆ†åŒºå¯¼å‡ºã€‚\n",
    "    æ¯ä¸ªåˆ†åŒºè¾“å‡ºä¸€ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶åæ ¼å¼ä¸º <min_md5>_<max_md5>_<num_rows>.parquetï¼Œ\n",
    "    å…¶ä¸­ min_md5 å’Œ max_md5 åˆ†åˆ«ä¸ºè¯¥åˆ†åŒºå†… md5sum çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œnum_rows ä¸ºè¡Œæ•°ã€‚\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹åˆå§‹åˆ†æ¡¶å¯¼å‡º...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    # è·å–æ‰€æœ‰ä¸åŒçš„ md5sum å‰ç¼€ï¼ˆå–å‰ä¸¤ä½ï¼‰\n",
    "    prefixes = conn.execute(f\"\"\"\n",
    "        SELECT DISTINCT substr(md5sum, 1, 2) as prefix \n",
    "        FROM read_parquet('{INPUT_PARQUET}')\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    for row in prefixes:\n",
    "        prefix = row[0]\n",
    "        # æŸ¥è¯¢è¯¥åˆ†åŒºçš„æœ€å°å’Œæœ€å¤§ md5sum\n",
    "        min_max = conn.execute(f\"\"\"\n",
    "            SELECT MIN(md5sum), MAX(md5sum) \n",
    "            FROM read_parquet('{INPUT_PARQUET}') \n",
    "            WHERE substr(md5sum, 1, 2) = '{prefix}'\n",
    "        \"\"\").fetchone()\n",
    "        min_md5, max_md5 = min_max\n",
    "        # è®¡ç®—è¯¥åˆ†åŒºçš„è¡Œæ•°\n",
    "        num_rows = conn.execute(f\"\"\"\n",
    "            SELECT COUNT(*) \n",
    "            FROM read_parquet('{INPUT_PARQUET}') \n",
    "            WHERE substr(md5sum, 1, 2) = '{prefix}'\n",
    "        \"\"\").fetchone()[0]\n",
    "        # åˆ›å»ºåˆ†åŒºç›®å½•ï¼Œä»¥å‰ç¼€å‘½å\n",
    "        partition_dir = os.path.join(OUTPUT_DIR, f\"md5sum_prefix={prefix}\")\n",
    "        os.makedirs(partition_dir, exist_ok=True)\n",
    "        # æ„é€ è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œæ–‡ä»¶åæ ¼å¼: <min_md5>_<max_md5>_<num_rows>.parquet\n",
    "        output_file = os.path.join(partition_dir, f\"{min_md5}_{max_md5}_{num_rows}.parquet\")\n",
    "        sql = f\"\"\"\n",
    "            COPY (\n",
    "                SELECT * \n",
    "                FROM read_parquet('{INPUT_PARQUET}') \n",
    "                WHERE substr(md5sum, 1, 2) = '{prefix}'\n",
    "            )\n",
    "            TO '{output_file}'\n",
    "            WITH (\n",
    "                FORMAT PARQUET\n",
    "            );\n",
    "        \"\"\"\n",
    "        conn.execute(sql)\n",
    "        print(f\"åˆ†åŒº {prefix} å¯¼å‡ºå®Œæˆï¼š{output_file}\")\n",
    "    print(\"æ‰€æœ‰åˆ†åŒºåˆ†æ¡¶å¯¼å‡ºå®Œæˆï¼\")\n",
    "\n",
    "def append_incremental_data(conn, new_data_parquet):\n",
    "    \"\"\"\n",
    "    å°†å¢é‡æ•°æ®æŒ‰ md5sum å‰ä¸¤ä½è¿½åŠ åˆ°å„åˆ†åŒºã€‚\n",
    "    æ¯ä¸ªåˆ†åŒºå¯¼å‡ºä¸ºå•ç‹¬çš„å¢é‡æ–‡ä»¶ï¼Œæ–‡ä»¶åæ ¼å¼ä¸º inc_<timestamp>.parquetã€‚\n",
    "    åç»­å¯è°ƒç”¨ merge_small_files è¿›è¡Œåˆå¹¶ã€‚\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹å¢é‡æ•°æ®è¿½åŠ ...\")\n",
    "    prefixes = conn.execute(f\"\"\"\n",
    "        SELECT DISTINCT substr(md5sum, 1, 2) as prefix \n",
    "        FROM read_parquet('{new_data_parquet}')\n",
    "    \"\"\").fetchall()\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    for row in prefixes:\n",
    "        prefix = row[0]\n",
    "        partition_dir = os.path.join(OUTPUT_DIR, f\"md5sum_prefix={prefix}\")\n",
    "        os.makedirs(partition_dir, exist_ok=True)\n",
    "        output_file = os.path.join(partition_dir, f\"inc_{timestamp}.parquet\")\n",
    "        sql = f\"\"\"\n",
    "            COPY (\n",
    "                SELECT * \n",
    "                FROM read_parquet('{new_data_parquet}') \n",
    "                WHERE substr(md5sum, 1, 2) = '{prefix}'\n",
    "            )\n",
    "            TO '{output_file}'\n",
    "            WITH (\n",
    "                FORMAT PARQUET\n",
    "            );\n",
    "        \"\"\"\n",
    "        conn.execute(sql)\n",
    "        print(f\"å¢é‡åˆ†åŒº {prefix} å¯¼å‡ºå®Œæˆï¼š{output_file}\")\n",
    "    print(\"å¢é‡æ•°æ®è¿½åŠ å®Œæˆï¼\")\n",
    "\n",
    "def merge_small_files(conn):\n",
    "    \"\"\"\n",
    "    åˆå¹¶å„åˆ†åŒºå†…çš„å°æ–‡ä»¶ã€‚\n",
    "    å¯¹äºæ¯ä¸ªåˆ†åŒºï¼ˆç›®å½•åä¸º md5sum çš„å‰ä¸¤ä½ï¼‰ï¼Œ\n",
    "    å¦‚æœå­˜åœ¨å¤šä¸ªæ–‡ä»¶æˆ–å•ä¸ªæ–‡ä»¶å°äºé˜ˆå€¼ï¼Œåˆ™è¯»å–è¯¥åˆ†åŒºæ‰€æœ‰æ•°æ®ï¼Œ\n",
    "    é‡æ–°è®¡ç®—è¯¥åˆ†åŒºçš„ md5sum æœ€å°ã€æœ€å¤§å€¼ä»¥åŠè¡Œæ•°ï¼Œå¹¶å¯¼å‡ºä¸ºæ–°çš„æ–‡ä»¶ï¼Œ\n",
    "    æ–‡ä»¶åæ ¼å¼ä¸º <min_md5>_<max_md5>_<num_rows>.parquetï¼Œæœ€ååˆ é™¤æ—§æ–‡ä»¶ã€‚\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹åˆå¹¶å°æ–‡ä»¶...\")\n",
    "    # éå† OUTPUT_DIR ä¸‹æ‰€æœ‰ç›®å½•ï¼ˆå‡å®šç›®å½•åä¸ºä¸¤ä½åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰\n",
    "    for prefix in os.listdir(OUTPUT_DIR):\n",
    "        partition_dir = os.path.join(OUTPUT_DIR, f\"md5sum_prefix={prefix}\")\n",
    "        if not os.path.isdir(partition_dir):\n",
    "            continue\n",
    "        files = glob.glob(os.path.join(partition_dir, \"*.parquet\"))\n",
    "        if not files:\n",
    "            continue\n",
    "        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ä¸”æ–‡ä»¶å¤§å°è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™æ— éœ€åˆå¹¶\n",
    "        if len(files) == 1:\n",
    "            try:\n",
    "                if os.path.getsize(files[0]) >= parse_file_size(MIN_FILE_SIZE):\n",
    "                    continue\n",
    "            except OSError as e:\n",
    "                print(f\"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {files[0]}, é”™è¯¯: {e}\")\n",
    "        # è®¡ç®—åˆå¹¶åçš„æœ€å°ã€æœ€å¤§ md5sum å’Œè¡Œæ•°\n",
    "        file_pattern = os.path.join(partition_dir, \"*.parquet\")\n",
    "        result = conn.execute(f\"\"\"\n",
    "            SELECT MIN(md5sum), MAX(md5sum), COUNT(*)\n",
    "            FROM read_parquet('{file_pattern}')\n",
    "        \"\"\").fetchone()\n",
    "        min_md5, max_md5, num_rows = result\n",
    "        merged_file = os.path.join(partition_dir, f\"{min_md5}_{max_md5}_{num_rows}.parquet\")\n",
    "        merge_sql = f\"\"\"\n",
    "            COPY (\n",
    "                SELECT * FROM read_parquet('{file_pattern}')\n",
    "            )\n",
    "            TO '{merged_file}'\n",
    "            WITH (\n",
    "                FORMAT PARQUET, \n",
    "                FILE_SIZE '{MIN_FILE_SIZE}'\n",
    "            );\n",
    "        \"\"\"\n",
    "        conn.execute(merge_sql)\n",
    "        print(f\"åˆ†åŒº {prefix} åˆå¹¶å®Œæˆï¼š{merged_file}\")\n",
    "        # åˆ é™¤æ—§æ–‡ä»¶\n",
    "        for f in files:\n",
    "            try:\n",
    "                os.remove(f)\n",
    "            except OSError as e:\n",
    "                print(f\"åˆ é™¤æ–‡ä»¶å¤±è´¥: {f}, é”™è¯¯: {e}\")\n",
    "    print(\"æ‰€æœ‰å°æ–‡ä»¶åˆå¹¶å®Œæˆï¼\")\n",
    "\n",
    "def parse_file_size(size_str):\n",
    "    \"\"\"å°†æ–‡ä»¶å¤§å°å­—ç¬¦ä¸²ï¼ˆå¦‚ '128MB'ï¼‰è½¬æ¢ä¸ºå­—èŠ‚æ•°\"\"\"\n",
    "    size_str = size_str.replace(\" \", \"\")\n",
    "    units = {\"B\": 1, \"KB\": 1024, \"MB\": 1024**2, \"GB\": 1024**3}\n",
    "    num, unit = size_str[:-2], size_str[-2:]\n",
    "    return int(num) * units[unit.upper()]\n",
    "\n",
    "def query_example(conn, target_md5):\n",
    "    \"\"\"\n",
    "    ç¤ºä¾‹æŸ¥è¯¢ï¼šæ ¹æ®ç›®æ ‡ md5sum æå–å‰ä¸¤ä½ä½œä¸ºåˆ†åŒºå‰ç¼€ï¼Œ\n",
    "    ç„¶åä»å¯¹åº”åˆ†åŒºä¸‹æŸ¥è¯¢æ»¡è¶³æ¡ä»¶çš„æ•°æ®ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prefix = target_md5[:2]\n",
    "    except Exception as e:\n",
    "        print(f\"æ— æ•ˆçš„ md5sum: {target_md5}, é”™è¯¯: {e}\")\n",
    "        return []\n",
    "    file_pattern = os.path.join(OUTPUT_DIR, f\"md5sum_prefix={prefix}\", \"*.parquet\")\n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM read_parquet('{file_pattern}')\n",
    "        WHERE md5sum = '{target_md5}';\n",
    "    \"\"\"\n",
    "    print(f\"æ‰§è¡ŒæŸ¥è¯¢: {query}\")\n",
    "    return conn.execute(query).fetchall()\n",
    "\n",
    "def main():\n",
    "    conn = setup_database()\n",
    "    \n",
    "    # åˆå§‹åˆ†æ¡¶å¯¼å‡ºï¼ˆå…¨é‡æ•°æ®ï¼‰\n",
    "    init_bucketed_export(conn)\n",
    "    \n",
    "    # å¢é‡æ•°æ®è¿½åŠ ç¤ºä¾‹ï¼ˆæ¨¡æ‹Ÿæ–°æ•°æ®ï¼‰\n",
    "    # append_incremental_data(conn, \"/path/to/new_data.parquet\")\n",
    "    \n",
    "    # åˆå¹¶å°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰\n",
    "    # merge_small_files(conn)\n",
    "    \n",
    "    # ç¤ºä¾‹æŸ¥è¯¢\n",
    "    # result = query_example(conn, \"d41d8cd98f00b204e9800998ecf8427e\")\n",
    "    # print(f\"æŸ¥è¯¢ç»“æœ: {result}\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
